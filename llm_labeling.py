import json
import random
import time
import os
import re
from typing import List, Dict, Any
from tqdm import tqdm
from openai import OpenAI
from datasets import load_dataset

# ==========================================
# 1. Configuration (Local Qwen)
# ==========================================

client = OpenAI(
    api_key="none",
    base_url="http://202.45.128.234:5788/v1/"
)
MODEL_NAME = "/nfs/whlu/models/Qwen3-Coder-30B-A3B-Instruct"

DOMAIN_LABELS = ["entertainment", "stem", "humanities", "lifestyle"]

SYSTEM_PROMPT = """You are an advanced data annotator for a RAG router. 
Your task is to classify user queries into specific knowledge domains.
You must output the result in a strict JSON format."""

# ==========================================
# Prompt Templates
# ==========================================

# ç”¨äº WoW æ ‡æ³¨ï¼šåˆ©ç”¨å…¨éƒ¨ä¿¡æ¯ï¼ˆtopic + knowledge + context + responseï¼‰
WOW_LABEL_PROMPT_TEMPLATE = """
Analyze the following user query and determine the probability distribution across these 4 domains:

1. **Entertainment**: Movies, music, celebrities, video games, comics, fictional books.
2. **STEM**: Science, technology, engineering, mathematics, physics, biology, computer science, software.
3. **Humanities**: History, philosophy, religion, literature, art, social studies, war, ancient/modern history.
4. **Lifestyle**: Sports, food/cooking, travel, fashion, cars/vehicles, pets, hobbies, health/fitness.

**Conversation Topic:** "{topic}"
**Conversation Context:** {context}
**Knowledge Sources Used:** {knowledge_summary}
**User Query to classify:** "{query}"

**Instructions:**
1. Use ALL the above information (topic, context, knowledge) to deeply understand the domain, but classify **based on the user query itself**.
2. Assign a probability (float between 0.0 and 1.0) to each domain.
3. The sum of all probabilities **must equal 1.0**.
4. **Capture Uncertainty**: If the query is ambiguous or spans multiple domains, distribute the probability mass (e.g., [0.45, 0.05, 0.45, 0.05]). 
5. Output strictly in this JSON format:
{{
    "probabilities": [P_entertainment, P_stem, P_humanities, P_lifestyle],
    "reasoning": "A short explanation"
}}
"""

# ç”¨äº WoW ç”Ÿæˆ context summaryï¼šç”Ÿæˆè¶…ç®€çŸ­çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆæœ€å¤š15ä¸ªè¯ï¼‰
WOW_SUMMARY_PROMPT_TEMPLATE = """
Given the following multi-turn conversation, generate an EXTREMELY CONCISE context summary (max 15 words) that captures ONLY the key topic.

**Conversation Topic:** "{topic}"
**Recent Posts (last 3):** {recent_posts}
**Latest User Query:** "{query}"

**Instructions:**
1. Generate a summary in 10-15 words maximum.
2. Focus ONLY on the main topic, NOT specific details.
3. Do NOT repeat the user query.
4. Examples:
   - "Discussion about red hair genetics and frequency"
   - "Conversation on internet access benefits"
   - "Talk about science fiction and time travel"
5. Output strictly in this JSON format:
{{
    "summary": "A 10-15 word summary"
}}
"""

# ç”¨äº HotpotQAï¼ˆæœ‰ answer + supporting_facts è¾…åŠ©æ ‡æ³¨ï¼‰
HOTPOT_PROMPT_TEMPLATE = """
Analyze the following user query and determine the probability distribution across these 4 domains:

1. **Entertainment**: Movies, music, celebrities, video games, comics, fictional books.
2. **STEM**: Science, technology, engineering, mathematics, physics, biology, computer science, software.
3. **Humanities**: History, philosophy, religion, literature, art, social studies, war, ancient/modern history.
4. **Lifestyle**: Sports, food/cooking, travel, fashion, cars/vehicles, pets, hobbies, health/fitness.

**User Query:** "{query}"
**Answer:** "{answer}"
**Related Topics:** {supporting_facts}

**Instructions:**
1. Use the answer and related topics as hints to understand the user query's domain, but classify **based on the user query itself**.
2. Assign a probability (float between 0.0 and 1.0) to each domain.
3. The sum of all probabilities **must equal 1.0**.
4. **Capture Uncertainty**: If the query is ambiguous or spans multiple domains, distribute the probability mass (e.g., [0.45, 0.05, 0.45, 0.05]). 
5. Output strictly in this JSON format:
{{
    "probabilities": [P_entertainment, P_stem, P_humanities, P_lifestyle],
    "reasoning": "A short explanation"
}}
"""

# ==========================================
# 2. Data Loading (Mixed WoW + HotpotQA)
# ==========================================

def extract_knowledge_summary(knowledge_lists):
    """
    ä» WoW çš„ knowledge å­—æ®µä¸­æå–å…³é”®çŸ¥è¯†æ‘˜è¦ã€‚
    knowledge æ ¼å¼: List[List[str]]ï¼Œæ¯è½®å¯¹è¯å¯¹åº”ä¸€ç»„çŸ¥è¯†æ¡ç›®ã€‚
    """
    key_facts = []
    seen = set()
    for turn_knowledge in knowledge_lists:
        for entry in turn_knowledge:
            if not isinstance(entry, str):
                continue
            # è·³è¿‡ no_passages_used
            if "no_passages_used" in entry:
                continue
            # æ ¼å¼: "Topic __knowledge__ fact text"
            if "__knowledge__" in entry:
                parts = entry.split("__knowledge__", 1)
                if len(parts) == 2:
                    fact = parts[1].strip()
                    # å»é‡ + é™åˆ¶é•¿åº¦
                    if fact and fact not in seen and len(fact) > 20:
                        seen.add(fact)
                        key_facts.append(fact)
            if len(key_facts) >= 5:  # æœ€å¤šå–5æ¡å…³é”®çŸ¥è¯†
                break
        if len(key_facts) >= 5:
            break
    return key_facts


def load_mixed_data(sample_size=2000):
    queries = []
    print(f"ğŸ“¥ Loading datasets (Target total: {sample_size})...")
    
    samples_per_dataset = sample_size // 2
    
    # Load Wizard of Wikipedia
    try:
        print(f"  Loading Wizard of Wikipedia (target: {samples_per_dataset})...")
        ds_wow = load_dataset("chujiezheng/wizard_of_wikipedia", split="train", streaming=True)
        it_wow = iter(ds_wow)
        wow_count = 0
        
        for _ in range(samples_per_dataset * 5):
            if wow_count >= samples_per_dataset:
                break
            try:
                item = next(it_wow)
                
                if 'post' not in item or not item['post']:
                    continue
                
                posts = item['post']
                if not isinstance(posts, list) or len(posts) == 0:
                    continue
                
                # æå– topicï¼ˆpost[0] é€šå¸¸åŒ…å«ä¸»é¢˜è¯ï¼‰
                raw_topic = posts[0].strip() if isinstance(posts[0], str) else ""
                topic = raw_topic.split("\n")[0].strip() if raw_topic else ""
                
                # æå–ç”¨æˆ·å‘è¨€ï¼šä» post[1:] ä¸­å–ç¬¬ä¸€ä¸ªå®Œæ•´å¥å­ä½œä¸º query
                user_query = None
                query_turn_idx = -1
                for idx, post in enumerate(posts[1:], start=1):
                    if isinstance(post, str) and len(post.strip()) > 15 and ' ' in post.strip():
                        user_query = post.strip()
                        query_turn_idx = idx
                        break
                
                if not user_query:
                    continue
                
                # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆå‹ç¼©ï¼‰
                context_posts = []
                start_idx = max(0, query_turn_idx - 3)
                for p in posts[start_idx:query_turn_idx]:
                    if isinstance(p, str) and len(p.strip()) > 0:
                        # æˆªæ–­æ¯æ¡ post åˆ°æœ€å¤š 80 å­—ç¬¦
                        truncated = p.strip()[:80]
                        context_posts.append(truncated)
                
                # æå– knowledge æ‘˜è¦ï¼ˆè¾…åŠ©æ ‡æ³¨ç”¨ï¼‰
                knowledge = item.get('knowledge', [])
                knowledge_summary = extract_knowledge_summary(knowledge)
                
                # æå– topics åˆ—è¡¨
                topics_list = item.get('topics', [])
                
                queries.append({
                    "text": user_query,                    # ç”¨æˆ·æŸ¥è¯¢
                    "topic": topic,                        # å¯¹è¯ä¸»é¢˜
                    "context": context_posts,              # æœ€è¿‘3è½®çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå·²æˆªæ–­ï¼‰
                    "all_posts": [p.strip() for p in posts if isinstance(p, str) and len(p.strip()) > 0],
                    "knowledge_summary": knowledge_summary, # å…³é”®çŸ¥è¯†æ¡ç›®ï¼ˆè¾…åŠ©æ ‡æ³¨ï¼‰
                    "topics_list": topics_list,            # å…¨éƒ¨ä¸»é¢˜æ ‡ç­¾
                    "source": "wizard_of_wikipedia"
                })
                wow_count += 1
                    
            except StopIteration:
                print(f"  âš ï¸ WoW exhausted at {wow_count} samples")
                break
            except Exception as e:
                continue
                
        print(f"  âœ“ Loaded {wow_count} samples from Wizard of Wikipedia")
    except Exception as e:
        print(f"âš ï¸ Wizard of Wikipedia load error: {e}")

    # Load HotpotQA
    try:
        print(f"  Loading HotpotQA (target: {samples_per_dataset})...")
        ds_hotpot = load_dataset("hotpot_qa", "distractor", split="train", streaming=True)
        it_hotpot = iter(ds_hotpot)
        hotpot_count = 0
        
        for _ in range(samples_per_dataset * 2):
            if hotpot_count >= samples_per_dataset:
                break
            try:
                item = next(it_hotpot)
                if 'question' in item and len(item['question'].strip()) > 10:
                    answer = item.get('answer', '').strip()
                    
                    supporting_facts = []
                    if 'supporting_facts' in item and item['supporting_facts']:
                        sf = item['supporting_facts']
                        if isinstance(sf, dict) and 'title' in sf:
                            supporting_facts = list(set(sf['title']))
                        elif isinstance(sf, list):
                            for fact in sf:
                                if isinstance(fact, (list, tuple)) and len(fact) > 0:
                                    supporting_facts.append(fact[0])
                            supporting_facts = list(set(supporting_facts))
                    
                    queries.append({
                        "text": item['question'].strip(),
                        "answer": answer,
                        "supporting_facts": supporting_facts,
                        "source": "hotpot_qa"
                    })
                    hotpot_count += 1
            except StopIteration:
                print(f"  âš ï¸ HotpotQA exhausted at {hotpot_count} samples")
                break
            except Exception as e:
                continue
                
        print(f"  âœ“ Loaded {hotpot_count} samples from HotpotQA")
    except Exception as e:
        print(f"âš ï¸ HotpotQA load error: {e}")

    print(f"\nğŸ“Š Total queries loaded: {len(queries)}")
    print(f"   - Wizard of Wikipedia: {sum(1 for q in queries if q['source'] == 'wizard_of_wikipedia')}")
    print(f"   - HotpotQA: {sum(1 for q in queries if q['source'] == 'hotpot_qa')}")
    
    random.shuffle(queries)
    return queries

# ==========================================
# 3. Qwen Labeler
# ==========================================

class QwenLabeler:
    
    def _call_llm(self, prompt, temperature=0.1):
        """ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£"""
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature,
        )
        content = response.choices[0].message.content
        
        # æå– JSON
        json_str = content
        if "```json" in content:
            match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
            if match:
                json_str = match.group(1)
        elif "{" in content:
            json_str = content[content.find("{"):content.rfind("}")+1]
        
        return json.loads(json_str)
    
    def generate_wow_summary(self, item):
        """
        ä¸º WoW å¤šè½®å¯¹è¯ç”Ÿæˆè¶…ç®€çŸ­çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆ10-15è¯ï¼‰ã€‚
        è¿™ä¸ª summary ä¼šè¢«æ‹¼æ¥åˆ° text ä¸­ï¼Œç”¨äºè’¸é¦è®­ç»ƒã€‚
        """
        topic = item.get("topic", "")
        context = item.get("context", [])  # å·²ç»æ˜¯æœ€è¿‘3è½®çš„æˆªæ–­ç‰ˆæœ¬
        query = item["text"]
        
        # åªä¼ é€’æœ€è¿‘3è½®ç»™ LLMï¼ˆèŠ‚çœ tokenï¼‰
        recent_posts_str = json.dumps(context[-3:] if len(context) > 3 else context, ensure_ascii=False)
        
        prompt = WOW_SUMMARY_PROMPT_TEMPLATE.format(
            topic=topic,
            recent_posts=recent_posts_str,
            query=query
        )
        
        for attempt in range(3):
            try:
                data = self._call_llm(prompt, temperature=0.3)
                summary = data.get("summary", "").strip()
                # éªŒè¯é•¿åº¦ï¼ˆæœ€å¤š20è¯ä½œä¸ºç¡¬é™åˆ¶ï¼‰
                if summary and len(summary.split()) <= 20:
                    return summary
            except Exception as e:
                if attempt == 2:
                    print(f"\nâš ï¸ Summary generation failed for '{query[:30]}...': {e}")
                time.sleep(0.3)
        
        # å…œåº•ï¼šç”¨ topic åšç®€å• summaryï¼ˆ5-8è¯ï¼‰
        if topic:
            return f"Discussion about {topic}"
        return ""
    
    def annotate(self, item):
        """
        æ ‡æ³¨ä¸€æ¡æ•°æ®ã€‚
        
        å¯¹äº WoW æ•°æ®ï¼š
          1. åˆ©ç”¨å…¨éƒ¨ä¿¡æ¯ï¼ˆtopic, context, knowledgeï¼‰åšé«˜è´¨é‡æ ‡æ³¨
          2. é¢å¤–ç”Ÿæˆç®€çŸ­ context summaryï¼ˆ10-15è¯ï¼‰ï¼Œæ‹¼æˆ distill_text
        
        å¯¹äº HotpotQA æ•°æ®ï¼š
          1. åˆ©ç”¨ answer + supporting_facts åšè¾…åŠ©æ ‡æ³¨
          2. distill_text å°±æ˜¯ query æœ¬èº«ï¼ˆå•è½®æ— ä¸Šä¸‹æ–‡ï¼‰
        """
        text = item["text"]
        
        # ========== Step 1: åŸŸæ ‡æ³¨ ==========
        if item["source"] == "wizard_of_wikipedia":
            topic = item.get("topic", "")
            context = item.get("context", [])
            knowledge_summary = item.get("knowledge_summary", [])
            
            context_str = json.dumps(context, ensure_ascii=False)
            knowledge_str = json.dumps(knowledge_summary[:5], ensure_ascii=False) if knowledge_summary else "[]"
            
            prompt = WOW_LABEL_PROMPT_TEMPLATE.format(
                topic=topic,
                context=context_str,
                knowledge_summary=knowledge_str,
                query=text
            )
        elif item["source"] == "hotpot_qa":
            answer = item.get("answer", "")
            supporting_facts = item.get("supporting_facts", [])
            supporting_facts_str = json.dumps(supporting_facts, ensure_ascii=False)
            prompt = HOTPOT_PROMPT_TEMPLATE.format(
                query=text,
                answer=answer,
                supporting_facts=supporting_facts_str
            )
        else:
            prompt = f"""
Classify the following query into domains: Entertainment, STEM, Humanities, Lifestyle.
Query: "{text}"
Output JSON: {{"probabilities": [P_ent, P_stem, P_hum, P_life], "reasoning": "..."}}
"""
        
        # æ ‡æ³¨æ¦‚ç‡
        label_result = None
        for attempt in range(3):
            try:
                data = self._call_llm(prompt)
                probs = data.get("probabilities")
                
                if not probs or len(probs) != 4:
                    continue
                
                total = sum(probs)
                norm_probs = [round(float(p)/total, 4) if total > 0 else 0.25 for p in probs]
                hard_label_idx = norm_probs.index(max(norm_probs))
                
                label_result = {
                    "teacher_probs": norm_probs,
                    "hard_label": hard_label_idx,
                    "label_name": DOMAIN_LABELS[hard_label_idx],
                    "reasoning": data.get("reasoning", "")
                }
                break
                
            except Exception as e:
                if attempt == 2:
                    print(f"\nâŒ Label error for '{text[:30]}...': {e}")
                time.sleep(0.5)
        
        if not label_result:
            return None
        
        # ========== Step 2: æ„å»º distill_text ==========
        if item["source"] == "wizard_of_wikipedia":
            # å¤šè½®å¯¹è¯ï¼šç”Ÿæˆç®€çŸ­ summaryï¼ˆ10-15è¯ï¼‰+ query æ‹¼æ¥
            summary = self.generate_wow_summary(item)
            if summary:
                distill_text = f"[context summary : {summary}] {text}"
            else:
                distill_text = text
        else:
            # HotpotQA å•è½®ï¼šç›´æ¥ç”¨ query
            distill_text = text
        
        return {
            "text": distill_text,           # è’¸é¦è®­ç»ƒç”¨çš„è¾“å…¥ï¼ˆå«ç®€çŸ­ä¸Šä¸‹æ–‡æ‘˜è¦ï¼‰
            "query": text,                  # åŸå§‹çº¯ queryï¼ˆå¤‡ç”¨ï¼‰
            **label_result
        }

# ==========================================
# 4. Main Execution
# ==========================================

def main():
    OUTPUT_FILE = "train_distill_mixed_qwen_v4.jsonl"
    
    TOTAL_SAMPLES = 10000  # å…ˆæµ‹è¯•ï¼Œä¹‹åæ”¹å¤§
    raw_data = load_mixed_data(sample_size=TOTAL_SAMPLES)
    
    if len(raw_data) == 0:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®,è¯·æ£€æŸ¥æ•°æ®é›†è¿æ¥!")
        return
    
    # æ‰“å°å‰5ä¸ªæ ·æœ¬æŸ¥çœ‹æ•ˆæœ
    print("\nğŸ“ Sample queries:")
    for i, item in enumerate(raw_data[:5]):
        src = item['source']
        txt = item['text'][:60]
        if src == "wizard_of_wikipedia":
            extra = f"topic={item.get('topic', 'N/A')}, context_turns={len(item.get('context', []))}"
        else:
            extra = f"answer={item.get('answer', 'N/A')[:30]}, facts={item.get('supporting_facts', [])[:2]}"
        print(f"  {i+1}. [{src}] {extra}")
        print(f"      query: {txt}...")
    
    labeler = QwenLabeler()
    print(f"\nğŸš€ Starting labeling with Qwen at {MODEL_NAME}...")
    
    valid_count = 0
    wow_with_summary = 0
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for item in tqdm(raw_data):
            res = labeler.annotate(item)
            if res:
                res["dataset_source"] = item["source"]
                f.write(json.dumps(res, ensure_ascii=False) + "\n")
                valid_count += 1
                
                # ç»Ÿè®¡æœ‰ summary çš„ WoW æ ·æœ¬
                if item["source"] == "wizard_of_wikipedia" and "[" in res["text"] and "]" in res["text"]:
                    wow_with_summary += 1
                
                if valid_count % 10 == 0:
                    f.flush()

    print(f"\nâœ… Finished! Saved {valid_count} samples to {OUTPUT_FILE}")
    print(f"   - WoW samples with context summary: {wow_with_summary}")
    print(f"   - HotpotQA samples (query only): {valid_count - wow_with_summary}")

if __name__ == "__main__":
    main()
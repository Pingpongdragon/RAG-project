"""
Motivation 2A: Domain Shift â†’ Accuracy Drop â†’ Scaling Recovery

å®éªŒè®¾è®¡:
  - Source: SQuAD (é˜…è¯»ç†è§£, çŸ­ç­”æ¡ˆ)
  - Target: HotpotQA (å¤šè·³æ¨ç†, 1:600 å™ªå£°æ¯”)
  - ç”¨è’¸é¦åˆ†ç±»å™¨å¯¹ query åšé¢†åŸŸæ ‡æ³¨ï¼Œå±•ç¤º shift å‰ååˆ†å¸ƒå·®å¼‚
  - å¤šç§ retriever (Dense/Hybrid) å¯¹æ¯”
  - é€æ­¥å¢åŠ  target æ–‡æ¡£ï¼Œè§‚å¯Ÿæ¢å¤æ›²çº¿

è¾“å‡º: motivation_2/fig_scaling_law_recovery.png
"""
import sys
import gc
import json
import random
from pathlib import Path
from typing import List, Tuple, Dict, Any

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import os

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from plot_config import setup_style, COLORS, MODEL_STYLES, save_fig
from utils import build_pipeline, evaluate_batch, save_results

from common import (
    get_detector, classify_queries,
    get_domain_distribution, compute_jsd,
    DOMAIN_NAMES, NUM_DOMAINS, OUT_DIR,
)

# os.environ["CUDA_VISIBLE_DEVICES"] = ""
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # âœ… æ–°å¢ï¼šé¿å… tokenizer å¤šçº¿ç¨‹æ­»é”
os.environ["OMP_NUM_THREADS"] = "1"              # âœ… æ–°å¢ï¼šé™åˆ¶ OpenMP çº¿ç¨‹æ•°
os.environ["MKL_NUM_THREADS"] = "1"  

# ==========================================
# å®éªŒé…ç½®
# ==========================================
TEST_SAMPLE_SIZE = 100
BATCH_SIZE = 32
NOISE_RATIO = 600               # 1:600 å™ªå£°æ¯”
SCALE_RATIOS = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
RANDOM_SEED = 42

RETRIEVER_CONFIGS = [
    ("sentence-transformers/all-MiniLM-L6-v2", "MiniLM-L6 (Dense)", False),
    ("BAAI/bge-small-en-v1.5",                 "BGE-Small (Dense)", False),
    ("BAAI/bge-large-en-v1.5",                 "BGE-Large (Dense)", False),
    ("BAAI/bge-small-en-v1.5",                 "BGE-Small (Hybrid)", True),
]


# ==========================================
# éšæœºç§å­åˆå§‹åŒ–
# ==========================================
def set_random_seed(seed: int = RANDOM_SEED):
    """ç»Ÿä¸€è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    try:
        import torch
        torch.manual_seed(seed)
        # if torch.cuda.is_available():
        #     torch.cuda.manual_seed_all(seed)
    except ImportError:
        pass
    print(f"ğŸ² Random seed set to {seed}")


# ==========================================
# æ•°æ®åŠ è½½ - é˜¶æ®µ 1: SQuAD (Source Domain)
# ==========================================
def load_squad_source(max_samples: int = TEST_SAMPLE_SIZE) -> Tuple[List[Any], List[Dict]]:
    """
    åŠ è½½ SQuAD æ•°æ®é›†ä½œä¸º source domain
    
    Args:
        max_samples: æœ€å¤§æ ·æœ¬æ•°
    
    Returns:
        src_docs: Document å¯¹è±¡åˆ—è¡¨
        src_test: æµ‹è¯•æ ·æœ¬åˆ—è¡¨ [{"q": question, "a": answers}, ...]
    """
    from datasets import load_dataset
    from langchain_core.documents import Document

    print("ğŸ“š Loading SQuAD (Source Domain)...")
    
    ds_squad = load_dataset("squad", split="validation")
    src_test, src_docs = [], []
    
    for item in ds_squad:
        if len(src_test) >= max_samples:
            break
        
        answers = item['answers']['text']
        # è¿‡æ»¤ï¼šåªä¿ç•™çŸ­ç­”æ¡ˆï¼ˆâ‰¤5è¯ï¼‰
        if not answers or len(answers[0].split()) > 5:
            continue
        
        src_test.append({"q": item['question'], "a": answers})
        src_docs.append(Document(
            page_content=item['context'],
            metadata={"doc_id": item['id'], "source": "squad"},
        ))
    
    print(f"  âœ… Loaded {len(src_docs)} docs, {len(src_test)} questions")
    return src_docs, src_test


# ==========================================
# æ•°æ®åŠ è½½ - é˜¶æ®µ 2: HotpotQA (Target Domain)
# ==========================================
def load_hotpotqa_target(
    max_samples: int = TEST_SAMPLE_SIZE,
    noise_ratio: int = NOISE_RATIO
) -> Tuple[List[Dict], List[Any], List[Any]]:
    """
    åŠ è½½ HotpotQA æ•°æ®é›†ä½œä¸º target domain
    
    Args:
        max_samples: æµ‹è¯•é›†æ ·æœ¬æ•°
        noise_ratio: å™ªå£°æ–‡æ¡£ä¸é‡‘æ ‡æ–‡æ¡£çš„æ¯”ä¾‹
    
    Returns:
        target_test: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
        gold_docs: é‡‘æ ‡æ–‡æ¡£åˆ—è¡¨
        noise_docs: å™ªå£°æ–‡æ¡£åˆ—è¡¨
    """
    from datasets import load_dataset
    from langchain_core.documents import Document
    import itertools

    print("ğŸ“š Loading HotpotQA (Target Domain)...")
    
    # ä½¿ç”¨æµå¼åŠ è½½é¿å…ç¼“å­˜å…¼å®¹æ€§é—®é¢˜
    ds_stream = load_dataset(
        "hotpot_qa", 
        "distractor", 
        split="train",
        streaming=True,
        trust_remote_code=True
    )

    # ===== ç¬¬ä¸€éï¼šå–æµ‹è¯•æ ·æœ¬ï¼Œç»Ÿè®¡ gold docs æ•°é‡ =====
    print(f"  ğŸ“¥ Phase 1: Extracting {max_samples} test samples...")
    buffer = list(itertools.islice(ds_stream, max_samples))
    
    target_test = []
    gold_docs = []
    
    for item in buffer:
        target_test.append({
            "q": item['question'], 
            "a": item['answer']
        })
        
        titles = item['context']['title']
        sentences = item['context']['sentences']
        gold_titles = set(item['supporting_facts']['title'])
        
        for title, sent_list in zip(titles, sentences):
            if title in gold_titles:
                text = f"{title}: " + " ".join(sent_list)
                gold_docs.append(Document(
                    page_content=text,
                    metadata={"doc_id": f"hp_gold_{title}", "source": "hotpot_gold"},
                ))
    
    # ===== ç¬¬äºŒéï¼šç»§ç»­ä»æµä¸­è¯»å–ï¼Œæ„é€ è¶³é‡å™ªå£°æ–‡æ¡£ =====
    target_noise_count = len(gold_docs) * noise_ratio
    noise_docs = []
    
    # æ¯æ¡ HotpotQA å¤§çº¦èƒ½æä¾› 10 ä¸ªæ®µè½ï¼Œé¢„ä¼°éœ€è¦è¯»å–çš„æ¡ç›®æ•°
    estimated_items_needed = (target_noise_count // 8) + 1000  # ç•™ä½™é‡
    print(f"  ğŸ”§ Phase 2: Mining {target_noise_count} noise docs "
          f"(streaming ~{estimated_items_needed} more items)...")
    
    batch_count = 0
    for item in ds_stream:  # ä»ä¸Šæ¬¡æ–­ç‚¹ç»§ç»­è¯»å–ï¼ˆæµå¼è¿­ä»£å™¨è‡ªåŠ¨ç»­æ¥ï¼‰
        if len(noise_docs) >= target_noise_count:
            break
        
        titles = item['context']['title']
        sentences = item['context']['sentences']
        
        for title, sent_list in zip(titles, sentences):
            if len(noise_docs) >= target_noise_count:
                break
            
            text = f"{title}: " + " ".join(sent_list)
            noise_docs.append(Document(
                page_content=text,
                metadata={"doc_id": f"hp_noise_{len(noise_docs)}", "source": "hotpot_noise"},
            ))
        
        batch_count += 1
        if batch_count % 5000 == 0:
            print(f"    ... processed {batch_count} items, noise docs: {len(noise_docs)}/{target_noise_count}")
    
    print(f"  âœ… Loaded {len(target_test)} questions")
    print(f"  âœ… Gold docs: {len(gold_docs)}, Noise docs: {len(noise_docs)}")
    actual_ratio = len(noise_docs) // max(len(gold_docs), 1)
    print(f"  âœ… Noise ratio: 1:{actual_ratio}")
    
    if actual_ratio < noise_ratio:
        print(f"  âš ï¸  Warning: target ratio 1:{noise_ratio}, actual 1:{actual_ratio} "
              f"(dataset may not have enough data)")
    
    return target_test, gold_docs, noise_docs


# ==========================================
# æ•°æ®æ•´åˆ
# ==========================================
def prepare_experiment_data() -> Tuple[List[Any], List[Dict], List[Any], List[Dict]]:
    """
    æ•´åˆæ‰€æœ‰å®éªŒæ•°æ®
    
    Returns:
        src_docs: Source domain æ–‡æ¡£
        src_test: Source domain æµ‹è¯•é›†
        mixed_pool: Target domain æ··åˆæ–‡æ¡£æ± ï¼ˆgold + noiseï¼‰
        target_test: Target domain æµ‹è¯•é›†
    """
    set_random_seed()
    
    # åŠ è½½ source domain
    src_docs, src_test = load_squad_source()
    
    # åŠ è½½ target domain
    target_test, gold_docs, noise_docs = load_hotpotqa_target()
    
    # æ··åˆé‡‘æ ‡ + å™ªå£°æ–‡æ¡£
    mixed_pool = gold_docs + noise_docs
    random.shuffle(mixed_pool)
    
    print(f"\nğŸ“¦ Total KB size: {len(mixed_pool)} docs")
    print(f"   Source: {len(src_docs)} docs")
    print(f"   Target: {len(gold_docs)} gold + {len(noise_docs)} noise")
    
    return src_docs, src_test, mixed_pool, target_test


# ==========================================
# å®éªŒé˜¶æ®µå‡½æ•°
# ==========================================
def evaluate_source_baseline(
    src_docs: List[Any],
    src_test: List[Dict],
    model_name: str,
    use_hybrid: bool
) -> float:
    """Phase 1: è¯„ä¼° source domain åŸºçº¿æ€§èƒ½"""
    pipe = build_pipeline(src_docs, model_name, use_hybrid)
    acc = evaluate_batch(pipe, src_test, BATCH_SIZE)
    del pipe
    gc.collect()
    return acc


def evaluate_domain_shift(
    src_docs: List[Any],
    target_test: List[Dict],
    model_name: str,
    use_hybrid: bool
) -> float:
    """Phase 2: è¯„ä¼° domain shift åçš„æ€§èƒ½ä¸‹é™"""
    pipe = build_pipeline(src_docs, model_name, use_hybrid)
    acc = evaluate_batch(pipe, target_test, BATCH_SIZE)
    del pipe
    gc.collect()
    return acc


def evaluate_scaling_recovery(
    src_docs: List[Any],
    mixed_pool: List[Any],
    target_test: List[Dict],
    model_name: str,
    use_hybrid: bool,
    scale_ratios: List[float]
) -> List[float]:
    """Phase 3: è¯„ä¼°é€šè¿‡æ‰©å…… target çŸ¥è¯†çš„æ¢å¤æ›²çº¿"""
    import torch
    
    total_target = len(mixed_pool)
    accs = []
    
    for ratio in scale_ratios:
        n = int(total_target * ratio)
        docs = src_docs + mixed_pool[:n]
        
        pipe = build_pipeline(docs, model_name, use_hybrid)
        acc = evaluate_batch(pipe, target_test, BATCH_SIZE)
        accs.append(acc)
        
        print(f"    Scale {ratio:.0%} ({n:,} docs): {acc:.2%}")
        
        del pipe
        gc.collect()
        # torch.cuda.empty_cache()
    
    return accs


# ==========================================
# æ ¸å¿ƒå®éªŒæµç¨‹
# ==========================================
def run():
    """Part A: Domain Shift + Scaling Recovery"""
    print("=" * 60)
    print("ğŸ“Š Motivation 2A: Domain Shift + Scaling Recovery")
    print("=" * 60)

    # ===== æ•°æ®å‡†å¤‡ =====
    src_docs, src_test, mixed_pool, target_test = prepare_experiment_data()

    # ===== é¢†åŸŸåˆ†å¸ƒåˆ†æ =====
    print("\nğŸ” Analyzing domain shift with distilled detector...")
    detector = get_detector()
    
    src_queries = [item["q"] for item in src_test]
    tgt_queries = [item["q"] for item in target_test]
    
    src_labels = classify_queries(src_queries, detector)
    tgt_labels = classify_queries(tgt_queries, detector)

    src_dist = get_domain_distribution(src_labels)
    tgt_dist = get_domain_distribution(tgt_labels)
    shift_jsd = compute_jsd(src_dist, tgt_dist)

    print(f"  Source dist: {dict(zip(DOMAIN_NAMES, src_dist.round(3)))}")
    print(f"  Target dist: {dict(zip(DOMAIN_NAMES, tgt_dist.round(3)))}")
    print(f"  JSD:         {shift_jsd:.4f}")

    # ===== å¤šæ¨¡å‹å®éªŒ =====
    x_labels = ["Source\nBaseline", "Domain\nShift"] + [f"{r:.0%}" for r in SCALE_RATIOS]
    all_results = {}

    for model_name, display_name, use_hybrid in RETRIEVER_CONFIGS:
        print(f"\nğŸš€ Evaluating: {display_name}")
        accs = []

        # Phase 1: Source Baseline
        acc_base = evaluate_source_baseline(src_docs, src_test, model_name, use_hybrid)
        accs.append(acc_base)
        print(f"  âœ… Source Baseline: {acc_base:.2%}")

        # Phase 2: Domain Shift
        acc_shift = evaluate_domain_shift(src_docs, target_test, model_name, use_hybrid)
        accs.append(acc_shift)
        print(f"  âš ï¸  Domain Shift:    {acc_shift:.2%} (drop: {acc_base - acc_shift:.2%})")

        # Phase 3: Scaling Recovery
        print(f"  ğŸ”„ Scaling Recovery:")
        accs_recovery = evaluate_scaling_recovery(
            src_docs, mixed_pool, target_test, 
            model_name, use_hybrid, SCALE_RATIOS
        )
        accs.extend(accs_recovery)

        all_results[display_name] = accs

    # ===== ä¿å­˜ç»“æœ =====
    domain_info = {
        "src_dist": src_dist.tolist(),
        "tgt_dist": tgt_dist.tolist(),
        "shift_jsd": shift_jsd,
    }

    out_path = OUT_DIR / "results_scaling.json"
    save_results({
        "results": all_results, 
        "labels": x_labels, 
        "domain_info": domain_info
    }, str(out_path))
    print(f"\nğŸ’¾ Results saved: {out_path}")

    # ===== ç»˜å›¾ =====
    plot(all_results, x_labels, domain_info)

    return all_results, x_labels, domain_info


# ==========================================
# ç»˜å›¾
# ==========================================
def plot(results=None, x_labels=None, domain_info=None):
    """ç»˜åˆ¶ Scaling Recovery + é¢†åŸŸåˆ†å¸ƒå¯¹æ¯”å›¾"""

    # ä»æ–‡ä»¶åŠ è½½
    if results is None:
        data = json.loads((OUT_DIR / "results_scaling.json").read_text())
        results = data["results"]
        x_labels = data["labels"]
        domain_info = data.get("domain_info")

    setup_style()

    if domain_info:
        fig = plt.figure(figsize=(15, 8))
        gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1], wspace=0.25)
        ax = fig.add_subplot(gs[0])
        ax_dist = fig.add_subplot(gs[1])
    else:
        fig, ax = plt.subplots(figsize=(13, 6))

    x = range(len(x_labels))

    # ---- ä¸»å›¾: Scaling Recovery ----
    ax.axvspan(-0.5, 0.5, alpha=0.06, color=COLORS['accent1'])
    ax.axvspan(0.5, 1.5, alpha=0.08, color=COLORS['secondary'])
    ax.axvspan(1.5, len(x_labels) - 0.5, alpha=0.04, color=COLORS['primary'])
    ax.axvline(x=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)
    ax.axvline(x=1.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)

    for name, accs in results.items():
        style = MODEL_STYLES.get(name, {'color': 'gray', 'marker': 'o'})
        ax.plot(x, accs, color=style['color'], marker=style['marker'],
                linewidth=2.5, markersize=8, label=name,
                markeredgecolor='white', markeredgewidth=1.5)
        for xi, yi in zip(x, accs):
            ax.text(xi, yi + 0.012, f"{yi:.0%}", ha='center', fontsize=8,
                    fontweight='bold', color=style['color'])

    # Drop ç®­å¤´
    all_base = [results[m][0] for m in results]
    all_shift = [results[m][1] for m in results]
    max_b, min_s = max(all_base), min(all_shift)
    ax.annotate('', xy=(1, min_s), xytext=(1, max_b),
                arrowprops=dict(arrowstyle='<->', color=COLORS['secondary'], lw=2.5))
    drop = max_b - min_s
    ax.text(1.3, (min_s + max_b) / 2, f'âˆ’{drop:.0%}\ndrop',
            fontsize=12, fontweight='bold', color=COLORS['secondary'],
            bbox=dict(boxstyle='round,pad=0.3', facecolor=COLORS['light_red'],
                      edgecolor=COLORS['secondary'], alpha=0.9))

    # åŒºåŸŸæ ‡ç­¾
    ax.text(0, 0.72, 'Baseline', ha='center', fontsize=10,
            fontweight='bold', color=COLORS['accent1'], alpha=0.8)
    ax.text(1, 0.28, 'Domain\nShift', ha='center', fontsize=10,
            fontweight='bold', color=COLORS['secondary'], alpha=0.9)
    mid_scale = (1.5 + len(x_labels) - 0.5) / 2
    ax.text(mid_scale, 0.72, 'Knowledge Scaling (Recovery)',
            ha='center', fontsize=10, fontweight='bold', color=COLORS['primary'], alpha=0.8)

    ax.set_xlabel('Experiment Phase', fontsize=12)
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_title('(a) Domain Shift Impact & Scaling Recovery\n'
                 f'(Target KB noise ratio = 1:{NOISE_RATIO})',
                 fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(x_labels)
    ax.set_ylim([0.20, 0.78])
    ax.legend(loc='lower right', ncol=2, framealpha=0.95, fontsize=9)

    # ---- å³ä¾§å°å›¾: é¢†åŸŸåˆ†å¸ƒå¯¹æ¯” ----
    if domain_info:
        src_dist = np.array(domain_info["src_dist"])
        tgt_dist = np.array(domain_info["tgt_dist"])
        jsd_val = domain_info["shift_jsd"]

        bar_width = 0.35
        x_dom = np.arange(NUM_DOMAINS)
        ax_dist.barh(x_dom - bar_width / 2, src_dist, bar_width,
                     color=COLORS['accent1'], edgecolor='black', linewidth=0.8,
                     label='Source (SQuAD)')
        ax_dist.barh(x_dom + bar_width / 2, tgt_dist, bar_width,
                     color=COLORS['secondary'], edgecolor='black', linewidth=0.8,
                     label='Target (HotpotQA)')

        ax_dist.set_yticks(x_dom)
        ax_dist.set_yticklabels([d.capitalize() for d in DOMAIN_NAMES], fontsize=10)
        ax_dist.set_xlabel('Proportion', fontsize=11)
        ax_dist.set_title(f'(b) Query Domain Distribution\n(JSD = {jsd_val:.4f})',
                          fontsize=12, fontweight='bold')
        ax_dist.legend(fontsize=8, loc='lower right')
        ax_dist.set_xlim(0, max(max(src_dist), max(tgt_dist)) * 1.3)

        for i, (s, t) in enumerate(zip(src_dist, tgt_dist)):
            ax_dist.text(s + 0.01, i - bar_width / 2, f"{s:.2f}",
                         va='center', fontsize=8, color=COLORS['accent1'], fontweight='bold')
            ax_dist.text(t + 0.01, i + bar_width / 2, f"{t:.2f}",
                         va='center', fontsize=8, color=COLORS['secondary'], fontweight='bold')
        ax_dist.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    out = str(OUT_DIR / "fig_scaling_law_recovery.png")
    save_fig(fig, out)
    plt.close()
    print(f"  âœ… Figure saved: {out}")


# ==========================================
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--plot-only", action="store_true", help="ä»…ç»˜å›¾ (éœ€å·²æœ‰ç»“æœæ–‡ä»¶)")
    args = parser.parse_args()

    if args.plot_only:
        plot()
    else:
        run()
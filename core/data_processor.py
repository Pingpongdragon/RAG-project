from typing import List, Union
from pathlib import Path
import logging
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from datasets import load_dataset
from config import settings
from config.logger_config import configure_logger

logger = configure_logger(__name__)  # ä½¿ç”¨ç»Ÿä¸€é…ç½®çš„logger

# -------------------------
# æ•°æ®é›†å¤„ç†ç›¸å…³
# -------------------------
def _load_hf_dataset(dataset_name: str) -> List[Document]:
    """ç§æœ‰æ–¹æ³•ï¼šåŠ è½½å¹¶è½¬æ¢æ•°æ®é›†"""
    logger.debug(f"ğŸ” å¼€å§‹åŠ è½½æ•°æ®é›†: {dataset_name}")
    raw_data = load_dataset(dataset_name, "question-answer", split="test")
    
    docs = []
    for item in raw_data:
        try:
            doc = Document(
                page_content=f"Question: {item['question']}\nAnswer: {item['answer']}",
                metadata={
                    "q_id": int(item["id"]),
                    "source": "rag-wiki-qa",
                    "question": item["question"]
                }
            )
            docs.append(doc)
        except KeyError as e:
            logger.error(f"âŒ æ•°æ®æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡æ¡ç›®ID {item.get('id', 'unknown')}: {str(e)}")
    
    logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} æ¡æ–‡æ¡£")
    return docs

def _build_hybrid_vector_index(docs: List[Document], embedder: HuggingFaceEmbeddings) -> FAISS:
    """ç§æœ‰æ–¹æ³•ï¼šæ„å»ºé—®é¢˜-ç­”æ¡ˆåˆ†ç¦»çš„ç´¢å¼•ç»“æ„"""
    from langchain.text_splitter import CharacterTextSplitter  # å»¶è¿Ÿå¯¼å…¥å‡å°‘å†…å­˜å¼€é”€
    
    logger.debug("ğŸ› ï¸ å¼€å§‹æ„å»ºæ··åˆç´¢å¼•...")
    
    # é…ç½®åˆ†å‰²å™¨
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=200,
        chunk_overlap=30
    )
    
    processed_docs = []
    for doc in docs:
        try:
            # åˆ†å‰²é—®é¢˜ä¸ç­”æ¡ˆ
            question = doc.metadata["question"]
            answer = doc.page_content.split("\nAnswer: ")[1]
            
            # ç”Ÿæˆä¸¤ç§ç±»å‹æ–‡æ¡£
            processed_docs.extend([
                Document(
                    page_content=question,
                    metadata={**doc.metadata, "content_type": "question"}
                ),
                Document(
                    page_content=text_splitter.split_text(answer)[0],  # å–ç¬¬ä¸€æ®µç­”æ¡ˆ
                    metadata={**doc.metadata, "content_type": "answer"}
                )
            ])
        except Exception as e:
            logger.warning(f"âš ï¸ æ–‡æ¡£å¤„ç†å¤±è´¥ID {doc.metadata.get('q_id', 'unknown')}: {str(e)}")
    
    logger.info(f"ğŸ“Š æ„å»ºç´¢å¼•ä½¿ç”¨æ–‡æ¡£æ•°: {len(processed_docs)}")
    return FAISS.from_documents(processed_docs, embedder)

# -------------------------
# ç´¢å¼•ç®¡ç†ä¸»å‡½æ•°
# -------------------------
def load_or_build_index(
    embedder: Union[HuggingFaceEmbeddings, None] = None,
    index_path: Union[str, Path, None] = None
) -> FAISS:
    """æ™ºèƒ½ç´¢å¼•ç®¡ç†ç³»ç»Ÿ"""
    # å‚æ•°é»˜è®¤å€¼å¤„ç†
    if embedder is None:
        embedder = HuggingFaceEmbeddings(
            model_name=settings.EMBEDDING_MODEL,
            **settings.EMBEDDING_CONFIG
        )
    
    index_path = Path(index_path) if index_path else settings.DEFAULT_INDEX_PATH
    
    # å­˜åœ¨æ€§æ£€æŸ¥
    if index_path.exists():
        try:
            logger.info(f"ğŸ“‚ åŠ è½½ç°æœ‰ç´¢å¼•: : {index_path}")
            return FAISS.load_local(
                folder_path=str(index_path),
                embeddings=embedder,
                allow_dangerous_deserialization=True
            )
        except Exception as e:
            logger.error(f"ğŸš¨ ç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}")
            raise RuntimeError("ç´¢å¼•æ–‡ä»¶å¯èƒ½å·²æŸåï¼Œè¯·åˆ é™¤åé‡æ–°æ„å»º") from e
    
    # æ–°å»ºç´¢å¼•æµç¨‹
    logger.warning("âš ï¸ æœªæ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œå¼€å§‹æ–°æ„å»ºæµç¨‹...")
    try:
        docs = _load_hf_dataset("rag-datasets/rag-mini-wikipedia")
        index = _build_hybrid_vector_index(docs, embedder)
        index.save_local(str(index_path))
        logger.success(f"ğŸ‰ æ–°ç´¢å¼•å·²ä¿å­˜è‡³ {index_path}")  # ä½¿ç”¨å¢å¼ºæ—¥å¿—
        return index
    except Exception as e:
        logger.critical(f"ğŸ”¥ ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
        raise

from typing import List, Union, Tuple
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from datasets import load_dataset
from models.embeddings import embedding_service
from config import settings
import pickle
from config.logger_config import configure_logger
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain.text_splitter import CharacterTextSplitter

logger = configure_logger(__name__)  # ä½¿ç”¨ç»Ÿä¸€é…ç½®çš„logger

# -------------------------
# æ•°æ®é›†å¤„ç†ç›¸å…³
# -------------------------
def _load_hf_dataset() -> List[Document]:
    """æ ¹æ®é…ç½®åŠ è½½æ•°æ®é›†ï¼ˆé€‚é…å½“å‰æµ‹è¯•é›†ç»“æ„ï¼‰"""
    
    # ä»é…ç½®è¯»å–å‚æ•°
    cfg = settings.DATASET_CONFIG
    logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½æ•°æ®é›† {cfg['dataset_name']}")
    
    try:
        # è‡ªåŠ¨æ£€æµ‹æœ¬åœ°ç¼“å­˜è·¯å¾„
        cache_dir = Path(settings.DATA_CACHE_DIR) 
        raw_data = load_dataset(
            cfg['dataset_name'], 
            cfg['config_name'],
            split=cfg['split'],
            cache_dir=str(cache_dir)
        )
    except Exception as e:
        logger.error(f"ğŸ”¥ æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
        return []

    # æœ‰æ•ˆæ€§æ ¡éªŒï¼ˆåŠ¨æ€æ£€æŸ¥å­—æ®µï¼‰
    required_columns = cfg['text_columns'] + [cfg['id_column']]
    valid_items = [
        item for item in raw_data 
        if all(k in item for k in required_columns)
    ]
    invalid_count = len(raw_data) - len(valid_items)
    
    logger.info(f"ğŸ“Š åŠ è½½å®Œæˆ: æœ‰æ•ˆ {len(valid_items)}æ¡, è·³è¿‡æ— æ•ˆ {invalid_count}æ¡")
    
    # ç»„åˆå¤šä¸ªæ–‡æœ¬å­—æ®µ
    return [
        Document(
            page_content= "\n".join(str(item[col]) for col in cfg['text_columns']),
            metadata={
                "doc_id": item[cfg['id_column']],
                "source": cfg['dataset_name']
            }
        ) for item in valid_items
    ]


def _build_hybrid_vector_index(docs: List[Document]) -> FAISS:
    """åŸºäºçº¯æ–‡æœ¬çš„æ··åˆç´¢å¼•æ„å»º"""
    
    cfg = settings.DATASET_CONFIG
    text_splitter = CharacterTextSplitter(
        chunk_size=cfg['chunk_size'],
        chunk_overlap=cfg['chunk_overlap']
    )
    
    processed = []
    for doc in docs:
        try:
            # ç›´æ¥åˆ†å—åŸå§‹æ–‡æœ¬å†…å®¹
            chunks = text_splitter.split_text(doc.page_content)
            for i, chunk in enumerate(chunks):
                processed.append(Document(
                    page_content=chunk,
                    metadata={
                        "doc_id": doc.metadata["doc_id"],
                        "chunk_id": i,
                        "source": doc.metadata["source"]
                    }
                ))
        except KeyError as e:
            logger.error(f"å…ƒæ•°æ®å­—æ®µç¼ºå¤±: {e} äºæ–‡æ¡£ {doc.metadata}")
        except Exception as e:
            logger.error(f"å¤„ç†å¼‚å¸¸: {str(e)}")
    
    logger.info(f"ğŸ¯ ç”Ÿæˆ {len(processed)} ä¸ªåˆ†å—")
    return FAISS.from_documents(
        processed, 
        embedding_service,
        distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT
    )




# -------------------------
# ç´¢å¼•ç®¡ç†ä¸»å‡½æ•°
# -------------------------
def load_or_build_index() -> Tuple[FAISS, List[Document]]:
    """è‡ªåŠ¨åŒ–ç´¢å¼•å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    
    cfg = settings.DATASET_CONFIG
    index_path = Path(cfg['index_save_path'])
    docs_file = Path(settings.DATA_CACHE_DIR)  / "docs.pkl"
    
    # å½“å­˜åœ¨å®Œæ•´ç´¢å¼•æ—¶åŠ è½½
    if (index_path / "index.faiss").exists() and docs_file.exists():
        logger.info(f"ğŸ”„ åŠ è½½ç°æœ‰ç´¢å¼•: {index_path}")
        vector_db = FAISS.load_local(
            folder_path=str(index_path),
            embeddings=embedding_service,
            allow_dangerous_deserialization=True,
            distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT
        )
        with open(docs_file, "rb") as f:
            docs = pickle.load(f)
        return vector_db, docs
    
    # æ–°å»ºç´¢å¼•æµç¨‹
    logger.warning("âš ï¸ æ— å¯ç”¨ç´¢å¼•ï¼Œå¯åŠ¨æ„å»ºæµç¨‹...")
    try:
        docs = _load_hf_dataset()
        if not docs:
            raise ValueError("æ•°æ®é›†åŠ è½½åä¸ºç©ºï¼Œè¯·æ£€æŸ¥é…ç½®")
            
        index = _build_hybrid_vector_index(docs)
        
        # ä¿è¯ä¿å­˜è·¯å¾„å­˜åœ¨
        index_path.mkdir(parents=True, exist_ok=True)
        index.save_local(str(index_path))
        
        with open(docs_file, "wb") as f:
            pickle.dump(docs, f)
            
        logger.success(f"âœ… æ–°ç´¢å¼•ä¿å­˜è‡³é¡¹ç›®ä¸‹ç›®å½• {index_path}")
        return index, docs
    except Exception as e:
        logger.critical(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}", exc_info=True)
        raise



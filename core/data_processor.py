from typing import List, Tuple
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from datasets import load_dataset
from RAG_project.models.embeddings import SafeHuggingFaceEmbedder
from RAG_project.config import settings
import pickle
from RAG_project.config.logger_config import configure_console_logger
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain.text_splitter import CharacterTextSplitter
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))
from streamingqa import extraction

logger = configure_console_logger(__name__)  # ä½¿ç”¨ç»Ÿä¸€é…ç½®çš„logger

# åˆ›å»º LangChain å…¼å®¹çš„ Embeddings å¯¹è±¡
class CompatibleEmbeddings:
    """å…¼å®¹ LangChain çš„ Embeddings åŒ…è£…å™¨"""
    def __init__(self):
        self.client = SafeHuggingFaceEmbedder().embedder
        self.model_name = settings.EMBEDDING_MODEL
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨"""
        embeddings = self.client.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        embedding = self.client.encode([text], convert_to_numpy=True)
        return embedding[0].tolist()
    
    def __call__(self, text: str) -> List[float]:
        """ä½¿å¯¹è±¡å¯è°ƒç”¨ï¼Œç”¨äºå‘åå…¼å®¹"""
        return self.embed_query(text)
    
    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
        """å¼‚æ­¥åµŒå…¥æ–‡æ¡£ï¼ˆfallbackåˆ°åŒæ­¥ï¼‰"""
        return self.embed_documents(texts)
    
    async def aembed_query(self, text: str) -> List[float]:
        """å¼‚æ­¥åµŒå…¥æŸ¥è¯¢ï¼ˆfallbackåˆ°åŒæ­¥ï¼‰"""
        return self.embed_query(text)

# å…¨å±€ embedding service
embedding_service = CompatibleEmbeddings()

# åˆ›å»º LangChain å…¼å®¹çš„ Embeddings å¯¹è±¡
# å…¨å±€ embedding service
embedding_service = CompatibleEmbeddings()

# -------------------------
# æ•°æ®é›†å¤„ç†ç›¸å…³
# -------------------------

def _load_wmt_dataset(counter_size=30000) -> List[Document]:
    """åŠ è½½WMTæ•°æ®é›†"""
    logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½WMTæ•°æ®é›†")
    wmt_docs = []
    wmt_dir = Path(settings.WMT_DIR)
    # å‡è®¾è¿™é‡Œé…ç½®äº†WMTå­˜æ¡£æ–‡ä»¶è·¯å¾„å’Œå»é‡æ’åºé”®æ–‡ä»¶è·¯å¾„
    wmt_archive_file_paths = [
        str(wmt_dir / 'news-docs.2019.en.filtered.gz'),
    ]
    streamingqa_dir = Path(settings.STREAMINGQA_DIR)    
    deduplicated_sorting_keys_file_path = str(streamingqa_dir / 'wmt_sorting_key_ids.txt.gz')

    try:
        wmt_doc_objects = extraction.get_deduplicated_wmt_docs(
            wmt_archive_files=wmt_archive_file_paths,
            deduplicated_sorting_keys_file=deduplicated_sorting_keys_file_path
        )
         # éšæœºé‡‡æ ·æ–‡æ¡£
        wmt_passage_objects = extraction.get_wmt_passages_from_docs(wmt_doc_objects)
        counter = 0
        for wmt_passage in wmt_passage_objects:
            if counter >= counter_size:
                break
            passage = Document(
                page_content=wmt_passage.text.decode(),
                metadata={
                    "doc_id": wmt_passage.id.split('_')[0], 
                    "source": "WMT"
                }
            )
            wmt_docs.append(passage)
            counter += 1
    except Exception as e:
        logger.error(f"ğŸ”¥ åŠ è½½WMTæ•°æ®é›†å¤±è´¥: {str(e)}")

    logger.info(f"ğŸ“Š åŠ è½½å®Œæˆ: å…± {len(wmt_docs)} æ¡")
    return wmt_docs

def _load_hf_dataset(cfg=None) -> List[Document]:
    """åŠ è½½æ•°æ®é›†ï¼Œé’ˆå¯¹MMLUåªä¿ç•™é—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆ
    
    Args:
        cfg: æ•°æ®é›†é…ç½®ï¼Œä¸ºNoneæ—¶ä½¿ç”¨é»˜è®¤é…ç½®
        
    Returns:
        List[Document]: åŠ è½½çš„é—®ç­”æ–‡æ¡£åˆ—è¡¨
    """
    if not cfg:
        cfg = settings.KNOWLEDGE_DATASET_CONFIG
    
    logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½æ•°æ®é›† {cfg['dataset_name']} from {cfg['dataset_source']}")
    
    if cfg['dataset_source'] == 'local':
        return _load_wmt_dataset()
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºMMLUæ•°æ®é›†ï¼Œä»¥ä¾¿ç‰¹æ®Šå¤„ç†
    is_mmlu = "mmlu" in cfg['dataset_name'].lower()
    documents = []
    
    # å¤„ç†é…ç½®åç§°
    config_names = cfg['config_name']
    if isinstance(config_names, str):
        config_names = [config_names]
        
    for config_name in config_names:
        logger.info(f"åŠ è½½é…ç½®: {config_name}")
        try:
            # åŠ è½½æ•°æ®é›†
            cache_dir = Path(settings.DATA_CACHE_DIR) 
            raw_data = load_dataset(
                cfg['dataset_name'], 
                config_name,
                split=cfg['split'],
                cache_dir=str(cache_dir)
            )
            
            # MMLUç‰¹æ®Šå¤„ç†: æå–é—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆ
            if is_mmlu and "question" in raw_data.column_names and "answer" in raw_data.column_names:
                logger.info(f"æ£€æµ‹åˆ°MMLUæ•°æ®é›†ï¼Œæå–é—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆ...")
                
                valid_items = []
                for item in raw_data:
                    if "question" in item and "choices" in item and "answer" in item:
                        try:
                            # æ˜ å°„ç­”æ¡ˆå­—æ¯åˆ°é€‰é¡¹
                            answer_index = item['answer']
                            if 0 <= answer_index < len(item["choices"]):
                                valid_items.append({
                                    "id": item.get(cfg['id_column'], f"item_{len(valid_items)}"),
                                    "question": item["question"],
                                    "correct_answer": item["choices"][answer_index],
                                    "subject": config_name.replace("_", " ")
                                })
                        except Exception as e:
                            logger.error(f"å¤„ç†MMLUé—®é¢˜æ—¶å‡ºé”™: {str(e)}")
                
                # åˆ›å»ºæ–‡æ¡£
                config_docs = []
                for item in valid_items:
                    # æ ¼å¼åŒ–å†…å®¹ä¸ºé—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆ
                    content = f"Subject: {item['subject']}\nQuestion: {item['question']}\nAnswer: {item['correct_answer']}"
                    
                    doc = Document(
                        page_content=content,
                        metadata={ 
                            "doc_id": f"{config_name}_{item['id']}", 
                            "source": f"{cfg['dataset_name']}_{config_name}",
                            "config": config_name,
                            "subject": item['subject']
                        }
                    )
                    config_docs.append(doc)
                
                logger.info(f"ä»MMLUé…ç½® {config_name} æå–äº† {len(config_docs)} ä¸ªé—®ç­”å¯¹")
                documents.extend(config_docs)
                
            # æ ‡å‡†å¤„ç†æµç¨‹
            else:
                required_columns = cfg['text_columns'] + [cfg['id_column']]
                valid_items = [
                    item for item in raw_data 
                    if all(k in item for k in required_columns)
                ]
                
                logger.info(f"é…ç½® {config_name} åŠ è½½å®Œæˆ: æœ‰æ•ˆ {len(valid_items)}æ¡")
                
                # åˆ›å»ºæ ‡å‡†æ–‡æ¡£
                config_docs = [
                    Document(
                        page_content="\n".join(str(item[col]) for col in cfg['text_columns']),
                        metadata={ 
                            "doc_id": f"{config_name}_{item[cfg['id_column']]}", 
                            "source": f"{cfg['dataset_name']}_{config_name}",
                            "config": config_name
                        }
                    ) for item in valid_items
                ]
                
                documents.extend(config_docs)
                
        except Exception as e:
            logger.error(f"ğŸ”¥ é…ç½® {config_name} åŠ è½½å¤±è´¥: {str(e)}")
    
    logger.info(f"ğŸ“Š æ‰€æœ‰é…ç½®åŠ è½½å®Œæˆ: å…± {len(documents)} æ¡æ–‡æ¡£")
    return documents

def _build_hybrid_vector_index(docs: List[Document]) -> FAISS:
    """åŸºäºçº¯æ–‡æœ¬çš„æ··åˆç´¢å¼•æ„å»º"""
    
    cfg = settings.KNOWLEDGE_DATASET_CONFIG
    text_splitter = CharacterTextSplitter(
        chunk_size=cfg['chunk_size'],
        chunk_overlap=cfg['chunk_overlap']
    )
    
    processed = []
    for doc in docs:
        try:
            # ç›´æ¥åˆ†å—åŸå§‹æ–‡æœ¬å†…å®¹
            chunks = text_splitter.split_text(doc.page_content)
            for i, chunk in enumerate(chunks):
                processed.append(Document(
                    page_content=chunk,
                    metadata={
                        "doc_id": doc.metadata["doc_id"],
                        "chunk_id": i,
                        "source": doc.metadata["source"]
                    }
                ))
        except KeyError as e:
            logger.error(f"å…ƒæ•°æ®å­—æ®µç¼ºå¤±: {e} äºæ–‡æ¡£ {doc.metadata}")
        except Exception as e:
            logger.error(f"å¤„ç†å¼‚å¸¸: {str(e)}")
    
    logger.info(f"ğŸ¯ ç”Ÿæˆ {len(processed)} ä¸ªåˆ†å—")
    return FAISS.from_documents(
        processed, 
        embedding_service,
        distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT
    )

# -------------------------
# ç´¢å¼•ç®¡ç†ä¸»å‡½æ•°
# -------------------------
def load_or_build_index() -> Tuple[FAISS, List[Document]]:
    """è‡ªåŠ¨åŒ–ç´¢å¼•å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    
    cfg = settings.KNOWLEDGE_DATASET_CONFIG
    index_path = Path(cfg['index_save_path'])
    docs_file = Path(settings.DATA_CACHE_DIR)  / "docs.pkl"
    
    # å½“å­˜åœ¨å®Œæ•´ç´¢å¼•æ—¶åŠ è½½
    if (index_path / "index.faiss").exists() and docs_file.exists():
        logger.info(f"ğŸ”„ åŠ è½½ç°æœ‰ç´¢å¼•: {index_path}")
        vector_db = FAISS.load_local(
            folder_path=str(index_path),
            embeddings=embedding_service,
            allow_dangerous_deserialization=True,
            distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT
        )
        with open(docs_file, "rb") as f:
            docs = pickle.load(f)
        return vector_db, docs
    
    # æ–°å»ºç´¢å¼•æµç¨‹
    logger.warning("âš ï¸ æ— å¯ç”¨ç´¢å¼•ï¼Œå¯åŠ¨æ„å»ºæµç¨‹...")
    try:
        docs = _load_hf_dataset()
        if not docs:
            raise ValueError("æ•°æ®é›†åŠ è½½åä¸ºç©ºï¼Œè¯·æ£€æŸ¥é…ç½®")
            
        index = _build_hybrid_vector_index(docs)
        
        # ä¿è¯ä¿å­˜è·¯å¾„å­˜åœ¨
        index_path.mkdir(parents=True, exist_ok=True)
        index.save_local(str(index_path))
        
        with open(docs_file, "wb") as f:
            pickle.dump(docs, f)
            
        logger.success(f"âœ… æ–°ç´¢å¼•ä¿å­˜è‡³é¡¹ç›®ä¸‹ç›®å½• {index_path}")
        return index, docs
    except Exception as e:
        logger.critical(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}", exc_info=True)
        raise

# -------------------------
# å¢é‡æ›´æ–°ç›¸å…³åŠŸèƒ½ï¼ˆæ–°å¢éƒ¨åˆ†ï¼‰
# -------------------------

import hashlib
import json
from datetime import datetime

class KnowledgeBaseUpdater:
    """çŸ¥è¯†åº“å¢é‡æ›´æ–°ç®¡ç†å™¨"""
    
    def __init__(self, vector_db: FAISS, docs_list: List[Document]):
        self.vector_db = vector_db
        self.docs_list = docs_list
        self.metadata_file = Path(settings.DATA_CACHE_DIR) / "kb_metadata.json"
        self.load_metadata()
    
    def load_metadata(self):
        """åŠ è½½çŸ¥è¯†åº“å…ƒæ•°æ®"""
        try:
            if self.metadata_file.exists():
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                
                # è°ƒè¯•ï¼šæ‰“å°å…ƒæ•°æ®ç»“æ„
                
                # è‡ªåŠ¨ä¿®æ­£ï¼šç¡®ä¿æ‰€æœ‰ documents çš„ value éƒ½æ˜¯ dict
                if isinstance(self.metadata.get("documents"), dict):
                    fixed_docs = {}
                    for k, v in self.metadata.get("documents", {}).items():
                        if isinstance(v, dict):
                            fixed_docs[k] = v
                        else:
                            logger.warning(f"âš ï¸ æ–‡æ¡£ {k} çš„å€¼ä¸æ˜¯å­—å…¸ï¼Œè·³è¿‡: {v}")
                            fixed_docs[k] = {}
                    self.metadata["documents"] = fixed_docs
                else:
                    logger.warning(f"âš ï¸ documentså­—æ®µç±»å‹å¼‚å¸¸: {type(self.metadata.get('documents'))}")
                    self.metadata["documents"] = {}
            else:
                logger.info("ğŸ“ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¸ºç©º")
                self.metadata = {
                    "documents": {},  # doc_id -> {hash, timestamp, chunks_count}
                    "last_updated": None,
                    "total_chunks": 0
                }
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}ï¼Œåˆå§‹åŒ–ä¸ºç©º")
            self.metadata = {
                "documents": {},
                "last_updated": None,
                "total_chunks": 0
            }

    def save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®åˆ°æ–‡ä»¶"""
        self.metadata["last_updated"] = datetime.now().isoformat()
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
    
    def _calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def add_documents(self, new_docs: List[Document], force_update=False) -> bool:
        """å¢é‡æ·»åŠ æ–°æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        logger.info(f"ğŸš€ å¼€å§‹å¢é‡æ·»åŠ  {len(new_docs)} ä¸ªæ–‡æ¡£")
        
        cfg = settings.KNOWLEDGE_DATASET_CONFIG
        text_splitter = CharacterTextSplitter(
            chunk_size=cfg['chunk_size'],
            chunk_overlap=cfg['chunk_overlap']
        )
        
        new_chunks = []
        updated_docs = 0
        
        for doc in new_docs:
            doc_id = doc.metadata.get("doc_id")
            if not doc_id:
                logger.warning(f"æ–‡æ¡£ç¼ºå°‘doc_idï¼Œè·³è¿‡: {doc.page_content[:50]}...")
                continue
            
            content_hash = self._calculate_content_hash(doc.page_content)
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨ä¸”å†…å®¹æœªå˜åŒ–
            if doc_id in self.metadata["documents"] and not force_update:
                existing_hash = self.metadata["documents"][doc_id].get("hash")
                if existing_hash == content_hash:
                    logger.info(f"æ–‡æ¡£ {doc_id} å†…å®¹æ— å˜åŒ–ï¼Œè·³è¿‡")
                    continue
                else:
                    # å†…å®¹å·²å˜åŒ–ï¼Œå…ˆåˆ é™¤æ—§ç‰ˆæœ¬
                    self.remove_document(doc_id, save_metadata=False)
            
            # å¤„ç†æ–°æ–‡æ¡£
            try:
                chunks = text_splitter.split_text(doc.page_content)
                chunk_count = 0
                
                for i, chunk in enumerate(chunks):
                    chunk_doc = Document(
                        page_content=chunk,
                        metadata={
                            "doc_id": doc_id,
                            "chunk_id": i,
                            "source": doc.metadata.get("source", "unknown"),
                            "timestamp": datetime.now().isoformat(),
                            **{k: v for k, v in doc.metadata.items() if k not in ["doc_id", "chunk_id"]}
                        }
                    )
                    new_chunks.append(chunk_doc)
                    chunk_count += 1
                
                # æ›´æ–°å…ƒæ•°æ®
                self.metadata["documents"][doc_id] = {
                    "hash": content_hash,
                    "timestamp": datetime.now().isoformat(),
                    "chunks_count": chunk_count,
                    "source": doc.metadata.get("source", "unknown")
                }
                updated_docs += 1
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡æ¡£ {doc_id} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        if new_chunks:
            # æ·»åŠ æ–°chunksåˆ°å‘é‡æ•°æ®åº“
            logger.info(f"ğŸ“ æ·»åŠ  {len(new_chunks)} ä¸ªæ–°å—åˆ°å‘é‡æ•°æ®åº“")
            self.vector_db.add_documents(new_chunks)
            
            # æ›´æ–°æ–‡æ¡£åˆ—è¡¨
            self.docs_list.extend(new_chunks)
            
            # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•å’Œæ–‡æ¡£
            self._save_index_and_docs()
            
            # æ›´æ–°æ€»è®¡æ•°
            self.metadata["total_chunks"] = len(self.docs_list)
            self.save_metadata()
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ /æ›´æ–° {updated_docs} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ {len(new_chunks)} ä¸ªå—")
            return True
        
        logger.info("ğŸ“‹ æ²¡æœ‰æ–°å†…å®¹éœ€è¦æ·»åŠ ")
        return False
    
    def remove_document(self, doc_id: str, save_metadata: bool = True) -> bool:
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å—"""
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£: {doc_id}")
        
        # æ‰¾åˆ°è¦åˆ é™¤çš„å—ç´¢å¼•
        indices_to_remove = []
        for i, doc in enumerate(self.docs_list):
            if doc.metadata.get("doc_id") == doc_id:
                indices_to_remove.append(i)
        
        if not indices_to_remove:
            logger.warning(f"æœªæ‰¾åˆ°æ–‡æ¡£ {doc_id}")
            return False
        
        # ä»åå¾€å‰åˆ é™¤ï¼Œé¿å…ç´¢å¼•å˜åŒ–
        for i in reversed(indices_to_remove):
            del self.docs_list[i]
        
        # é‡å»ºå‘é‡ç´¢å¼•ï¼ˆFAISSä¸æ”¯æŒç›´æ¥åˆ é™¤ï¼Œéœ€è¦é‡å»ºï¼‰
        if self.docs_list:
            self._rebuild_vector_index()
        else:
            # å¦‚æœæ²¡æœ‰æ–‡æ¡£äº†ï¼Œåˆ›å»ºç©ºç´¢å¼•
            self.vector_db = FAISS.from_documents(
                [Document(page_content="dummy", metadata={"doc_id": "dummy"})], 
                embedding_service,
                distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT
            )
        
        # æ›´æ–°å…ƒæ•°æ®
        if doc_id in self.metadata["documents"]:
            del self.metadata["documents"][doc_id]
        
        self.metadata["total_chunks"] = len(self.docs_list)
        
        if save_metadata:
            self.save_metadata()
            self._save_index_and_docs()
        
        logger.info(f"âœ… å·²åˆ é™¤æ–‡æ¡£ {doc_id}ï¼Œç§»é™¤ {len(indices_to_remove)} ä¸ªå—")
        return True
    
    def update_document(self, doc_id: str, new_content: str, new_metadata: dict = None) -> bool:
        """æ›´æ–°ç°æœ‰æ–‡æ¡£çš„å†…å®¹"""
        logger.info(f"âœï¸ æ›´æ–°æ–‡æ¡£: {doc_id}")
        
        # å…ˆåˆ é™¤æ—§æ–‡æ¡£
        if not self.remove_document(doc_id, save_metadata=False):
            logger.warning(f"æ–‡æ¡£ {doc_id} ä¸å­˜åœ¨ï¼Œå°†ä½œä¸ºæ–°æ–‡æ¡£æ·»åŠ ")
        
        # åˆ›å»ºæ–°æ–‡æ¡£
        updated_metadata = new_metadata or {}
        updated_metadata["doc_id"] = doc_id
        
        new_doc = Document(
            page_content=new_content,
            metadata=updated_metadata
        )
        
        # æ·»åŠ æ–°ç‰ˆæœ¬
        success = self.add_documents([new_doc], force_update=True)
        
        if success:
            logger.info(f"âœ… æ–‡æ¡£ {doc_id} æ›´æ–°å®Œæˆ")
        else:
            logger.error(f"âŒ æ–‡æ¡£ {doc_id} æ›´æ–°å¤±è´¥")
        
        return success
    
    def _rebuild_vector_index(self):
        """é‡å»ºå‘é‡ç´¢å¼•"""
        logger.info("ğŸ”„ é‡å»ºå‘é‡ç´¢å¼•...")
        if self.docs_list:
            self.vector_db = FAISS.from_documents(
                self.docs_list, 
                embedding_service,
                distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT
            )
    
    def _save_index_and_docs(self):
        """ä¿å­˜ç´¢å¼•å’Œæ–‡æ¡£åˆ°æ–‡ä»¶"""
        cfg = settings.KNOWLEDGE_DATASET_CONFIG
        index_path = Path(cfg['index_save_path'])
        docs_file = Path(settings.DATA_CACHE_DIR) / "docs.pkl"
        
        # ä¿å­˜ç´¢å¼•
        index_path.mkdir(parents=True, exist_ok=True)
        self.vector_db.save_local(str(index_path))
        
        # ä¿å­˜æ–‡æ¡£
        with open(docs_file, "wb") as f:
            pickle.dump(self.docs_list, f)
    
    def get_document_info(self, doc_id: str = None) -> dict:
        """è·å–æ–‡æ¡£ä¿¡æ¯"""
        if doc_id:
            return self.metadata["documents"].get(doc_id, {})
        else:
            return {
                "total_documents": len(self.metadata["documents"]),
                "total_chunks": self.metadata["total_chunks"],
                "last_updated": self.metadata["last_updated"],
                "documents": self.metadata["documents"]
            }
    
    def list_sources(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æ•°æ®æº"""
        sources = set()
        for doc_info in self.metadata["documents"].values():
            if isinstance(doc_info, dict):
                sources.add(doc_info.get("source", "unknown"))
        return sorted(list(sources))

def get_knowledge_base_updater() -> KnowledgeBaseUpdater:
    """è·å–çŸ¥è¯†åº“æ›´æ–°å™¨çš„ä¾¿æ·å‡½æ•°"""
    vector_db, docs_list = load_or_build_index()
    return KnowledgeBaseUpdater(vector_db, docs_list)

def demo_incremental_update():
    """æ¼”ç¤ºå¢é‡æ›´æ–°åŠŸèƒ½"""
    logger.info("ğŸ”¬ å¼€å§‹å¢é‡æ›´æ–°æ¼”ç¤º...")
    
    try:
        # è·å–æ›´æ–°å™¨
        updater = get_knowledge_base_updater()
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        info = updater.get_document_info()
        logger.info(f"ğŸ“Š å½“å‰çŸ¥è¯†åº“çŠ¶æ€: {info['total_documents']} ä¸ªæ–‡æ¡£ï¼Œ{info['total_chunks']} ä¸ªå—")
        
        # æ·»åŠ æ–°æ–‡æ¡£ç¤ºä¾‹
        new_docs = [
            Document(
                page_content="è¿™æ˜¯ä¸€ä¸ªæ–°çš„æµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºæ¼”ç¤ºå¢é‡æ›´æ–°åŠŸèƒ½ã€‚å®ƒåŒ…å«äº†å…³äºæœºå™¨å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†ã€‚",
                metadata={"doc_id": "test_doc_1", "source": "manual_test", "topic": "machine_learning"}
            ),
            Document(
                page_content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚",
                metadata={"doc_id": "test_doc_2", "source": "manual_test", "topic": "deep_learning"}
            )
        ]
        
        # æ‰§è¡Œå¢é‡æ·»åŠ 
        success = updater.add_documents(new_docs)
        if success:
            logger.info("âœ… æ–°æ–‡æ¡£æ·»åŠ æˆåŠŸ")
        
        # æ›´æ–°æ–‡æ¡£ç¤ºä¾‹
        updated_success = updater.update_document(
            "test_doc_1", 
            "è¿™æ˜¯ä¸€ä¸ªæ›´æ–°åçš„æµ‹è¯•æ–‡æ¡£ï¼Œç°åœ¨åŒ…å«äº†æ›´å¤šå…³äºè‡ªç„¶è¯­è¨€å¤„ç†çš„ä¿¡æ¯ã€‚",
            {"doc_id": "test_doc_1", "source": "manual_test", "topic": "nlp"}
        )
        
        if updated_success:
            logger.info("âœ… æ–‡æ¡£æ›´æ–°æˆåŠŸ")
        
        # æ˜¾ç¤ºæ›´æ–°åçŠ¶æ€
        final_info = updater.get_document_info()
        logger.info(f"ğŸ“Š æ›´æ–°åçŸ¥è¯†åº“çŠ¶æ€: {final_info['total_documents']} ä¸ªæ–‡æ¡£ï¼Œ{final_info['total_chunks']} ä¸ªå—")
        
        # åˆ—å‡ºæ‰€æœ‰æ•°æ®æº
        try:
            sources = updater.list_sources()
            logger.info(f"ğŸ“‹ æ•°æ®æºåˆ—è¡¨: {sources}")
        except Exception as e:
            logger.error(f"âŒ è°ƒç”¨ list_sources å¤±è´¥: {e}")
            import traceback
            logger.error(f"âŒ è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")

        # æœç´¢æµ‹è¯•
        if updater.vector_db:
            logger.info("ğŸ” æµ‹è¯•å‘é‡æœç´¢åŠŸèƒ½...")
            try:
                results = updater.vector_db.similarity_search("æœºå™¨å­¦ä¹ ", k=3)
                logger.info(f"æœç´¢'æœºå™¨å­¦ä¹ 'æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³å—:")
                for i, result in enumerate(results):
                    logger.info(f"  ç»“æœ {i+1}: {result.page_content[:80]}...")
            except Exception as e:
                logger.warning(f"å‘é‡æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        
        logger.info("ğŸ‰ å¢é‡æ›´æ–°æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ å¢é‡æ›´æ–°æ¼”ç¤ºå¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")

if __name__ == "__main__":
    # è¿è¡Œå¢é‡æ›´æ–°æ¼”ç¤º
    demo_incremental_update()
from typing import List, Union
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from datasets import load_dataset
from models.embeddings import embedding_service
from config import settings
from config.logger_config import configure_logger

logger = configure_logger(__name__)  # ä½¿ç”¨ç»Ÿä¸€é…ç½®çš„logger

# -------------------------
# æ•°æ®é›†å¤„ç†ç›¸å…³
# -------------------------
def _load_hf_dataset(dataset_name: str) -> List[Document]:
    """æ•°æ®åŠ è½½ä¸åŸºç¡€å¤„ç† (å¸¦å®Œæ•´æ—¥å¿—)"""
    logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name}")
    try:
        raw_data = load_dataset(dataset_name, "question-answer", split="test")
    except Exception as e:
        logger.error(f"ğŸ”¥ æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
        return []

    valid_items = [item for item in raw_data 
                  if all(k in item for k in ["id", "question", "answer"])]
    invalid_count = len(raw_data) - len(valid_items)
    
    logger.info(f"ğŸ“Š æœ‰æ•ˆè®°å½•: {len(valid_items)}æ¡, æ— æ•ˆè·³è¿‡: {invalid_count}æ¡")
    
    return [
        Document(
            page_content=f"Question:{item['question']}",
            metadata={
                "q_id": item["id"],
                "question": item["question"],
                "raw_answer": item["answer"],  # å®Œæ•´ä¿ç•™åŸå§‹å›ç­”
                "source": dataset_name
            }
        ) for item in valid_items
    ]

def _build_hybrid_vector_index(docs: List[Document]) -> FAISS:
    """é—®ç­”èåˆç´¢å¼•æ„å»º (ä½¿ç”¨raw_answer)"""
    from langchain.text_splitter import CharacterTextSplitter
    
    logger.debug("ğŸ”§ åˆå§‹åŒ–æ··åˆç´¢å¼•å¤„ç†å™¨")
    text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=30)
    
    processed = []
    for doc in docs:
        try:
            answer_chunk = text_splitter.split_text(doc.metadata["raw_answer"])[0]  # ç›´æ¥ä½¿ç”¨åŸå§‹å›ç­”
            processed.append(Document(
                page_content=f"Q:{doc.metadata['question']}\nA:{answer_chunk}",
                metadata={
                    "source": "hybrid-index",
                    "origin_id": doc.metadata["q_id"]
                }
            ))
            logger.debug(f"âœ… æˆåŠŸå¤„ç†æ–‡æ¡£: {doc.metadata['q_id']}")
        except Exception as e:
            logger.warning(f"âš ï¸ å¤„ç†å¼‚å¸¸ {doc.metadata.get('q_id')}: {str(e)}")
    
    logger.info(f"ğŸ¯ å®Œæˆç´¢å¼•æ„å»ºï¼Œæ€»æ–‡æ¡£æ•°: {len(processed)}")
    return FAISS.from_documents(processed, embedding_service)



# -------------------------
# ç´¢å¼•ç®¡ç†ä¸»å‡½æ•°
# -------------------------
def load_or_build_index(
    index_path: Union[str, Path, None] = None
) -> FAISS:
    index_path = Path(index_path) if index_path else settings.DEFAULT_INDEX_PATH
    
    if index_path.exists():
        logger.info(f"ğŸ“‚ åŠ è½½ç°æœ‰ç´¢å¼•: {index_path}")
        return FAISS.load_local(
            folder_path=str(index_path),
            embeddings=embedding_service,  # ä½¿ç”¨å…¨å±€å•ä¾‹
            allow_dangerous_deserialization=True
        )
    
    logger.warning("âš ï¸ æœªæ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œå¼€å§‹æ–°æ„å»ºæµç¨‹...")
    try:
        docs = _load_hf_dataset("rag-datasets/rag-mini-wikipedia")
        index = _build_hybrid_vector_index(docs)
        index.save_local(str(index_path))
        logger.success(f"ğŸ‰ æ–°ç´¢å¼•å·²ä¿å­˜è‡³ {index_path}")
        return index
    except Exception as e:
        logger.critical(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}", stack_info=True)  # æ·»åŠ è¯¦ç»†å †æ ˆ
        raise


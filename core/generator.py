import re
from typing import List, Dict, Optional,Tuple
from RAG_project.core.utils.context_utils import format_context
from RAG_project.config import settings
from RAG_project.config.logger_config import configure_console_logger
from swift.llm import InferRequest, RequestConfig
import os

logger = configure_console_logger(__name__)
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,0.0.0.0'
# -------------------------
# LLM ç”Ÿæˆæ¨¡æ¿é…ç½®
# -------------------------
CONTEXT_PROMPT_TEMPLATE_ZH = '''
ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼š

æ ¼å¼è¦æ±‚ï¼š
1. å›ç­”åŒ…æ‹¬ç­”æ¡ˆå’Œå‚è€ƒæ–‡çŒ®å†…å®¹ï¼Œå…¶ä½™ä¸éœ€è¦
2. å‚è€ƒæ–‡çŒ®éœ€è¦å†…å®¹è€Œä¸æ˜¯æ ‡å·
3. æ— ç›¸å…³è¯æ®æ—¶æ˜ç¡®è¯´æ˜

[æ–‡æœ¬]
{context}

[é—®é¢˜]
{query}

[ç­”æ¡ˆå’Œå‚è€ƒæ–‡çŒ®]
'''

NO_CONTEXT_PROMPT_TEMPLATE_ZH = '''
ç”¨1-2å¥ä¸­æ–‡ç›´æ¥å›ç­”ï¼š
æ ¼å¼è¦æ±‚ï¼š
1. è¯´å®Œç­”æ¡ˆå³å¯
2. ä¸ç¡®å®šæ—¶å»ºè®®ç ”ç©¶æ–¹å‘

é—®é¢˜ï¼š
{query}

ç­”æ¡ˆï¼š
'''

CONTEXT_PROMPT_TEMPLATE_EN = '''
Provide the final answer directly (in English), without including the reasoning process:

Format requirements:
1. Begin with your answer and include the full content of the references at the end
2. References should contain content rather than just numbers
3. Clearly state if there is no relevant evidence

[contexts]
{context}

[Question]
{query}

[Answer And References]
'''

CONTEXT_PROMPT_MMLU_TEMPLATE_EN = '''
You are answering a multiple-choice question from the MMLU (Massive Multitask Language Understanding) benchmark, 
give the answer (in English), without including the reasoning process:

Format requirements:
1. Give your answer like "The answer is X", where X is one of: A, B, C, or D, and include the full content of the chosen option
2. Begin with your answer and include the full content of the references at the end
3. References should contain content rather than just numbers
4. Clearly state if there is no relevant evidence

[contexts]
{context}

[Question]
{query}

[Answer And References]
'''

NO_CONTEXT_PROMPT_MMLU_TEMPLATE_EN = '''
Answer directly in 1-2 sentences (in English):

Format requirements:
1. Do not include any additional text after your answer to the question
2. Suggest research directions if uncertain

Question: {query}

[Answer]
'''

NO_CONTEXT_PROMPT_TEMPLATE_EN = '''
You are answering a multiple-choice question from the MMLU (Massive Multitask Language Understanding) benchmark, answer directly in 1-2 sentences (in English):

Format requirements:
1. Give your answer like "The answer is X", where X is one of: A, B, C, or D, and include the full content of the chosen option
2. Do not include any additional text after your answer to the question
3. Suggest research directions if uncertain

Question: {query}

[Answer]
'''


# -------------------------
# ç”Ÿæˆä¸»å‡½æ•°
# -------------------------
def generate_llm_response(
    query: str,
    context: List[Dict],
    language: str = "en",  
    max_retries: int = settings.MAX_RETRIES,
    context_top_n: int = settings.CONTEXT_TOP_N,
) -> Tuple[Optional[str], Optional[str]]:
    """æ”¯æŒä¸­è‹±æ–‡çš„å…¨åŠŸèƒ½ç‰ˆï¼Œé€šè¿‡ language å‚æ•°åŠ¨æ€åˆ‡æ¢"""
    
    # =============== æ™ºèƒ½æ„é€ Prompt ================
    use_context = bool(context not in (None, [])) 
    context_str = ""  # åˆå§‹åŒ– context_strï¼Œç¡®ä¿åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æœ‰å€¼
    
    if use_context:
        context_str = format_context(context, top_n=context_top_n, language=language)
        if language == "en":
            if settings.IS_MMLU:
                prompt = CONTEXT_PROMPT_MMLU_TEMPLATE_EN.format(context=context_str, query=query)
            else:
                prompt = CONTEXT_PROMPT_TEMPLATE_EN.format(context=context_str, query=query)
        else:
            prompt = CONTEXT_PROMPT_TEMPLATE_ZH.format(context=context_str, query=query)
    else:
        if language == "en":
            if settings.IS_MMLU:
                prompt = NO_CONTEXT_PROMPT_MMLU_TEMPLATE_EN.format(query=query)
                logger.info("No valid context found, enabling generic response mode for MMLU.")
            else:
                prompt = NO_CONTEXT_PROMPT_TEMPLATE_EN.format(query=query)
                logger.info("No valid context found, enabling generic response mode.")
        else:
            prompt = NO_CONTEXT_PROMPT_TEMPLATE_ZH.format(query=query)
            logger.info("æ— æœ‰æ•ˆä¸Šä¸‹æ–‡ï¼Œå¯ç”¨é€šç”¨å›ç­”æ¨¡å¼")
    
    # åŠ è½½æ¨ç†å¼•æ“
    ENGINE = settings.model_manager.get_engine(settings.MODEL_DIR, settings.ADAPTER_DIR, settings.MODEL_TYPE)
    
    # =============== ç”Ÿæˆé€»è¾‘ ================
    for retry in range(max_retries):
        try:
            infer_request = [InferRequest(messages=[{'role': 'user', 'content': prompt}])]
            request_config = RequestConfig(max_tokens=settings.MAX_NEW_TOKEN,temperature=settings.TEMPERATURE)
            resp_list = ENGINE.infer(infer_request, request_config)
            response = resp_list[0].choices[0].message.content
            return context_str, _post_process(response, use_context, language)
        except Exception as e:
            logger.error(f"Generation attempt {retry+1} failed: {e}")
    
    # æ‰€æœ‰é‡è¯•å¤±è´¥åè¿”å›æç¤ºä¿¡æ¯
    return context_str, "æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè¯·ç¨åå†è¯•ã€‚" if language == "zh" else "Unable to generate a response. Please try again later."


def generate_batch_llm_response(
    queries: List[str],
    contexts: List[List[Dict]], # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæŸ¥è¯¢å¯¹åº”ä¸€ç»„ä¸Šä¸‹æ–‡
    language: str = "en",
    max_batch_size: int = settings.MAX_BATCH_SIZE,
) -> List[str]:
    """
    Batch ç”Ÿæˆå‡½æ•°ï¼Œå¤§å¹…æå‡å®éªŒé€Ÿåº¦
    """
    
    # 1. æ‰¹é‡æ„é€  Prompts
    prompts = []
    # è®°å½•æ¯ä¸ªæ ·æœ¬æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡ï¼Œç”¨äºåå¤„ç†
    has_context_flags = [] 

    for query, context in zip(queries, contexts):
        use_context = bool(context not in (None, []))
        has_context_flags.append(use_context)
        
        context_str = ""
        if use_context:
            context_str = format_context(context, top_n=settings.CONTEXT_TOP_N, language=language)
            
        # æ ¹æ®è¯­è¨€å’Œä»»åŠ¡é€‰æ‹©æ¨¡æ¿
        if language == "en":
            if settings.IS_MMLU:
                prompt = CONTEXT_PROMPT_MMLU_TEMPLATE_EN.format(context=context_str, query=query)
            else:
                prompt = CONTEXT_PROMPT_TEMPLATE_EN.format(context=context_str, query=query)
        else:
            prompt = CONTEXT_PROMPT_TEMPLATE_ZH.format(context=context_str, query=query)
            
        # æ— ä¸Šä¸‹æ–‡æ—¶çš„ fallback é€»è¾‘ (ä¸å•æ¡ä¿æŒä¸€è‡´)
        if not use_context:
            if language == "en":
                if settings.IS_MMLU:
                    prompt = NO_CONTEXT_PROMPT_MMLU_TEMPLATE_EN.format(query=query)
                else:
                    prompt = NO_CONTEXT_PROMPT_TEMPLATE_EN.format(query=query)
            else:
                prompt = NO_CONTEXT_PROMPT_TEMPLATE_ZH.format(query=query)
        
        prompts.append(prompt)

    # 2. è·å–å¼•æ“ (å•ä¾‹)
    ENGINE = settings.model_manager.get_engine(settings.MODEL_DIR, settings.ADAPTER_DIR, settings.MODEL_TYPE)
    
    # 3. æ„é€ æ‰¹é‡è¯·æ±‚
    # Swift/vLLM çš„ infer æ¥å£é€šå¸¸æ”¯æŒä¼ å…¥ list[InferRequest]
    infer_requests = [InferRequest(messages=[{'role': 'user', 'content': p}]) for p in prompts]
    request_config = RequestConfig(max_tokens=settings.MAX_NEW_TOKEN, temperature=settings.TEMPERATURE,
                                   )
    
    # 4. æ‰§è¡Œæ‰¹é‡æ¨ç† (æœ€è€—æ—¶çš„æ­¥éª¤ï¼Œç°åœ¨å¹¶è¡Œäº†)
    try:
        resp_list = ENGINE.infer(infer_requests, request_config)
    except Exception as e:
        logger.error(f"Batch generation failed: {e}")
        # å¦‚æœæ‰¹é‡å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²åˆ—è¡¨ä»¥é¿å…ç¨‹åºå´©æºƒ
        return ["" for _ in queries]

    # 5. æ‰¹é‡åå¤„ç†
    final_responses = []
    for i, resp in enumerate(resp_list):
        raw_text = resp.choices[0].message.content
        processed = _post_process(raw_text, has_context_flags[i], language)
        final_responses.append(processed)
        
    return final_responses
    
# å…³é”®é€»è¾‘ï¼šæˆªæ–­æ€ç»´é“¾å†…å®¹ ğŸš€
# =================== æ”¹è¿›ç‰ˆåå¤„ç†æµç¨‹ ===================
def _post_process(raw_response: str, has_context: bool, language: str = "zh") -> str:
    # æ‰¾åˆ° '</think>' åçš„å†…å®¹
    index = raw_response.find('</think>')
    if index != -1:
        raw_response = raw_response[index + len('</think>'):]
    
    if language == "en":
        conclusion_pattern = r'\[Conclusion\]\s*[:ï¼š]?\s*(.*?)\s*(?:\[References\]|Answer|Response)'
    else:
        conclusion_pattern = r'ç»“è®º\s*[:ï¼š]?\s*(.*?)\s*(?:ç­”æ¡ˆ|å›ç­”|åŸå§‹ç­”æ¡ˆ)'
    match = re.search(conclusion_pattern, raw_response, re.DOTALL)
    processed_text = match.group(1).strip() if match else raw_response.strip()

    # ä¿æŒåŸå§‹æ ¼å¼ï¼Œä¸è¿›è¡Œé¢å¤–çš„æ¢è¡Œæˆ–ç©ºæ ¼å¤„ç†
    if language == "en":
        processed_text = f"{processed_text}"
    else:
        processed_text = f"{processed_text}"

    # è¿”å›å¤„ç†åçš„æ–‡æœ¬
    return processed_text




import re
from typing import List, Dict, Optional,Tuple
from RAG_project.core.utils.context_utils import format_context
from RAG_project.config import settings
from RAG_project.config.logger_config import configure_console_logger
from openai import OpenAI
from swift.llm import InferRequest, RequestConfig
import os

logger = configure_console_logger(__name__)
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,0.0.0.0'


# -------------------------
# API å®¢æˆ·ç«¯åˆå§‹åŒ–
# -------------------------
def _get_api_client():
    """æ ¹æ®é…ç½®è¿”å›å¯¹åº”çš„ OpenAI å…¼å®¹å®¢æˆ·ç«¯"""
    if settings.ACTIVE_MODEL_TYPE == "qwen":
        return OpenAI(
            api_key=settings.QWEN_API_CONFIG["api_key"],
            base_url=settings.QWEN_API_CONFIG["base_url"]
        )
    elif settings.ACTIVE_MODEL_TYPE == "deepseek":
        return OpenAI(
            api_key=settings.DEEPSEEK_CONFIG["api_key"],
            base_url=settings.DEEPSEEK_CONFIG["base_url"]
        )
    else:
        return None  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹

def _get_model_name():
    """è¿”å›å½“å‰æ¿€æ´»æ¨¡å‹çš„åç§°"""
    if settings.ACTIVE_MODEL_TYPE == "qwen":
        return settings.QWEN_API_CONFIG["model"]
    elif settings.ACTIVE_MODEL_TYPE == "deepseek":
        return settings.DEEPSEEK_CONFIG["model"]
    else:
        return None


# -------------------------
# LLM ç”Ÿæˆæ¨¡æ¿é…ç½®
# -------------------------
CONTEXT_PROMPT_TEMPLATE_ZH = '''
ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼š

æ ¼å¼è¦æ±‚ï¼š
1. å›ç­”åŒ…æ‹¬ç­”æ¡ˆå’Œå‚è€ƒæ–‡çŒ®å†…å®¹ï¼Œå…¶ä½™ä¸éœ€è¦
2. å‚è€ƒæ–‡çŒ®éœ€è¦å†…å®¹è€Œä¸æ˜¯æ ‡å·
3. æ— ç›¸å…³è¯æ®æ—¶æ˜ç¡®è¯´æ˜

[æ–‡æœ¬]
{context}

[é—®é¢˜]
{query}

[ç­”æ¡ˆå’Œå‚è€ƒæ–‡çŒ®]
'''

NO_CONTEXT_PROMPT_TEMPLATE_ZH = '''
ç”¨1-2å¥ä¸­æ–‡ç›´æ¥å›ç­”ï¼š
æ ¼å¼è¦æ±‚ï¼š
1. è¯´å®Œç­”æ¡ˆå³å¯
2. ä¸ç¡®å®šæ—¶å»ºè®®ç ”ç©¶æ–¹å‘

é—®é¢˜ï¼š
{query}

ç­”æ¡ˆï¼š
'''

CONTEXT_PROMPT_TEMPLATE_EN = '''
Provide the final answer directly (in English), without including the reasoning process:

Format requirements:
1. Begin with your answer and include the full content of the references at the end
2. References should contain content rather than just numbers
3. Clearly state if there is no relevant evidence

[contexts]
{context}

[Question]
{query}

[Answer And References]
'''

CONTEXT_PROMPT_MMLU_TEMPLATE_EN = '''
You are answering a multiple-choice question from the MMLU (Massive Multitask Language Understanding) benchmark, 
give the answer (in English), without including the reasoning process:

Format requirements:
1. Give your answer like "The answer is X", where X is one of: A, B, C, or D, and include the full content of the chosen option
2. Begin with your answer and include the full content of the references at the end
3. References should contain content rather than just numbers
4. Clearly state if there is no relevant evidence

[contexts]
{context}

[Question]
{query}

[Answer And References]
'''

NO_CONTEXT_PROMPT_MMLU_TEMPLATE_EN = '''
Answer directly in 1-2 sentences (in English):

Format requirements:
1. Do not include any additional text after your answer to the question
2. Suggest research directions if uncertain

Question: {query}

[Answer]
'''

NO_CONTEXT_PROMPT_TEMPLATE_EN = '''
You are answering a multiple-choice question from the MMLU (Massive Multitask Language Understanding) benchmark, answer directly in 1-2 sentences (in English):

Format requirements:
1. Give your answer like "The answer is X", where X is one of: A, B, C, or D, and include the full content of the chosen option
2. Do not include any additional text after your answer to the question
3. Suggest research directions if uncertain

Question: {query}

[Answer]
'''


# -------------------------
# ç”Ÿæˆä¸»å‡½æ•°
# -------------------------
def generate_llm_response(
    query: str,
    context: List[Dict],
    language: str = "en",  
    max_retries: int = settings.MAX_RETRIES,
    context_top_n: int = settings.CONTEXT_TOP_N,
) -> Tuple[Optional[str], Optional[str]]:
    """æ”¯æŒä¸­è‹±æ–‡çš„å…¨åŠŸèƒ½ç‰ˆï¼Œé€šè¿‡ language å‚æ•°åŠ¨æ€åˆ‡æ¢"""
    
    # =============== æ™ºèƒ½æ„é€ Prompt ================
    use_context = bool(context not in (None, [])) 
    context_str = ""
    
    if use_context:
        context_str = format_context(context, top_n=context_top_n, language=language)
        if language == "en":
            if settings.IS_MMLU:
                prompt = CONTEXT_PROMPT_MMLU_TEMPLATE_EN.format(context=context_str, query=query)
            else:
                prompt = CONTEXT_PROMPT_TEMPLATE_EN.format(context=context_str, query=query)
        else:
            prompt = CONTEXT_PROMPT_TEMPLATE_ZH.format(context=context_str, query=query)
    else:
        if language == "en":
            if settings.IS_MMLU:
                prompt = NO_CONTEXT_PROMPT_MMLU_TEMPLATE_EN.format(query=query)
                logger.info("No valid context found, enabling generic response mode for MMLU.")
            else:
                prompt = NO_CONTEXT_PROMPT_TEMPLATE_EN.format(query=query)
                logger.info("No valid context found, enabling generic response mode.")
        else:
            prompt = NO_CONTEXT_PROMPT_TEMPLATE_ZH.format(query=query)
            logger.info("æ— æœ‰æ•ˆä¸Šä¸‹æ–‡,å¯ç”¨é€šç”¨å›ç­”æ¨¡å¼")
    
    # =============== ç”Ÿæˆé€»è¾‘ ================
    api_client = _get_api_client()
    
    for retry in range(max_retries):
        try:
            if api_client:  # ä½¿ç”¨ API
                response = api_client.chat.completions.create(
                    model=_get_model_name(),
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=settings.MAX_NEW_TOKEN,
                    temperature=settings.TEMPERATURE
                )
                response_text = response.choices[0].message.content
            else:  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                ENGINE = settings.model_manager.get_engine(
                    settings.MODEL_DIR, 
                    settings.ADAPTER_DIR, 
                    settings.MODEL_TYPE
                )
                infer_request = [InferRequest(messages=[{'role': 'user', 'content': prompt}])]
                request_config = RequestConfig(
                    max_tokens=settings.MAX_NEW_TOKEN,
                    temperature=settings.TEMPERATURE
                )
                resp_list = ENGINE.infer(infer_request, request_config)
                response_text = resp_list[0].choices[0].message.content
            
            return context_str, _post_process(response_text, use_context, language)
            
        except Exception as e:
            logger.error(f"Generation attempt {retry+1} failed: {e}")
    
    return context_str, "æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè¯·ç¨åå†è¯•ã€‚" if language == "zh" else "Unable to generate a response. Please try again later."


def generate_batch_llm_response(
    queries: List[str],
    contexts: List[List[Dict]],
    language: str = "en",
    max_batch_size: int = settings.MAX_BATCH_SIZE,
) -> List[str]:
    """Batch ç”Ÿæˆå‡½æ•°ï¼Œå¤§å¹…æå‡å®éªŒé€Ÿåº¦"""
    
    # 1. æ‰¹é‡æ„é€  Prompts
    prompts = []
    has_context_flags = []

    for query, context in zip(queries, contexts):
        use_context = bool(context not in (None, []))
        has_context_flags.append(use_context)
        
        context_str = ""
        if use_context:
            context_str = format_context(context, top_n=settings.CONTEXT_TOP_N, language=language)
            
        if language == "en":
            if settings.IS_MMLU:
                prompt = CONTEXT_PROMPT_MMLU_TEMPLATE_EN.format(context=context_str, query=query)
            else:
                prompt = CONTEXT_PROMPT_TEMPLATE_EN.format(context=context_str, query=query)
        else:
            prompt = CONTEXT_PROMPT_TEMPLATE_ZH.format(context=context_str, query=query)
            
        if not use_context:
            if language == "en":
                if settings.IS_MMLU:
                    prompt = NO_CONTEXT_PROMPT_MMLU_TEMPLATE_EN.format(query=query)
                else:
                    prompt = NO_CONTEXT_PROMPT_TEMPLATE_EN.format(query=query)
            else:
                prompt = NO_CONTEXT_PROMPT_TEMPLATE_ZH.format(query=query)
        
        prompts.append(prompt)

    # 2. è·å– API å®¢æˆ·ç«¯æˆ–æœ¬åœ°å¼•æ“
    api_client = _get_api_client()
    
    # 3. æ‰§è¡Œæ‰¹é‡æ¨ç†
    try:
        if api_client:  # ä½¿ç”¨ API (é€ä¸ªè°ƒç”¨ï¼Œå› ä¸º OpenAI API ä¸æ”¯æŒçœŸæ­£çš„æ‰¹é‡)
            resp_list = []
            for prompt in prompts:
                response = api_client.chat.completions.create(
                    model=_get_model_name(),
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=settings.MAX_NEW_TOKEN,
                    temperature=settings.TEMPERATURE
                )
                resp_list.append(response.choices[0].message.content)
        else:  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ‰¹é‡æ¨ç†
            ENGINE = settings.model_manager.get_engine(
                settings.MODEL_DIR, 
                settings.ADAPTER_DIR, 
                settings.MODEL_TYPE
            )
            infer_requests = [
                InferRequest(messages=[{'role': 'user', 'content': p}]) 
                for p in prompts
            ]
            request_config = RequestConfig(
                max_tokens=settings.MAX_NEW_TOKEN, 
                temperature=settings.TEMPERATURE
            )
            responses = ENGINE.infer(infer_requests, request_config)
            resp_list = [resp.choices[0].message.content for resp in responses]
            
    except Exception as e:
        logger.error(f"Batch generation failed: {e}")
        return ["" for _ in queries]

    # 4. æ‰¹é‡åå¤„ç†
    final_responses = []
    for i, raw_text in enumerate(resp_list):
        processed = _post_process(raw_text, has_context_flags[i], language)
        final_responses.append(processed)
        
    return final_responses
    
# å…³é”®é€»è¾‘ï¼šæˆªæ–­æ€ç»´é“¾å†…å®¹ ğŸš€
# =================== æ”¹è¿›ç‰ˆåå¤„ç†æµç¨‹ ===================
def _post_process(raw_response: str, has_context: bool, language: str = "zh") -> str:
    # æ‰¾åˆ° '</think>' åçš„å†…å®¹
    index = raw_response.find('</think>')
    if index != -1:
        raw_response = raw_response[index + len('</think>'):]
    
    if language == "en":
        conclusion_pattern = r'\[Conclusion\]\s*[:ï¼š]?\s*(.*?)\s*(?:\[References\]|Answer|Response)'
    else:
        conclusion_pattern = r'ç»“è®º\s*[:ï¼š]?\s*(.*?)\s*(?:ç­”æ¡ˆ|å›ç­”|åŸå§‹ç­”æ¡ˆ)'
    match = re.search(conclusion_pattern, raw_response, re.DOTALL)
    processed_text = match.group(1).strip() if match else raw_response.strip()

    # ä¿æŒåŸå§‹æ ¼å¼ï¼Œä¸è¿›è¡Œé¢å¤–çš„æ¢è¡Œæˆ–ç©ºæ ¼å¤„ç†
    if language == "en":
        processed_text = f"{processed_text}"
    else:
        processed_text = f"{processed_text}"

    # è¿”å›å¤„ç†åçš„æ–‡æœ¬
    return processed_text




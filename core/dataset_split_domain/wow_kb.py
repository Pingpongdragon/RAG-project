"""
ÊûÑÂª∫ WoW ÂÖ®Â±ÄÁü•ËØÜÂ∫ìÔºàÊâÄÊúâÂÄôÈÄâÊñáÊ°£ÔºâÂπ∂Êåâ Domain ÂàÜÁ±ª
Á≠ñÁï•Ôºö
1. ÂÖà‰ªé 500 Êù° triplets ÁöÑÁõ∏ÂÖ≥ÊñáÊ°£‰∏≠ÊèêÂèñ
2. Â¶ÇÊûú‰∏çË∂≥ 2000 Êù°Ôºå‰ªéÂâ©‰ΩôÊï∞ÊçÆ‰∏≠Ë°•ÂÖÖÂà∞ 2000 Êù°
ËæìÂá∫Ôºöwow_kb/{0_entertainment, 1_stem, 2_humanities, 3_lifestyle}.jsonl
"""
import json, hashlib
from pathlib import Path
from tqdm import tqdm
import os

HERE = Path(__file__).parent
TRIPLET_DIR = HERE / "wow_triplets"
OUTPUT_DIR = HERE / "wow_kb"

# KB Â§ßÂ∞èÈÖçÁΩÆ
TARGET_KB_SIZE = 2000

# È¢ÜÂüüÂÖ≥ÈîÆËØçÔºàÁî®‰∫éÂàÜÁ±ªÔºâ
KEYWORDS = {
    "entertainment": ["music", "movie", "tv", "film", "actor", "actress", "celebrity", "game", "comic", "fiction", "beatles", "pop", "song", "album", "band", "xbox", "nintendo", "video game"],
    "stem": ["science", "technology", "physics", "biology", "chemistry", "computer", "internet", "space", "nasa", "machine", "robot", "species", "formula", "theory", "software", "engineering", "mathematics"],
    "humanities": ["history", "politics", "war", "battle", "army", "empire", "king", "queen", "president", "minister", "art", "literature", "writer", "philosophy", "religion", "democracy", "dynasty"],
    "lifestyle": ["sport", "football", "basketball", "baseball", "olympic", "league", "team", "coach", "food", "cooking", "fashion", "travel", "pet", "hobby", "garden", "car", "fitness"]
}

DOMAIN_NAMES = ["entertainment", "stem", "humanities", "lifestyle"]

# È¢ÜÂüüÂà∞Êï∞Â≠óÁ¥¢ÂºïÁöÑÊò†Â∞Ñ
DOMAIN_TO_INDEX = {
    "entertainment": 0,
    "stem": 1,
    "humanities": 2,
    "lifestyle": 3
}

def get_domain(text):
    """Ê†πÊçÆÂÖ≥ÈîÆËØçÂà§Êñ≠ÊñáÊú¨Â±û‰∫éÂì™‰∏™È¢ÜÂüü"""
    if not text:
        return None
    
    text_lower = text.lower()
    scores = {}
    
    for domain, keywords in KEYWORDS.items():
        score = sum(1 for kw in keywords if kw in text_lower)
        scores[domain] = score
    
    # ÊâæÂà∞ÂæóÂàÜÊúÄÈ´òÁöÑÈ¢ÜÂüü
    best_domain = max(scores, key=scores.get)
    
    # Â¶ÇÊûúÊúÄÈ´òÂàÜ‰∏∫ 0ÔºåËøîÂõû None
    return best_domain if scores[best_domain] > 0 else None

def download_wow():
    """‰ªé HuggingFace Âä†ËΩΩ WoW Êï∞ÊçÆÈõÜ"""
    print("üì• Âä†ËΩΩ Wizard of Wikipedia Êï∞ÊçÆÈõÜ...")
    try:
        from datasets import load_dataset
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
        
        dataset = load_dataset("chujiezheng/wizard_of_wikipedia", split="train")
        print(f"‚úÖ Âä†ËΩΩÂÆåÊàêÔºåÂÖ± {len(dataset)} ‰∏™Ê†∑Êú¨")
        return dataset
        
    except Exception as e:
        print(f"‚ùå Âä†ËΩΩÂ§±Ë¥•: {e}")
        exit(1)

def process():
    """ÊûÑÂª∫ WoW Áü•ËØÜÂ∫ì"""
    
    # ===== Á¨¨‰∏ÄÊ≠•ÔºöÊî∂ÈõÜ‰∏âÂÖÉÁªÑ‰∏≠‰ΩøÁî®ÁöÑ topics Âíå gold_docs =====
    print("üìã Step 1: Loading triplets to extract used topics and gold docs...")
    used_topics = {d: set() for d in DOMAIN_NAMES}
    used_gold_docs = {d: set() for d in DOMAIN_NAMES}
    
    if TRIPLET_DIR.exists():
        for domain in DOMAIN_NAMES:
            domain_index = DOMAIN_TO_INDEX[domain]
            triplet_file = TRIPLET_DIR / f"{domain_index}_{domain}.jsonl"
            if triplet_file.exists():
                with open(triplet_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        item = json.loads(line)
                        used_topics[domain].add(item.get("topic", ""))
                        # Ê∑ªÂä†ÊâÄÊúâ gold_docs
                        for doc in item.get("gold_docs", []):
                            used_gold_docs[domain].add(doc)
        
        total_topics = sum(len(t) for t in used_topics.values())
        total_gold_docs = sum(len(g) for g in used_gold_docs.values())
        print(f"   Found {total_topics} used topics, {total_gold_docs} gold docs")
    else:
        print("   ‚ö†Ô∏è Triplet directory not found!")
        return
    
    # ===== Á¨¨‰∫åÊ≠•ÔºöÂä†ËΩΩÊï∞ÊçÆÈõÜ =====
    data = download_wow()
    
    # ===== Á¨¨‰∏âÊ≠•ÔºöÂÖàÊèêÂèñ‰∏âÂÖÉÁªÑÁõ∏ÂÖ≥ÁöÑÊñáÊ°£ =====
    print("\nüîÑ Step 2: Extracting docs from triplet topics...")
    buckets = {d: {} for d in DOMAIN_NAMES}
    used_sample_ids = {d: set() for d in DOMAIN_NAMES}  # ËÆ∞ÂΩïÊØè‰∏™ domain Â∑≤‰ΩøÁî®ÁöÑÊ†∑Êú¨
    
    for idx, sample in enumerate(tqdm(data, desc="Processing triplet docs")):
        topics_list = sample.get('topics', [])
        knowledge_list = sample.get('knowledge', [])
        
        # Ëé∑ÂèñËØ•ÂØπËØùÁöÑ‰∏ª topicÔºàÈÄöÂ∏∏ÊòØÁ¨¨‰∏Ä‰∏™Ôºâ
        main_topic = topics_list[0] if topics_list else ""
        
        # Âà§Êñ≠ËØ• topic Â±û‰∫éÂì™‰∏™ domain
        domain = get_domain(main_topic)
        if domain is None:
            continue
        
        # Âè™Â§ÑÁêÜ‰∏âÂÖÉÁªÑ‰∏≠‰ΩøÁî®ÁöÑ topic
        if main_topic not in used_topics[domain]:
            continue
        
        used_sample_ids[domain].add(idx)
        
        # ÊèêÂèñËØ•ÂØπËØù‰∏≠ÁöÑÊâÄÊúâÁü•ËØÜÊÆµËêΩ
        for knowledge_passages in knowledge_list:
            for passage in knowledge_passages:
                if not passage or "no_passages_used" in passage.lower():
                    continue
                
                # ‰ΩøÁî® passage ‰Ωú‰∏∫ÊñáÊ°£ÂÜÖÂÆπ
                doc_id = hashlib.md5(passage.encode()).hexdigest()
                
                if doc_id not in buckets[domain]:
                    buckets[domain][doc_id] = {
                        "doc_id": doc_id,
                        "dataset": "wow",
                        "domain": domain,
                        "title": main_topic,
                        "text": passage,
                        "source": "from_triplet"
                    }
    
    # ===== Á¨¨ÂõõÊ≠•ÔºöÈ™åËØÅ gold docs ÊòØÂê¶ÈÉΩÂú® KB ‰∏≠ =====
    print("\nüîç Step 3: Verifying gold docs coverage...")
    for domain in DOMAIN_NAMES:
        texts_in_kb = {doc["text"] for doc in buckets[domain].values()}
        missing = used_gold_docs[domain] - texts_in_kb
        if missing:
            print(f"   ‚ö†Ô∏è {domain}: {len(missing)} gold docs missing, adding them...")
            # Â∞ÜÁº∫Â§±ÁöÑ gold docs Ê∑ªÂä†Âà∞ KB
            for gold_doc in missing:
                doc_id = hashlib.md5(gold_doc.encode()).hexdigest()
                if doc_id not in buckets[domain]:
                    # Â∞ùËØï‰ªé used_topics ‰∏≠Êâæ‰∏Ä‰∏™ topic ‰Ωú‰∏∫ title
                    topic = list(used_topics[domain])[0] if used_topics[domain] else "unknown"
                    buckets[domain][doc_id] = {
                        "doc_id": doc_id,
                        "dataset": "wow",
                        "domain": domain,
                        "title": topic,
                        "text": gold_doc,
                        "source": "from_triplet"
                    }
        else:
            print(f"   ‚úÖ {domain}: All gold docs covered")
    
    # ===== Á¨¨‰∫îÊ≠•ÔºöÂ¶ÇÊûú‰∏çË∂≥ 2000 Êù°Ôºå‰ªéÂâ©‰ΩôÊï∞ÊçÆ‰∏≠Ë°•ÂÖÖ =====
    print("\nüîÑ Step 4: Filling KB to target size (2000 per domain)...")
    for domain in DOMAIN_NAMES:
        current_size = len(buckets[domain])
        print(f"   {domain}: {current_size} docs from triplets", end="")
        
        if current_size >= TARGET_KB_SIZE:
            print(" ‚úÖ (already sufficient)")
            continue
        
        needed = TARGET_KB_SIZE - current_size
        print(f", need {needed} more...")
        
        added = 0
        for idx, sample in enumerate(data):
            if added >= needed:
                break
            
            # Ë∑≥ËøáÂ∑≤‰ΩøÁî®ÁöÑÊ†∑Êú¨
            if idx in used_sample_ids[domain]:
                continue
            
            topics_list = sample.get('topics', [])
            main_topic = topics_list[0] if topics_list else ""
            
            # Âà§Êñ≠ domain
            sample_domain = get_domain(main_topic)
            if sample_domain != domain:
                continue
            
            knowledge_list = sample.get('knowledge', [])
            
            for knowledge_passages in knowledge_list:
                if added >= needed:
                    break
                for passage in knowledge_passages:
                    if added >= needed:
                        break
                    if not passage or "no_passages_used" in passage.lower():
                        continue
                    
                    doc_id = hashlib.md5(passage.encode()).hexdigest()
                    
                    if doc_id not in buckets[domain]:
                        buckets[domain][doc_id] = {
                            "doc_id": doc_id,
                            "dataset": "wow",
                            "domain": domain,
                            "title": main_topic,
                            "text": passage,
                            "source": "filler"
                        }
                        added += 1
            
            used_sample_ids[domain].add(idx)
        
        print(f"      ‚Üí Added {added} filler docs, total: {len(buckets[domain])}")
    
    # ===== Á¨¨ÂÖ≠Ê≠•Ôºö‰øùÂ≠ò =====
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    print("\nüíæ Step 5: Saving KB...")
    for domain in DOMAIN_NAMES:
        domain_index = DOMAIN_TO_INDEX[domain]
        path = OUTPUT_DIR / f"{domain_index}_{domain}.jsonl"
        with open(path, 'w', encoding='utf-8') as f:
            for doc in buckets[domain].values():
                f.write(json.dumps(doc, ensure_ascii=False) + '\n')
        
        from_triplet = sum(1 for d in buckets[domain].values() if d.get("source") == "from_triplet")
        filler = sum(1 for d in buckets[domain].values() if d.get("source") == "filler")
        print(f"‚úÖ {domain}: {len(buckets[domain])} docs ({from_triplet} from triplets, {filler} fillers) ‚Üí {path}")

if __name__ == "__main__":
    process()
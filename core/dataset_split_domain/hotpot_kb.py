"""
ÊûÑÂª∫ HotpotQA ÂÖ®Â±ÄÁü•ËØÜÂ∫ìÔºàÊâÄÊúâÂÄôÈÄâÊñáÊ°£ÔºâÂπ∂Êåâ Domain ÂàÜÁ±ª
Á≠ñÁï•Ôºö
1. ÂÖà‰ªé 500 Êù° triplets ÁöÑ context ‰∏≠ÊèêÂèñÊñáÊ°£
2. Â¶ÇÊûú‰∏çË∂≥ 5000 Êù°Ôºå‰ªéÂâ©‰ΩôÊï∞ÊçÆ‰∏≠Ë°•ÂÖÖÂà∞ 5000 Êù°
ËæìÂá∫Ôºöhotpot_kb/{0_entertainment, 1_stem, 2_humanities, 3_lifestyle}.jsonl
"""
import json, hashlib
from pathlib import Path
from tqdm import tqdm

HERE = Path(__file__).parent
HOTPOT_FILE = HERE / "hotpot_train_v1.1.json"
TRIPLET_DIR = HERE / "hotpot_triplets"
OUTPUT_DIR = HERE / "hotpot_kb"

# KB Â§ßÂ∞èÈÖçÁΩÆ
TARGET_KB_SIZE = 8000  # ÊØè‰∏™ domain ÁöÑÁõÆÊ†á KB Â§ßÂ∞è

KEYWORDS = {
    0: ["music", "movie", "tv", "film", "actor", "actress", "celebrity", "game", "comic", "fiction", "beatles", "pop", "song", "album", "band", "xbox", "nintendo"],
    1: ["science", "technology", "physics", "biology", "chemistry", "computer", "internet", "space", "nasa", "machine", "robot", "species", "formula", "theory", "software", "engineering"],
    2: ["history", "politics", "war", "battle", "army", "empire", "king", "queen", "president", "minister", "art", "literature", "writer", "philosophy", "religion", "democracy", "dynasty"],
    3: ["sport", "football", "basketball", "baseball", "olympic", "league", "team", "coach", "food", "cooking", "fashion", "travel", "pet", "hobby", "garden", "car", "fitness"]
}

DOMAIN_NAMES = {0: "0_entertainment", 1: "1_stem", 2: "2_humanities", 3: "3_lifestyle"}

def get_domain(text):
    if not text: return None
    t = text.lower()
    scores = {k: sum(1 for kw in kws if kw in t) for k, kws in KEYWORDS.items()}
    best = max(scores, key=scores.get)
    return best if scores[best] > 0 else None

def download_hotpot():
    """Ëá™Âä®‰∏ãËΩΩ HotpotQA Êï∞ÊçÆÈõÜ"""
    if HOTPOT_FILE.exists():
        print(f"‚úÖ {HOTPOT_FILE} already exists, skip download.")
        return
    
    print("üì• Downloading HotpotQA from HuggingFace...")
    try:
        from datasets import load_dataset
        import os
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
        
        dataset = load_dataset("hotpot_qa", "distractor", split="train")
        
        data = []
        for item in tqdm(dataset, desc="Converting"):
            entry = {
                "_id": item["id"],
                "question": item["question"],
                "answer": item["answer"],
                "type": item["type"],
                "context": [
                    [title, sentences] 
                    for title, sentences in zip(item["context"]["title"], item["context"]["sentences"])
                ],
                "supporting_facts": [
                    [title, sent_id]
                    for title, sent_id in zip(item["supporting_facts"]["title"], item["supporting_facts"]["sent_id"])
                ]
            }
            data.append(entry)
        
        print(f"üíæ Saving to {HOTPOT_FILE}...")
        with open(HOTPOT_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Downloaded {len(data)} samples")
        
    except Exception as e:
        print(f"‚ùå Download failed: {e}")
        exit(1)

def process():
    download_hotpot()
    
    if not HOTPOT_FILE.exists():
        print(f"‚ùå {HOTPOT_FILE} not found!")
        return
    
    # ===== Step 1: Êî∂ÈõÜ‰∏âÂÖÉÁªÑ‰ΩøÁî®ÁöÑ query IDs Âíå supporting_facts =====
    print("üìã Step 1: Loading triplets to get used query IDs and supporting facts...")
    used_triplet_ids = set()
    triplet_gold_docs = {}  # Â≠òÂÇ®ÊØè‰∏™tripletÁöÑgold_docs
    
    if TRIPLET_DIR.exists():
        for dom_id in range(4):
            triplet_file = TRIPLET_DIR / f"{DOMAIN_NAMES[dom_id]}.jsonl"
            if triplet_file.exists():
                with open(triplet_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        item = json.loads(line)
                        triplet_id = item["triplet_id"]
                        used_triplet_ids.add(triplet_id)
                        # ‰øùÂ≠òsupporting_factsÁî®‰∫éÊ†áËÆ∞gold docs
                        if "gold_docs" in item:
                            triplet_gold_docs[triplet_id] = item["gold_docs"]
        print(f"   Found {len(used_triplet_ids)} triplets")
    else:
        print("   ‚ö†Ô∏è Triplet directory not found!")
        return
    
    # ===== Step 2: Âä†ËΩΩ HotpotQA =====
    print(f"üìñ Step 2: Loading {HOTPOT_FILE}...")
    data = json.load(open(HOTPOT_FILE, encoding="utf-8"))
    
    buckets = {i: {} for i in range(4)}
    used_entries = set()
    
    # ===== Step 3: ‰ªé triplet Áõ∏ÂÖ≥ÁöÑ query ‰∏≠ÊèêÂèñÊñáÊ°£ =====
    print("üîÑ Step 3: Extracting docs from triplet queries...")
    for entry in tqdm(data):
        entry_id = entry.get("_id", "")[:12]
        
        if entry_id not in used_triplet_ids:
            continue
        
        used_entries.add(entry_id)
        
        # Ëé∑ÂèñËØ•tripletÁöÑgold_docs
        gold_docs = triplet_gold_docs.get(entry_id, [])
        # gold_docsÊòØÊñáÊ°£ÊñáÊú¨ÂàóË°®Ôºå‰ΩøÁî®Ââç50‰∏™Â≠óÁ¨¶‰Ωú‰∏∫Ê†áËØÜ
        gold_prefixes = {doc[:50] for doc in gold_docs if isinstance(doc, str)}
        
        context = entry.get("context", [])
        
        for title, sentences in context:
            full_text = " ".join(sentences).strip()
            
            if not full_text:
                continue
            
            # ‚úÖ Áî® title + Ââç300Â≠óÁ¨¶ÂàÜÁ±ªÔºàÈÅøÂÖçÊñáÊú¨ËøáÈïøÔºâ
            doc_snippet = f"{title} {full_text[:300]}"
            dom = get_domain(doc_snippet)
            if dom is None:
                continue
            
            # ‰ΩøÁî® title ‰Ωú‰∏∫ÊñáÊ°£ ID
            doc_id = hashlib.md5(title.encode()).hexdigest()
            
            # üîß Ê†áËÆ∞ÊòØÂê¶‰∏∫gold doc
            is_gold = any(full_text.startswith(prefix) for prefix in gold_prefixes)
            
            if doc_id not in buckets[dom]:
                buckets[dom][doc_id] = {
                    "doc_id": doc_id,
                    "dataset": "hotpotqa",
                    "domain": DOMAIN_NAMES[dom],
                    "title": title,  # ‚úÖ ‰øùÁïôtitleÁî®‰∫éÂåπÈÖç
                    "text": full_text,
                    "source": "from_triplet",
                    "is_gold_doc": is_gold,  # ‚úÖ Ê†áËÆ∞gold doc
                    "from_triplet_id": entry_id  # ‚úÖ ËÆ∞ÂΩïÊù•Ê∫êtriplet
                }
        
    # ===== Step 4: Ë°•ÂÖÖÂà∞ÁõÆÊ†áÊï∞Èáè =====
    print("\nüîÑ Step 4: Filling KB to target size...")
    for dom_id in range(4):
        current_size = len(buckets[dom_id])
        print(f"   {DOMAIN_NAMES[dom_id]}: {current_size} docs from triplets", end="")
        
        if current_size >= TARGET_KB_SIZE:
            print(" ‚úÖ (already sufficient)")
            if current_size > TARGET_KB_SIZE:
                import random
                docs_list = list(buckets[dom_id].values())
                sampled = random.sample(docs_list, TARGET_KB_SIZE)
                buckets[dom_id] = {d["doc_id"]: d for d in sampled}
                print(f" ‚Üí Sampled to {TARGET_KB_SIZE}")
            continue
        
        needed = TARGET_KB_SIZE - current_size
        print(f", need {needed} more...")
        
        added = 0
        for entry in data:
            if added >= needed:
                break
            
            entry_id = entry.get("_id", "")[:12]
            if entry_id in used_entries:
                continue
            
            context = entry.get("context", [])
            for title, sentences in context:
                if added >= needed:
                    break
                
                full_text = " ".join(sentences).strip()
                if not full_text:
                    continue
                
                # ‚úÖ Áî® title + Ââç300Â≠óÁ¨¶ÂàÜÁ±ª
                doc_snippet = f"{title} {full_text[:300]}"
                dom = get_domain(doc_snippet)
                if dom != dom_id:
                    continue
                
                doc_id = hashlib.md5(title.encode()).hexdigest()
                
                if doc_id not in buckets[dom_id]:
                    buckets[dom_id][doc_id] = {
                        "doc_id": doc_id,
                        "dataset": "hotpotqa",
                        "domain": DOMAIN_NAMES[dom_id],
                        "title": title,  # ‚úÖ ‰øùÁïôtitle
                        "text": full_text,
                        "source": "filler",
                        "is_gold_doc": False,  # ‚úÖ fillerÊñáÊ°£‰∏çÊòØgold
                        "from_triplet_id": None
                    }
                    added += 1
            
            used_entries.add(entry_id)
        
        print(f"      ‚Üí Added {added} filler docs, total: {len(buckets[dom_id])}")
    
    # ===== Step 5: ‰øùÂ≠ò =====
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    print("\nüíæ Step 5: Saving KB...")
    for dom_id, docs_map in buckets.items():
        path = OUTPUT_DIR / f"{DOMAIN_NAMES[dom_id]}.jsonl"
        with open(path, 'w', encoding='utf-8') as f:
            for doc in docs_map.values():
                f.write(json.dumps(doc, ensure_ascii=False) + '\n')
        
        # ÁªüËÆ°
        from_triplet_count = sum(1 for d in docs_map.values() if d.get("source") == "from_triplet")
        gold_doc_count = sum(1 for d in docs_map.values() if d.get("is_gold_doc") == True)
        filler_count = sum(1 for d in docs_map.values() if d.get("source") == "filler")
        
        print(f"‚úÖ {DOMAIN_NAMES[dom_id]}: {len(docs_map)} docs "
              f"({from_triplet_count} from triplets, {gold_doc_count} gold docs, {filler_count} fillers) ‚Üí {path}")
    
    print("\n" + "="*70)
    print("üìä Summary:")
    print("="*70)
    total = sum(len(docs_map) for docs_map in buckets.values())
    total_gold = sum(sum(1 for d in docs_map.values() if d.get("is_gold_doc") == True) for docs_map in buckets.values())
    print(f"Total KB documents: {total}")
    print(f"Total gold documents: {total_gold}")
    print(f"Target per domain: {TARGET_KB_SIZE}")
    
if __name__ == "__main__":
    process()
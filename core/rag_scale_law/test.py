import time
import json
from pathlib import Path
from typing import List, Dict

try:
    import psutil
except ImportError:
    psutil = None
    print("âš ï¸ æœªå®‰è£… psutilï¼Œå†…å­˜å ç”¨ç»Ÿè®¡å°†ç¦ç”¨ã€‚è¯·è¿è¡Œ: pip install psutil")

from tqdm import tqdm
import sys
ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(ROOT))

# é¡¹ç›®å†…ä¾èµ–
from kb_base import ClusteredKnowledgeBase, load_kb_documents
from evaluator import load_test_data
from RAG_project.models.embeddings import embedding_service


def build_kb(doc_pool: Dict[str, List], total_capacity: int) -> ClusteredKnowledgeBase:
    """
    æ ¹æ®æ€»å®¹é‡æ„å»º KBï¼ˆå¹³å‡åˆ†é…åˆ°å„ domainï¼‰
    """
    kb = ClusteredKnowledgeBase(capacity=total_capacity)
    per_domain_cap = total_capacity // 4

    for domain, docs in doc_pool.items():
        # åªå–æ¯åŸŸçš„å‰ per_domain_cap ä¸ªæ–‡æ¡£
        for doc in docs[:per_domain_cap]:
            kb.add_document(doc, step=0)

    return kb


def measure_latency(kb: ClusteredKnowledgeBase, queries: List[Dict], query_embeddings) -> float:
    """
    ä»…æµ‹æ£€ç´¢è€—æ—¶ï¼ˆä¸åŒ…å« embedding ç¼–ç ï¼‰ï¼Œè¿”å›å¹³å‡æ¯«ç§’/æŸ¥è¯¢
    """
    latencies_ms = []

    # é¢„çƒ­ï¼Œé¿å…é¦–æ¬¡å¼€é”€å½±å“ç»“æœ
    warmup = min(50, len(queries))
    for i in range(warmup):
        _ = kb.search(query_embeddings[i], queries[i]["domain"], step=i, top_k=10)

    # æ­£å¼æµ‹é‡
    for i, q in enumerate(tqdm(queries, desc="Measuring retrieval latency")):
        qv = query_embeddings[i]
        t0 = time.perf_counter()
        _ = kb.search(qv, q["domain"], step=i, top_k=10)
        t1 = time.perf_counter()
        latencies_ms.append((t1 - t0) * 1000)

    avg_ms = sum(latencies_ms) / len(latencies_ms) if latencies_ms else 0.0
    return avg_ms


def measure_memory_after_build(kb_before_bytes: int) -> int:
    """
    è¿”å›æ„å»ºåè¿›ç¨‹çš„ RSSï¼ˆå­—èŠ‚ï¼‰ã€‚éœ€è¦ psutilã€‚
    """
    if psutil is None:
        return -1
    proc = psutil.Process()
    rss = proc.memory_info().rss  # bytes
    return rss - kb_before_bytes


def auto_capacity_list(doc_pool: Dict[str, List]) -> List[int]:
    """
    æ ¹æ®æ•°æ®æ± è‡ªåŠ¨ç”Ÿæˆå®¹é‡åˆ—è¡¨ï¼ˆæ€»å®¹é‡ï¼‰ï¼Œé¿å…è¶…è¿‡å¯ç”¨æ–‡æ¡£æ•°
    ä¾‹å¦‚ï¼šæ€»å¯ç”¨ 32k â†’ [8k, 16k, 24k, 32k]
    """
    min_per_domain = min(len(docs) for docs in doc_pool.values())
    max_total = 4 * min_per_domain

    steps = [0.25, 0.5, 0.75, 1.0]
    sizes = sorted({int(max_total * s) for s in steps if int(max_total * s) > 0})
    return sizes


def benchmark():
    print("ğŸ“š åŠ è½½æ–‡æ¡£æ± ...")
    doc_pool = load_kb_documents()

    # è‡ªåŠ¨ç”Ÿæˆå®¹é‡åˆ—è¡¨
    capacities = auto_capacity_list(doc_pool)
    print(f"ğŸ”§ æµ‹è¯•æ€»å®¹é‡: {capacities} (å¹³å‡æ¯åŸŸ = æ€»å®¹é‡/4)")

    # åŠ è½½å›ºå®šæŸ¥è¯¢é›†å¹¶æ‰¹é‡ç¼–ç ï¼ˆé¿å…è®¡å…¥ç¼–ç æ—¶é—´ï¼‰
    print("ğŸ” åŠ è½½æŸ¥è¯¢æ•°æ®å¹¶ç¼–ç ï¼ˆsudden shiftï¼Œ500 æ¡ï¼‰...")
    queries = load_test_data(shift_type="sudden")
    query_texts = [q["query"] for q in queries]
    query_embeddings = embedding_service.encode(
        query_texts,
        batch_size=32,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True
    )

    # ç»Ÿè®¡ç£ç›˜å ç”¨ï¼ˆæ•°æ®é›†åŸå§‹ KB æ–‡ä»¶æ€»å¤§å°ï¼Œä»…ä¾›å‚è€ƒï¼‰
    kb_dir = Path(__file__).parent / "dataset_split_domain" / "hotpot_kb"
    disk_bytes = 0
    if kb_dir.exists():
        for p in kb_dir.glob("*.jsonl"):
            disk_bytes += p.stat().st_size

    results = []

    # åŸºå‡†è¿›ç¨‹å†…å­˜
    base_rss = 0
    if psutil:
        base_rss = psutil.Process().memory_info().rss

    for total_cap in capacities:
        print(f"\n==============================================")
        print(f"âš™ï¸ æ„å»º KBï¼Œæ€»å®¹é‡={total_cap}ï¼ˆæ¯åŸŸâ‰ˆ{total_cap//4}ï¼‰")

        kb = build_kb(doc_pool, total_cap)

        # æµ‹æ£€ç´¢å¹³å‡è€—æ—¶
        avg_ms = measure_latency(kb, queries, query_embeddings)

        # æµ‹å†…å­˜å ç”¨ï¼ˆæ„å»ºåç›¸å¯¹åŸºçº¿çš„å¢é‡ï¼‰
        mem_delta_bytes = measure_memory_after_build(base_rss) if psutil else -1

        row = {
            "total_capacity": total_cap,
            "per_domain_capacity": total_cap // 4,
            "avg_latency_ms_per_query": round(avg_ms, 3),
            "memory_delta_MB": round(mem_delta_bytes / (1024 * 1024), 2) if mem_delta_bytes >= 0 else None,
            "kb_disk_usage_MB_reference": round(disk_bytes / (1024 * 1024), 2) if disk_bytes > 0 else None
        }
        results.append(row)

        print(f"âœ… ç»“æœ: {row}")

    # ä¿å­˜ç»“æœ
    out_path = Path(__file__).parent / "kb_benchmark_results.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜ â†’ {out_path}")
    print("ğŸ“ˆ è¯·å°† avg_latency_ms_per_query ä¸ memory_delta_MB ç”»æˆä¸¤å¼ æ›²çº¿ï¼ˆéšå®¹é‡é€’å¢ï¼‰")


if __name__ == "__main__":
    benchmark()
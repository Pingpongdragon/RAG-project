import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
from typing import Dict, List, Optional, Tuple
from collections import Counter, deque
from scipy.spatial.distance import jensenshannon
from scipy.stats import entropy
from dataclasses import dataclass, field
import logging
import json

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==========================================
# 0. æ ‡ç­¾æ˜ å°„ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
# ==========================================
LABEL_MAP = {
    "entertainment": 0,
    "stem": 1,
    "humanities": 2,
    "lifestyle": 3
}
ID2LABEL = {v: k for k, v in LABEL_MAP.items()}

@dataclass
class DetectionResult:
    step: int
    query: str
    
    # Individual é¢„æµ‹
    predicted_domain: str
    confidence: float
    calibrated_probs: Dict[str, float]  # æ ¡å‡†åçš„æ¦‚ç‡
    entropy: float  # é¢„æµ‹ç†µï¼ˆé«˜ç†µ = OOD ä¿¡å·ï¼‰
    
    # Global Shift æ£€æµ‹
    is_global_shift: bool = False
    jsd_score: float = 0.0  # JS æ•£åº¦
    psi_score: float = 0.0  # PSI æŒ‡æ ‡
    query_distribution: Dict[str, float] = field(default_factory=dict)
    
    # Local Shift æ£€æµ‹ï¼ˆé¢†åŸŸå†…ç²¾åº¦ä¸‹é™ï¼‰
    local_accuracy: Dict[str, float] = field(default_factory=dict)
    local_shifts: Dict[str, bool] = field(default_factory=dict)

# ==========================================
# 1. Temperature Scaling æ ¡å‡†å™¨
# ==========================================
class TemperatureScaling(nn.Module):
    """
    æ¨¡å‹æ ¡å‡†ï¼šè§£å†³å°æ¨¡å‹è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜
    å‚è€ƒè®ºæ–‡ï¼šOn Calibration of Modern Neural Networks (Guo et al., ICML 2017)
    """
    def __init__(self):
        super().__init__()
        self.temperature = nn.Parameter(torch.ones(1) * 1.5)  # åˆå§‹æ¸©åº¦

    def forward(self, logits):
        return logits / self.temperature

    def calibrate(self, model, val_loader, device, max_iter=50):
        """
        åœ¨éªŒè¯é›†ä¸Šå­¦ä¹ æœ€ä¼˜æ¸©åº¦å‚æ•°
        """
        nll_criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=max_iter)
        
        def eval_loss():
            optimizer.zero_grad()
            loss = 0.0
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(device)
                    mask = batch['attention_mask'].to(device)
                    labels = batch['hard_label'].to(device)
                    
                    logits = model(input_ids, attention_mask=mask).logits
                    loss += nll_criterion(self.forward(logits), labels)
            
            loss.backward()
            return loss
        
        optimizer.step(eval_loss)
        logger.info(f"âœ… æ ¡å‡†å®Œæˆï¼Œæœ€ä¼˜æ¸©åº¦: T = {self.temperature.item():.3f}")
        return self.temperature.item()

# ==========================================
# 2. RAG Drift Detector
# ==========================================
class RAGDriftDetector:
    def __init__(
        self,
        model_path: str,
        kb_distribution: Dict[str, float],  # RAG çŸ¥è¯†åº“çš„å…ˆéªŒåˆ†å¸ƒ
        global_window_size: int = 100,
        local_window_size: int = 30,
        jsd_threshold: float = 0.1,  # JS æ•£åº¦é˜ˆå€¼
        psi_threshold: float = 0.2,  # PSI é˜ˆå€¼
        local_acc_drop_threshold: float = 0.15,  # é¢†åŸŸå†…ç²¾åº¦ä¸‹é™é˜ˆå€¼
        ood_entropy_threshold: float = 1.2,  # OOD æ£€æµ‹çš„ç†µé˜ˆå€¼
        use_calibration: bool = True
    ):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.domains = list(LABEL_MAP.keys())
        
        # ========== 1. åŠ è½½è’¸é¦æ¨¡å‹ ==========
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model.eval()
        
        # ========== 2. æ¨¡å‹æ ¡å‡†ï¼ˆå¯é€‰ï¼‰ ==========
        self.use_calibration = use_calibration
        if use_calibration:
            self.temperature_scaler = TemperatureScaling().to(self.device)
            # å¦‚æœæœ‰éªŒè¯é›†ï¼Œå¯ä»¥åœ¨åˆå§‹åŒ–æ—¶è°ƒç”¨ï¼š
            # self.temperature_scaler.calibrate(self.model, val_loader, self.device)
        
        # ========== 3. KB å…ˆéªŒåˆ†å¸ƒ ==========
        total = sum(kb_distribution.values())
        self.kb_dist = {k: v / total for k, v in kb_distribution.items()}
        logger.info(f"ğŸ“Š RAG çŸ¥è¯†åº“åˆ†å¸ƒ: {self.kb_dist}")
        
        # ========== 4. Global Shift çŠ¶æ€ ==========
        self.global_window = deque(maxlen=global_window_size)  # å­˜å‚¨æ ¡å‡†åçš„æ¦‚ç‡å‘é‡
        self.jsd_threshold = jsd_threshold
        self.psi_threshold = psi_threshold
        self.ood_entropy_threshold = ood_entropy_threshold
        
        # ========== 5. Local Shift çŠ¶æ€ï¼ˆæ¯ä¸ªé¢†åŸŸç‹¬ç«‹è·Ÿè¸ªï¼‰ ==========
        self.local_window_size = local_window_size
        self.local_windows = {d: deque(maxlen=local_window_size) for d in self.domains}
        self.local_baseline_acc = {d: 0.85 for d in self.domains}  # åˆå§‹åŸºçº¿
        self.local_acc_drop_threshold = local_acc_drop_threshold
        
        # ========== 6. å†·å´æœŸï¼ˆé¿å…é¢‘ç¹æŠ¥è­¦ï¼‰ ==========
        self.global_cooldown = 0
        self.local_cooldown = {d: 0 for d in self.domains}
        
        logger.info(f"ğŸš€ Detector åˆå§‹åŒ–å®Œæˆ | Device: {self.device}")

    def _predict_with_calibration(self, query: str) -> Dict:
        """
        é¢„æµ‹ + æ ¡å‡†
        """
        inputs = self.tokenizer(
            query, 
            return_tensors="pt", 
            truncation=True, 
            max_length=128
        ).to(self.device)
        
        with torch.no_grad():
            logits = self.model(**inputs).logits
            
            # åº”ç”¨æ¸©åº¦ç¼©æ”¾
            if self.use_calibration:
                logits = self.temperature_scaler(logits)
            
            probs = F.softmax(logits, dim=1).cpu().numpy()[0]
        
        pred_id = int(np.argmax(probs))
        pred_entropy = entropy(probs, base=2)  # ç”¨äº OOD æ£€æµ‹
        
        return {
            "domain": ID2LABEL[pred_id],
            "confidence": float(probs[pred_id]),
            "probs": {ID2LABEL[i]: float(p) for i, p in enumerate(probs)},
            "entropy": float(pred_entropy)
        }

    def detect(
        self, 
        query: str, 
        step: int, 
        ground_truth: Optional[str] = None
    ) -> DetectionResult:
        """
        ä¸»æ£€æµ‹å‡½æ•°
        :param query: è¾“å…¥æŸ¥è¯¢
        :param step: å½“å‰æ­¥æ•°
        :param ground_truth: çœŸå®æ ‡ç­¾ï¼ˆç”¨äº Local Shift æ£€æµ‹ï¼‰
        """
        # ========== Individual é¢„æµ‹ ==========
        pred = self._predict_with_calibration(query)
        
        result = DetectionResult(
            step=step,
            query=query,
            predicted_domain=pred["domain"],
            confidence=pred["confidence"],
            calibrated_probs=pred["probs"],
            entropy=pred["entropy"]
        )
        
        # OOD æ£€æµ‹ï¼ˆé«˜ç†µ = æœªçŸ¥ç±»åˆ«ï¼‰
        if pred["entropy"] > self.ood_entropy_threshold:
            logger.warning(f"âš ï¸ [OOD Signal] Query: '{query[:50]}...' | Entropy: {pred['entropy']:.3f}")
        
        # ========== æ›´æ–°å…¨å±€çª—å£ ==========
        self.global_window.append(pred["probs"])
        
        # ========== Global Shift æ£€æµ‹ ==========
        if len(self.global_window) >= 10:
            result = self._detect_global_shift(result)
        
        # ========== Local Shift æ£€æµ‹ï¼ˆéœ€è¦çœŸå®æ ‡ç­¾ï¼‰ ==========
        if ground_truth is not None:
            result = self._detect_local_shift(result, ground_truth)
        
        return result

    def _detect_global_shift(self, result: DetectionResult) -> DetectionResult:
        """
        å…¨å±€åˆ†å¸ƒåç§»æ£€æµ‹ï¼ˆJS æ•£åº¦ + PSIï¼‰
        """
        # è®¡ç®—å½“å‰æŸ¥è¯¢æµçš„å¹³å‡åˆ†å¸ƒ
        query_dist_vectors = np.array([list(p.values()) for p in self.global_window])
        avg_query_dist = np.mean(query_dist_vectors, axis=0)
        query_dist = {d: float(avg_query_dist[i]) for i, d in enumerate(self.domains)}
        
        result.query_distribution = query_dist
        
        # JS æ•£åº¦
        kb_vec = np.array([self.kb_dist[d] for d in self.domains])
        query_vec = np.array([query_dist[d] for d in self.domains])
        jsd = jensenshannon(kb_vec, query_vec)
        result.jsd_score = float(jsd)
        
        # PSI (Population Stability Index)
        psi = self._calculate_psi(self.kb_dist, query_dist)
        result.psi_score = psi
        
        # åˆ¤å®š Global Shift
        if self.global_cooldown == 0:
            if jsd > self.jsd_threshold or psi > self.psi_threshold:
                result.is_global_shift = True
                logger.warning(f"ğŸš¨ [Global Shift] Step {result.step}")
                logger.warning(f"   JSD: {jsd:.4f} (é˜ˆå€¼: {self.jsd_threshold})")
                logger.warning(f"   PSI: {psi:.4f} (é˜ˆå€¼: {self.psi_threshold})")
                logger.warning(f"   KB åˆ†å¸ƒ:    {self.kb_dist}")
                logger.warning(f"   Query åˆ†å¸ƒ: {query_dist}")
                self.global_cooldown = 50  # å†·å´ 50 æ­¥
        
        if self.global_cooldown > 0:
            self.global_cooldown -= 1
        
        return result

    def _detect_local_shift(
        self, 
        result: DetectionResult, 
        ground_truth: str
    ) -> DetectionResult:
        """
        å±€éƒ¨é¢†åŸŸç²¾åº¦ä¸‹é™æ£€æµ‹
        """
        predicted = result.predicted_domain
        is_correct = (predicted == ground_truth)
        
        # è®°å½•åˆ°çœŸå®é¢†åŸŸçš„çª—å£
        self.local_windows[ground_truth].append(is_correct)
        
        # æ£€æµ‹æ¯ä¸ªé¢†åŸŸ
        for domain in self.domains:
            window = self.local_windows[domain]
            
            if len(window) >= 5:  # æœ€å°æ ·æœ¬æ•°
                current_acc = sum(window) / len(window)
                result.local_accuracy[domain] = current_acc
                
                baseline = self.local_baseline_acc[domain]
                acc_drop = baseline - current_acc
                
                # Local Shift åˆ¤å®š
                if self.local_cooldown[domain] == 0:
                    if acc_drop > self.local_acc_drop_threshold:
                        result.local_shifts[domain] = True
                        logger.warning(f"âš ï¸ [Local Shift] Domain: {domain}")
                        logger.warning(f"   å½“å‰å‡†ç¡®ç‡: {current_acc:.2%}")
                        logger.warning(f"   åŸºçº¿å‡†ç¡®ç‡: {baseline:.2%}")
                        logger.warning(f"   ä¸‹é™å¹…åº¦: {acc_drop:.2%}")
                        self.local_cooldown[domain] = 30
                    else:
                        result.local_shifts[domain] = False
                else:
                    result.local_shifts[domain] = False
                
                # æ›´æ–°åŸºçº¿ï¼ˆæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼‰
                if not result.local_shifts.get(domain, False):
                    self.local_baseline_acc[domain] = 0.9 * baseline + 0.1 * current_acc
                
                if self.local_cooldown[domain] > 0:
                    self.local_cooldown[domain] -= 1
        
        return result

    def _calculate_psi(self, expected: Dict, actual: Dict) -> float:
        """
        PSI (Population Stability Index)
        å…¬å¼: PSI = Î£ (actual% - expected%) * ln(actual% / expected%)
        """
        psi = 0.0
        for domain in self.domains:
            e = expected.get(domain, 1e-10)
            a = actual.get(domain, 1e-10)
            psi += (a - e) * np.log(a / e)
        return float(psi)

    def update_kb_distribution(self, new_kb_dist: Dict[str, float]):
        """
        å¤–éƒ¨è§¦å‘ KB æ›´æ–°æ—¶é‡ç½®çŠ¶æ€
        """
        total = sum(new_kb_dist.values())
        self.kb_dist = {k: v / total for k, v in new_kb_dist.items()}
        self.global_window.clear()
        self.global_cooldown = 0
        logger.info(f"ğŸ”„ KB åˆ†å¸ƒå·²æ›´æ–°: {self.kb_dist}")

    def reset_local_baseline(self, domain: str = None, new_baseline: float = 0.85):
        """
        é‡ç½®é¢†åŸŸåŸºçº¿ï¼ˆå¦‚é‡æ–°æ ‡æ³¨æ•°æ®åï¼‰
        """
        if domain:
            self.local_baseline_acc[domain] = new_baseline
            self.local_windows[domain].clear()
            logger.info(f"ğŸ”„ é¢†åŸŸ {domain} åŸºçº¿å·²é‡ç½®ä¸º {new_baseline}")
        else:
            for d in self.domains:
                self.local_baseline_acc[d] = new_baseline
                self.local_windows[d].clear()
            logger.info(f"ğŸ”„ æ‰€æœ‰é¢†åŸŸåŸºçº¿å·²é‡ç½®ä¸º {new_baseline}")

# ==========================================
# 3. ä½¿ç”¨ç¤ºä¾‹
# ==========================================
if __name__ == "__main__":
    # RAG çŸ¥è¯†åº“çš„å…ˆéªŒåˆ†å¸ƒï¼ˆéœ€è¦é¢„å…ˆç»Ÿè®¡ï¼‰
    kb_prior = {
        "entertainment": 0.15,
        "stem": 0.40,
        "humanities": 0.30,
        "lifestyle": 0.15
    }
    
    detector = RAGDriftDetector(
        model_path="./mini_router_best",
        kb_distribution=kb_prior,
        global_window_size=100,
        local_window_size=30,
        jsd_threshold=0.1,
        psi_threshold=0.2,
        local_acc_drop_threshold=0.15,
        use_calibration=True  # å¯ç”¨æ ¡å‡†
    )
    
    # æ¨¡æ‹Ÿæ•°æ®æµ
    test_queries = [
        ("What company sponsored the Toyota Owners 400?", "entertainment"),
        ("How to implement gradient descent in PyTorch?", "stem"),
        ("The impact of Renaissance on art", "humanities"),
        ("Best workout routine for beginners", "lifestyle"),
        ("Quantum computing fundamentals", "stem"),
        ("Latest celebrity gossip 2024", "entertainment"),  # å¯èƒ½è§¦å‘ OOD
    ]
    
    print("\n" + "="*80)
    print("                    RAG DRIFT DETECTION DEMO")
    print("="*80 + "\n")
    
    for i, (query, gt) in enumerate(test_queries):
        result = detector.detect(query, step=i, ground_truth=gt)
        
        print(f"\n{'â”€'*80}")
        print(f"ğŸ“ Step {result.step} | Query: {result.query[:60]}...")
        print(f"{'â”€'*80}")
        
        # Individual é¢„æµ‹
        print(f"\nğŸ” Individual Prediction:")
        print(f"   Predicted: {result.predicted_domain} (Conf: {result.confidence:.2%})")
        print(f"   Ground Truth: {gt}")
        print(f"   Entropy: {result.entropy:.3f} {'âš ï¸ OOD' if result.entropy > detector.ood_entropy_threshold else ''}")
        print(f"   Calibrated Probs: {result.calibrated_probs}")
        
        # Global Shift
        if result.query_distribution:
            print(f"\nğŸŒ Global Shift Detection:")
            print(f"   Status: {'ğŸš¨ SHIFT DETECTED' if result.is_global_shift else 'âœ… Normal'}")
            print(f"   JSD: {result.jsd_score:.4f} (é˜ˆå€¼: {detector.jsd_threshold})")
            print(f"   PSI: {result.psi_score:.4f} (é˜ˆå€¼: {detector.psi_threshold})")
            print(f"   Query åˆ†å¸ƒ: {result.query_distribution}")
        
        # Local Shift
        if result.local_accuracy:
            print(f"\nğŸ“Š Local Shift Detection (Domain-wise Accuracy):")
            for domain, acc in result.local_accuracy.items():
                is_shift = result.local_shifts.get(domain, False)
                status = "âš ï¸ SHIFT" if is_shift else "âœ… Normal"
                print(f"   {domain:15s}: {acc:.2%} {status}")
    
    print("\n" + "="*80 + "\n")
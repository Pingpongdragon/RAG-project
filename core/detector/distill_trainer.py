import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
# ç§»é™¤ AdamW
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
# å¯¼å…¥ PyTorch åŸç”Ÿçš„ AdamW
from torch.optim import AdamW 
import json
import numpy as np
import os
from tqdm import tqdm

# ==========================================
# 0. å…¨å±€é…ç½®ä¸æ ‡ç­¾æ˜ å°„
# ==========================================
LABEL_MAP = {
    "entertainment": 0,
    "stem": 1,
    "humanities": 2,
    "lifestyle": 3
}
ID2LABEL = {v: k for k, v in LABEL_MAP.items()}

class Config:
    # ä½ çš„çœŸå®æ•°æ®æ–‡ä»¶
    data_path = "train_distill_mixed_qwen.jsonl"
    
    model_name = "distilbert-base-uncased" 
    num_labels = 4         
    max_len = 128           # é€‚å½“å¢åŠ é•¿åº¦ä»¥åº”å¯¹ HotpotQA çš„é•¿é—®é¢˜
    batch_size = 32        
    lr = 3e-5               # å¾®è°ƒé€šå¸¸ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
    epochs = 5             # çœŸå®æ•°æ®å»ºè®® 5-10 è½®
    temperature = 4.0      # è’¸é¦æ¸©åº¦
    alpha = 0.5            # è½¯ç¡¬ Loss æ¯”ä¾‹
    val_split = 0.1        # 10% éªŒè¯é›†
    device = "cuda" if torch.cuda.is_available() else "cpu"
    save_dir = "./mini_router_best"

# ==========================================
# 1. æ•°æ®é›†åŠ è½½ç±»
# ==========================================
class DistillationDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_len):
        self.data = []
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
            
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    # ç®€å•è¿‡æ»¤ç¡®ä¿å­—æ®µå®Œæ•´
                    if 'text' in item and 'teacher_probs' in item:
                        self.data.append(item)
        
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…± {len(self.data)} æ¡ã€‚")
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        teacher_probs = torch.tensor(item['teacher_probs'], dtype=torch.float)
        hard_label = torch.tensor(item['hard_label'], dtype=torch.long)

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'teacher_probs': teacher_probs,
            'hard_label': hard_label
        }

# ==========================================
# 2. æŸå¤±å‡½æ•°
# ==========================================
def distillation_loss(student_logits, teacher_probs, hard_labels, temp, alpha):
    # Soft Loss: KLæ•£åº¦
    student_log_probs = F.log_softmax(student_logits / temp, dim=1)
    soft_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temp ** 2)
    
    # Hard Loss: äº¤å‰ç†µ
    hard_loss = F.cross_entropy(student_logits, hard_labels)
    
    return alpha * soft_loss + (1.0 - alpha) * hard_loss

# ==========================================
# 3. è®­ç»ƒä¸éªŒè¯å‡½æ•°
# ==========================================
def evaluate(model, dataloader, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            hard_labels = batch['hard_label'].to(device)
            
            outputs = model(input_ids, attention_mask=mask)
            logits = outputs.logits
            
            # è®¡ç®—å‡†ç¡®ç‡
            preds = torch.argmax(logits, dim=1)
            correct += (preds == hard_labels).sum().item()
            total += hard_labels.size(0)
            
    return correct / total

def train():
    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)
    full_dataset = DistillationDataset(Config.data_path, tokenizer, Config.max_len)
    
    # åˆ‡åˆ†æ•°æ®é›†
    val_size = int(len(full_dataset) * Config.val_split)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size)
    
    model = AutoModelForSequenceClassification.from_pretrained(Config.model_name, num_labels=Config.num_labels)
    model.to(Config.device)
    
    optimizer = AdamW(model.parameters(), lr=Config.lr, weight_decay=0.01) 
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = len(train_loader) * Config.epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)
    
    print(f"\nğŸš€ å¼€å§‹åœ¨ {Config.device} ä¸Šè®­ç»ƒ...")
    best_acc = 0
    
    for epoch in range(Config.epochs):
        model.train()
        train_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{Config.epochs}")
        
        for batch in pbar:
            optimizer.zero_grad()
            
            input_ids = batch['input_ids'].to(Config.device)
            mask = batch['attention_mask'].to(Config.device)
            teacher_probs = batch['teacher_probs'].to(Config.device)
            hard_labels = batch['hard_label'].to(Config.device)
            
            outputs = model(input_ids, attention_mask=mask)
            loss = distillation_loss(outputs.logits, teacher_probs, hard_labels, Config.temperature, Config.alpha)
            
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
        # éªŒè¯
        val_acc = evaluate(model, val_loader, Config.device)
        print(f"ğŸ“Š Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f}")
        
        # ä¿å­˜è¡¨ç°æœ€å¥½çš„æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            print(f"âœ¨ å‘ç°æ›´å¥½çš„æ¨¡å‹ï¼Œå·²ä¿å­˜è‡³ {Config.save_dir}")
            model.save_pretrained(Config.save_dir)
            tokenizer.save_pretrained(Config.save_dir)

# ==========================================
# 4. åœ¨çº¿æ£€æµ‹å™¨
# ==========================================
class OnlineDetector:
    def __init__(self, model_path):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model.eval()
        
    def predict(self, query):
        inputs = self.tokenizer(query, return_tensors="pt", truncation=True, max_length=128).to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = F.softmax(outputs.logits, dim=1).cpu().numpy()[0]
        
        pred_id = int(np.argmax(probs))
        entropy = -np.sum(probs * np.log(probs + 1e-9))
        
        return {
            "query": query,
            "top_label": ID2LABEL[pred_id],
            "confidence": float(probs[pred_id]),
            "all_probs": {ID2LABEL[i]: float(p) for i, p in enumerate(probs)},
            "entropy": float(entropy)
        }

if __name__ == "__main__":
    # 1. æ‰§è¡Œè®­ç»ƒ
    train()
    
    # 2. æµ‹è¯•æ¨ç† (ä½¿ç”¨ä¿å­˜çš„æœ€ä½³æ¨¡å‹)
    if os.path.exists(Config.save_dir):
        detector = OnlineDetector(Config.save_dir)
        print("\n" + "="*40)
        print("      Online Detection Test")
        print("="*40)
        
        samples = [
            "What company sponsored the Toyota Owners 400 from 2007 to 2011?",
            "How to write a transformer model in PyTorch?",
            "The impact of the French Revolution on modern democracy"
        ]
        
        for q in samples:
            res = detector.predict(q)
            print(f"\nQ: {res['query']}")
            print(f"Top Domain: [{res['top_label'].upper()}] (Conf: {res['confidence']:.2f})")
            print(f"Entropy: {res['entropy']:.4f}")
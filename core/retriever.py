from typing import List, Union, Dict
from abc import ABC, abstractmethod
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from RAG_project.config import settings
from RAG_project.config.logger_config import logger
from RAG_project.core.reranker import ReRanker 
from langchain_community.vectorstores.utils import DistanceStrategy
from rank_bm25 import BM25Okapi

class BaseRetriever(ABC):
    """æ£€ç´¢å™¨åŸºç±»ï¼ˆæœ€å°åŒ–ä¿®æ”¹ï¼‰"""
    @abstractmethod
    def get_results(self, query: str) -> List[Document]:
        pass


class DenseRetriever(BaseRetriever):
    def __init__(self, vector_db: FAISS):
        """
        ç›´æ¥å­˜å‚¨åŸå§‹ FAISS å®ä¾‹ï¼Œè·å¾—æœ€é«˜çµæ´»æ€§
        :param vector_db: å·²åˆå§‹åŒ–çš„ FAISS å‘é‡æ•°æ®åº“
        """
        self.vector_db = vector_db  # ç›´æ¥æŒæœ‰åŸå§‹å‘é‡åº“
        self.default_k = settings.DEFAULT_DENSE_K  # å¯é…ç½®çš„é»˜è®¤è¿”å›æ•°é‡
        self.score_threshold = settings.DENSE_SCORE_THRESHOLD  # å¯é…ç½®çš„åˆ†æ•°é˜ˆå€¼

    def get_results(self, query: str) -> List[Document]:
        """
        æ‰§è¡Œå¸¦åˆ†æ•°è¿‡æ»¤çš„å‘é‡æ£€ç´¢
        :param query: è¾“å…¥æŸ¥è¯¢æ–‡æœ¬
        :return: ç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå¸¦åˆ†æ•°å…ƒæ•°æ®ï¼‰
        """
        if len(query.strip()) < 2:
            logger.warning(f"âš ï¸ æ— æ•ˆçŸ­æŸ¥è¯¢: {query}ï¼ˆé•¿åº¦<2ï¼‰")
            return []

        try:
            # ç›´æ¥è°ƒç”¨ FAISS çš„å¸¦åˆ†æ•°æ£€ç´¢æ–¹æ³•ï¼ˆè¿”å› (doc, score) å…ƒç»„åˆ—è¡¨ï¼‰
            raw_results = self.vector_db.similarity_search_with_score(
                query=query,
                k=self.default_k,
                distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT
            )
        except Exception as e:
            logger.error(f"ğŸ” å‘é‡æ£€ç´¢å¤±è´¥: {str(e)}")
            return []

        processed = []
        for doc, score in raw_results:
            if score < self.score_threshold:
                continue
            # å­˜å‚¨è½¬æ¢åçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆç›´æ¥ä½¿ç”¨åŸå§‹åˆ†æ•°ï¼‰
            doc.metadata["dense_score"] = float(score)
            doc.metadata["dense_metric"] = "cosine_similarity"  # æ›´æ–°æŒ‡æ ‡ç±»å‹
            processed.append(doc)

        logger.info(f"ğŸš€ å‘é‡æ£€ç´¢å®Œæˆï¼Œç»“æœæ•°: {len(processed)}")  
        return processed
    

class BM25Retriever(BaseRetriever):
    def __init__(self, docs: List[Document]):
        self.docs = docs
        self.tokenized_docs = [self._tokenize(doc.page_content) for doc in docs]
        self.bm25 = BM25Okapi(self.tokenized_docs)
        logger.info(f"âœ… BM25 åˆå§‹åŒ–å®Œæˆï¼Œæ–‡æ¡£æ•°: {len(self.docs)}")

    def _tokenize(self, text: str) -> List[str]:
        return text.lower().split()  # ç®€å•ç©ºæ ¼åˆ†è¯ï¼ŒæŒ‰éœ€æ›¿æ¢

    def get_results(self, query: str) -> List[Document]:
        if len(query.strip()) < 1:
            logger.warning("âš ï¸ ç©ºç™½æŸ¥è¯¢")
            return []
        
        # åˆ†è¯å¹¶å¤„ç†æœªç™»å½•è¯ï¼ˆOut-Of-Vocabularyï¼‰
        tokenized_query = self._tokenize(query)
        valid_tokens = [
            token for token in tokenized_query 
            if any(token in doc_tokens for doc_tokens in self.tokenized_docs)
        ]
        if not valid_tokens:
            logger.warning(f"ğŸš« æŸ¥è¯¢è¯æœªåœ¨è¯­æ–™åº“ä¸­å‡ºç°: {query}")
            return []
        
        # è·å–åŸå§‹ BM25 å¾—åˆ†
        raw_scores = self.bm25.get_scores(valid_tokens)  # shape = (n_docs,)
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼šæ‰€æœ‰æ–‡æ¡£å¾—åˆ†ç›¸åŒ
        if len(set(raw_scores)) == 1:
            logger.warning("âš ï¸ æ‰€æœ‰æ–‡æ¡£å¾—åˆ†ç›¸åŒï¼Œå¯èƒ½æŸ¥è¯¢è¯æ— åŒºåˆ†åº¦")
            normalized_scores = [0.5] * len(raw_scores)  # èµ‹äºˆä¸­é—´å€¼
        else:
            # å½’ä¸€åŒ–åˆ° [0,1] èŒƒå›´
            min_score = min(raw_scores)
            max_score = max(raw_scores)
            normalized_scores = [
                (score - min_score) / (max_score - min_score + 1e-6)
                for score in raw_scores
            ]

        # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        candidates = [
            (i, norm_score)
            for i, norm_score in enumerate(normalized_scores)
            if norm_score >= settings.SPARSE_SCORE_THRESHOLD  # æ­¤æ—¶é˜ˆå€¼åº”åœ¨0-1ä¹‹é—´
        ]
        
        # æŒ‰å½’ä¸€åŒ–åˆ†æ•°æ’åºå¹¶æˆªæ–­
        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)[:settings.DEFAULT_SPARSE_K]

        logger.info(
            f"ğŸ” BM25æ£€ç´¢ç»“æœ | åŸå§‹åˆ†èŒƒå›´: {min(raw_scores):.2f}-{max(raw_scores):.2f} "
            f"å½’ä¸€åŒ–åé˜ˆå€¼è¿‡æ»¤: {len(candidates)} æ¡ (é˜ˆå€¼={settings.SPARSE_SCORE_THRESHOLD})"
        )

        return [
            Document(
                page_content=self.docs[i].page_content,
                metadata={
                    "bm25_raw_score": float(raw_scores[i]),  # ä¿ç•™åŸå§‹åˆ†ä»¥ä¾¿è°ƒè¯•
                    "bm25_norm_score": float(norm_score),    # å½’ä¸€åŒ–åˆ†ç”¨äºé˜ˆå€¼åˆ¤æ–­
                    **self.docs[i].metadata
                }
            ) for i, norm_score in candidates
        ]



class QARetriever:
    def __init__(
        self, 
        vector_db: FAISS,
        docs: List[Document], 
        reranker_model: Union[str, ReRanker] = settings.RERANKER_MODEL,
        hybrid_search: bool = True
    ):
        # åˆå§‹åŒ–å­ç»„ä»¶
        self.dense_retriever = DenseRetriever(vector_db)
        self.sparse_retriever = BM25Retriever(docs)
        self.hybrid_enabled = hybrid_search
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        self.reranker = (
            reranker_model 
            if isinstance(reranker_model, ReRanker) 
            else ReRanker(reranker_model)
        )

    def _merge_results(
        self, 
        dense_results: List[Document], 
        sparse_results: List[Document]
    ) -> List[Dict]:
        """æ··åˆæ£€ç´¢ç»“æœèåˆ"""
        # ç»“æœå»é‡ï¼ˆæ ¹æ®origin_idï¼‰
        merged = {}
        for doc in dense_results + sparse_results:
            uid = doc.metadata["doc_id"]
            if uid not in merged:
                merged[uid] = {
                    "dense_score": doc.metadata.get("dense_score", 0),
                    "sparse_score": doc.metadata.get("sparse_score", 0),
                    "doc": doc
                }
        
        # è®¡ç®—æ··åˆåˆ†æ•°
        weighted_results = []
        for uid, scores in merged.items():
            hybrid_score = (
                settings.HYBRID_DENSE_WEIGHT * scores["dense_score"] +
                (1 - settings.HYBRID_DENSE_WEIGHT) * scores["sparse_score"]
            )
            weighted_results.append({
                "text": scores["doc"].page_content,
                "hybrid_score": hybrid_score,
                "metadata": scores["doc"].metadata
            })
        
        return sorted(weighted_results, key=lambda x: x["hybrid_score"], reverse=True)[:settings.HYBRID_TOP_K]

    def retrieve(
        self, 
        query: str, 
        rerank_top_k: int = settings.DEFAULT_RERANK_K, 
        rerank_threshold: float = settings.RERANK_THRESHOLD
    ) -> List[Dict]:
        """ä¿æŒä¸å˜çš„å¯¹å¤–æ¥å£"""
        # é˜¶æ®µ1: åŸºç¡€å¬å›
        dense_results = self.dense_retriever.get_results(query)
        logger.info(f"ğŸš€ å‘é‡å¬å›æ•°é‡: {len(dense_results)}")

        # é˜¶æ®µ2: æ··åˆèåˆï¼ˆå¿…é¡»é…ç½®äº†hybridæ‰è§¦å‘ï¼‰
        if self.hybrid_enabled:
            sparse_results = self.sparse_retriever.get_results(query)
            candidates = self._merge_results(dense_results, sparse_results)
            logger.info(f"ğŸ”€ æ··åˆåç»“æœé‡: {len(candidates)}")
        else:
            candidates = [{
                "text": doc.page_content,
                "hybrid_score": doc.metadata["dense_score"],
                "metadata": doc.metadata
            } for doc in dense_results]
        
        # é˜¶æ®µ3: æ·±åº¦é‡æ’
        rerank_inputs = [item["text"] for item in candidates]
        reranked = self.reranker.rerank(query, rerank_inputs, top_k=rerank_top_k)
        logger.info(f"ğŸ¯ ç²¾æ’åæ•°é‡: {len(reranked)}")
        
        # é˜¶æ®µ4: æœ€ç»ˆè¿‡æ»¤
        final_results = []
        for res in reranked:
            original_item = next(
                item for item in candidates 
                if item["text"] == res["text"]
            )
            if res["rerank_score"] >= rerank_threshold:
                final_item = {
                    "text": res["text"],
                    "scores": {
                        "hybrid": original_item["hybrid_score"],
                        "rerank": res["rerank_score"]
                    },
                    "metadata": {
                        **original_item["metadata"],
                        **res.get("metadata", {})
                    }
                }
                final_results.append(final_item)
        
        logger.info(f"âœ… æœ€ç»ˆæœ‰æ•ˆç»“æœ: {len(final_results)}æ¡")
        if settings.RANK_ORDER == 0:  # é™åºæ’åº
            return sorted(
                final_results, 
                key=lambda x: 0.5 * x["scores"]["rerank"] + 0.5 * x["scores"]["hybrid"], 
                reverse=True
            )
        else:  # å‡åºæ’åº
            return sorted(
                final_results, 
                key=lambda x: 0.5 * x["scores"]["rerank"] + 0.5 * x["scores"]["hybrid"]
            )


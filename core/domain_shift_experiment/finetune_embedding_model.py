import os
# ğŸ”´ [å…³é”®ä¿®æ”¹ 1] å¿…é¡»æ”¾åœ¨æ‰€æœ‰ import ä¹‹å‰ï¼
# å¼ºåˆ¶ Tokenizers åªä½¿ç”¨å•çº¿ç¨‹ï¼Œé¿å…ä¸ DataLoader çš„å¤šè¿›ç¨‹å†²çªå¯¼è‡´æ­»é”/å˜æ…¢
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import json
import logging
import torch
import gc
from pathlib import Path
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses

logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)

DATA_ROOT = Path("./data")
FIQA_DIR = DATA_ROOT / "raw_data" / "fiqa"
MODELS_DIR = DATA_ROOT / "models"
MODELS_DIR.mkdir(parents=True, exist_ok=True)

EPOCHS = 2
MAX_SEQ_LENGTH = 512

TRAIN_TASKS = [
    {
        "name": "MiniLM-L6",
        "base_model": "sentence-transformers/all-MiniLM-L6-v2",
        "output_path": str(MODELS_DIR / "minilm_l6_fiqa_finetuned"),
        "batch_size": 64,  
        "use_grad_checkpoint": False 
    },
    {
        "name": "BGE-M3",
        "base_model": "BAAI/bge-m3", 
        "output_path": str(MODELS_DIR / "bge_m3_fiqa_finetuned"),
        "batch_size": 16,  
        "use_grad_checkpoint": True 
    }
]

def load_train_data():
    logging.info("ğŸ“– æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    
    corpus = {}
    with open(FIQA_DIR / "corpus.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            doc = json.loads(line)
            corpus[doc['_id']] = doc['text']

    queries = {}
    with open(FIQA_DIR / "queries.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            q = json.loads(line)
            queries[q['_id']] = q['text']

    train_examples = []
    qrels_path = FIQA_DIR / "qrels" / "train.tsv"
    
    if not qrels_path.exists():
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®: {qrels_path}")

    logging.info("ğŸ”¨ æ­£åœ¨æ„å»ºè®­ç»ƒæ ·æœ¬...")
    with open(qrels_path, 'r', encoding='utf-8') as f:
        next(f)
        lines = f.readlines()
        # ä¸ºäº†å¿«é€ŸéªŒè¯é€Ÿåº¦ï¼Œä½ å¯ä»¥å…ˆåªå–å‰ 1000 æ¡è·‘è·‘çœ‹ï¼Œç¡®è®¤é€Ÿåº¦æ­£å¸¸åå†è·‘å…¨é‡
        # lines = lines[:1000] 
        for line in lines:
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                qid, docid = parts[0], parts[1]
                if qid in queries and docid in corpus:
                    train_examples.append(InputExample(texts=[queries[qid], corpus[docid]]))

    logging.info(f"âœ… è®­ç»ƒé›†æ„å»ºå®Œæˆï¼Œå…± {len(train_examples)} ä¸ªæ ·æœ¬ã€‚")
    return train_examples

def train():
    torch.cuda.empty_cache()
    gc.collect()

    train_examples = load_train_data()
    
    for task in TRAIN_TASKS:
        logging.info("\n" + "="*60)
        logging.info(f"ğŸš€ å¼€å§‹å¾®è°ƒæ¨¡å‹: {task['name']}")
        logging.info(f"   Batch Size: {task['batch_size']}")
        logging.info("="*60)
        
        model = SentenceTransformer(task['base_model'])
        model.max_seq_length = MAX_SEQ_LENGTH
        
        if task['use_grad_checkpoint']:
            logging.info("âš¡ å·²å¼€å¯ Gradient Checkpointing")
            model[0].auto_model.gradient_checkpointing_enable()

        # ğŸ”´ [å…³é”®ä¿®æ”¹ 2] ä¼˜åŒ– DataLoader å‚æ•°
        train_dataloader = DataLoader(
                train_examples, 
                shuffle=True, 
                batch_size=task['batch_size'],
                # å»ºè®®è®¾ç½®ä¸º CPU æ ¸å¿ƒæ•°çš„ä¸€åŠï¼Œæˆ–è€… 4-8 ä¹‹é—´
                # å¦‚æœä½ çš„ CPU æ ¸å¿ƒå¾ˆå¤šï¼Œå¯ä»¥å°è¯• 8 æˆ– 16
                num_workers=8,      
                pin_memory=True,    
                # å¢åŠ é¢„å–å› å­ï¼Œè®© CPU æå‰å‡†å¤‡æ›´å¤šæ•°æ®
                prefetch_factor=2,  
                # ä¿æŒ worker è¿›ç¨‹å­˜æ´»ï¼Œé¿å…æ¯ä¸ª epoch ç»“æŸåé‡æ–°åˆ›å»ºè¿›ç¨‹çš„å¼€é”€
                persistent_workers=True 
            )
        
        train_loss = losses.MultipleNegativesRankingLoss(model)
        
        # ğŸ”´ [é¢å¤–æç¤º] å¦‚æœè¿˜æ˜¯æ…¢ï¼Œå¯ä»¥å°è¯•æŠŠ use_amp=True æ”¹ä¸º False æµ‹ä¸€ä¸‹
        # è™½ç„¶ amp ä¼šåŠ é€Ÿï¼Œä½†åœ¨æŸäº›æç«¯çš„é©±åŠ¨ç‰ˆæœ¬ä¸‹å¯èƒ½å¼•å‘å¼‚å¸¸ï¼Œä¸è¿‡é€šå¸¸å»ºè®®å¼€å¯
        model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=EPOCHS,
            warmup_steps=int(len(train_dataloader) * 0.1),
            show_progress_bar=True,
            output_path=task['output_path'],
            use_amp=True 
        )
        logging.info(f"âœ… {task['name']} å¾®è°ƒå®Œæˆï¼")
        
        del model
        del train_loss # æ˜¾å¼åˆ é™¤ Loss å¯¹è±¡
        torch.cuda.empty_cache()
        gc.collect()

if __name__ == "__main__":
    train()
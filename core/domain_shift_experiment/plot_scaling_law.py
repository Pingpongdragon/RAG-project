import sys
import os
from pathlib import Path
import random
import string
import matplotlib.pyplot as plt
import torch
import gc
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from langchain.schema import Document

# ================= è·¯å¾„ä¸ç¯å¢ƒ =================
current_file_path = Path(__file__).resolve()
project_root = current_file_path.parent
while not (project_root / "core").exists():
    if project_root == project_root.parent: 
        project_root = current_file_path.parent 
        break
    project_root = project_root.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import core.generator
from core.data_processor import _build_hybrid_vector_index
from core.retriever import QARetriever
from core.generator import generate_batch_llm_response
from RAG_project.config import settings

# ğŸŸ¢ åŸºç¡€é…ç½®
settings.KNOWLEDGE_DATASET_CONFIG = {'chunk_size': 512, 'chunk_overlap': 50}
settings.TEMPERATURE = 0.01

# ğŸ”´ å®éªŒè§„æ¨¡é…ç½®
TEST_SAMPLE_SIZE = 100       # æµ‹è¯•é—®é¢˜æ•°é‡
BATCH_SIZE_INFERENCE = 32
NOISE_RATIO = 600              # å™ªéŸ³æ˜¯ Gold çš„å¤šå°‘å€ (600å€)

# âœ… å®šä¹‰ç›®æ ‡åŸŸçŸ¥è¯†æ‰©å®¹æ¯”ä¾‹
SCALE_RATIOS = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]

RETRIEVER_CONFIGS = [
    ("sentence-transformers/all-MiniLM-L6-v2", "MiniLM-L6 (Dense)", False),
    ("BAAI/bge-small-en-v1.5", "BGE-Small (Dense)", False),
    ("BAAI/bge-large-en-v1.5", "BGE-Large (Dense)", False),       # ğŸŸ¢ æ–°å¢: Large æ¨¡å‹
    ("BAAI/bge-small-en-v1.5", "BGE-Small (Hybrid)", True),       # ğŸ”µ æ–°å¢: æ··åˆæ£€ç´¢ (Dense + BM25)
]
# ================= å·¥å…·å‡½æ•° =================
def normalize_answer(s):
    def white_space_fix(text): return ' '.join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    def lower(text): return text.lower()
    return white_space_fix(remove_punc(lower(s)))

def calculate_containment(prediction, ground_truths):
    norm_pred = normalize_answer(prediction)
    if isinstance(ground_truths, str): ground_truths = [ground_truths]
    for gt in ground_truths:
        norm_gt = normalize_answer(gt)
        if len(norm_gt) < 2: continue
        if norm_gt in norm_pred: return 1.0
    return 0.0

def build_pipeline(docs, model_name, use_hybrid=False):
    gc.collect()
    torch.cuda.empty_cache()
    settings.EMBEDDING_MODEL = model_name
    if not docs: return None
    vector_db = _build_hybrid_vector_index(docs)
    settings.DEFAULT_RERANK_K = 5 
    return QARetriever(vector_db=vector_db, docs=docs, hybrid_search=use_hybrid)

def evaluate_batch(retriever, test_set, desc=""):
    total_acc = 0
    if not test_set or not retriever: return 0.0
    
    num_batches = (len(test_set) + BATCH_SIZE_INFERENCE - 1) // BATCH_SIZE_INFERENCE
    
    for i in range(num_batches):
        start = i * BATCH_SIZE_INFERENCE
        end = min(start + BATCH_SIZE_INFERENCE, len(test_set))
        batch = test_set[start:end]
        
        qs = [x['q'] for x in batch]
        golds = [x['a'] for x in batch]
        
        ctxs = []
        for q in qs:
            try:
                res = retriever.retrieve(q, rerank_top_k=5)
                ctxs.append([{"text": r['text'], "score": r['scores']['rerank']} for r in res])
            except: ctxs.append([])
        
        try:
            resps = generate_batch_llm_response(qs, ctxs, language="en")
        except:
            resps = [""] * len(qs)
            
        for r, g in zip(resps, golds):
            total_acc += calculate_containment(r, g)
            
    return total_acc / len(test_set)

# ================= æ•°æ®åŠ è½½ (SQuAD + HotpotQA) =================

def load_all_data():
    print("ğŸ”¥ Loading Data Pools...")
    
    # --- 1. Load Source Domain (SQuAD) - High Baseline ---
    print("   - Loading Source (SQuAD)...")
    # âœ… æ”¹åŠ¨1ï¼šä½¿ç”¨ validation é›†ï¼Œæ•°æ®è´¨é‡é€šå¸¸æ¯” train æ›´é«˜
    ds_squad = load_dataset("squad", split="validation")
    src_test_set = []
    src_docs = []
    
    # âœ… æ”¹åŠ¨2ï¼šåªå–å‰ 100 ä¸ªå®¹æ˜“å›ç­”çš„é—®é¢˜ (ç­”æ¡ˆçŸ­)
    # å¹¶ä¸”ä¸æ·»åŠ ä»»ä½•å™ªéŸ³æ–‡æ¡£ï¼Œç¡®ä¿ Source Baseline æ¥è¿‘å®Œç¾ (Upper Bound)
    count = 0
    for item in list(ds_squad):
        if count >= TEST_SAMPLE_SIZE: break
        
        # è¿‡æ»¤ï¼šåªä¿ç•™ç­”æ¡ˆé•¿åº¦åœ¨ 5 ä¸ªè¯ä»¥å†…çš„ï¼Œæ’é™¤é•¿éš¾å¥ï¼Œä¿è¯ LLM å®¹æ˜“å‘½ä¸­
        answers = item['answers']['text']
        if not answers or len(answers[0].split()) > 5:
            continue
            
        src_test_set.append({"q": item['question'], "a": answers})
        src_docs.append(Document(page_content=item['context'], metadata={"doc_id": item['id'], "source": "squad"}))
        count += 1
    
    print(f"   -> Selected {len(src_test_set)} high-quality SQuAD samples (Short Answers).")
    
    # âŒ ç§»é™¤ SQuAD å™ªéŸ³ï¼šä¸ºäº†è®© Baseline å°½å¯èƒ½é«˜ï¼Œæˆ‘ä»¬ä¸å†æ·»åŠ é¢å¤–çš„ SQuAD å™ªéŸ³
    # è¿™æ ·æ£€ç´¢å™¨åœ¨ Source Domain åªæœ‰ 100 ä¸ªå€™é€‰ï¼Œå‡ ä¹æ˜¯ 100% å‘½ä¸­ç‡

    # --- 2. Load Target Domain (HotpotQA) ---
    print("   - Loading Target (HotpotQA)...")
    ds = load_dataset("hotpot_qa", "distractor", split="train")
    ds_list = list(ds)
    random.seed(42)
    random.shuffle(ds_list)
    
    target_test_set = []
    all_gold_docs = []
    noise_pool = []
    
    # A. æå–æµ‹è¯•é›† & Gold Docs
    print("     -> Extracting Target Questions & Gold Docs...")
    for i, item in enumerate(ds_list[:TEST_SAMPLE_SIZE]):
        q = item['question']
        a = item['answer']
        target_test_set.append({"q": q, "a": a})
        
        titles = item['context']['title']
        sentences = item['context']['sentences']
        gold_titles = set(item['supporting_facts']['title'])
        
        for t, s_list in zip(titles, sentences):
            text = f"{t}: " + "".join(s_list)
            doc = Document(page_content=text, metadata={"doc_id": f"hp_{t}", "source": "hotpot_gold"})
            if t in gold_titles:
                all_gold_docs.append(doc)

    # B. æ„å»º 5 å€é‡çš„å™ªéŸ³æ± 
    target_noise_count = len(all_gold_docs) * NOISE_RATIO
    print(f"     -> Mining {target_noise_count} Noise Docs from remaining data...")
    
    for item in ds_list[TEST_SAMPLE_SIZE:]:
        if len(noise_pool) >= target_noise_count: break
        titles = item['context']['title']
        sentences = item['context']['sentences']
        for t, s_list in zip(titles, sentences):
            if len(noise_pool) >= target_noise_count: break
            text = f"{t}: " + "".join(s_list)
            noise_pool.append(Document(page_content=text, metadata={"doc_id": f"noise_{len(noise_pool)}", "source": "hotpot_noise"}))
            
    # âœ… æ ¸å¿ƒä¿®æ”¹ï¼šå°† Gold å’Œ Noise æ··åˆæˆä¸€ä¸ªå¤§æ± å­ï¼Œå¹¶æ‰“ä¹±
    target_mixed_pool = all_gold_docs + noise_pool
    random.shuffle(target_mixed_pool)
    
    print(f"âœ… Data Ready:")
    print(f"   Source (SQuAD): {len(src_docs)} docs (Clean, No Noise)")
    print(f"   Target Mixed Pool: {len(target_mixed_pool)} docs (Gold+Noise)")
    
    return src_docs, src_test_set, target_mixed_pool, target_test_set

# ================= å®éªŒä¸»æ§ =================

def run_full_experiment():
    src_docs, src_test, target_mixed_pool, target_test = load_all_data()
    
    all_results = {}
    x_labels = ["Source\nBaseline", "Domain\nShift"] + [f"{r:.0%}" for r in SCALE_RATIOS]
    
    total_target_docs = len(target_mixed_pool)
    
    for model_name, display_name, use_hybrid in RETRIEVER_CONFIGS:
        print(f"\nğŸš€ Testing Retriever: {display_name}")
        accuracies = []
        
        # --- Phase 1: Source Baseline ---
        print("   [1/3] Source Baseline (SQuAD)...")
        pipe = build_pipeline(src_docs, model_name, use_hybrid)
        acc_src = evaluate_batch(pipe, src_test, desc="Source Base")
        accuracies.append(acc_src)
        print(f"      -> Acc: {acc_src:.2%}")
        del pipe
        
        # --- Phase 2: Domain Shift (The Drop) ---
        print("   [2/3] Domain Shift (Zero-shot)...")
        pipe = build_pipeline(src_docs, model_name, use_hybrid) 
        acc_shift = evaluate_batch(pipe, target_test, desc="Shift Drop")
        accuracies.append(acc_shift)
        print(f"      -> Acc: {acc_shift:.2%}")
        del pipe
        
        # --- Phase 3: Scaling Recovery ---
        print("   [3/3] Scaling Recovery (Mixed Pool)...")
        for ratio in SCALE_RATIOS:
            # âœ… æ ¸å¿ƒä¿®æ”¹ï¼šç›´æ¥ä»æ··åˆæ± ä¸­æŒ‰æ¯”ä¾‹åˆ‡ç‰‡
            # ç”±äº target_mixed_pool å·²ç» shuffle è¿‡ï¼Œä¸”è¿™é‡Œæ¯æ¬¡éƒ½ä»å¤´å–
            # ä¿è¯äº† ratio=0.3 çš„æ•°æ®å®Œå…¨åŒ…å« ratio=0.1 çš„æ•°æ® (å­é›†å…³ç³»)
            n_docs = int(total_target_docs * ratio)
            current_target_kb = target_mixed_pool[:n_docs]
            
            # çŸ¥è¯†åº“ = Source KB + éƒ¨åˆ† Target Mixed KB
            current_docs = src_docs + current_target_kb
            
            pipe = build_pipeline(current_docs, model_name, use_hybrid)
            acc = evaluate_batch(pipe, target_test, desc=f"Scale {ratio:.0%}")
            accuracies.append(acc)
            print(f"      -> Ratio {ratio:.0%} (Docs: {len(current_target_kb)}): {acc:.2%}")
            
            del pipe
            gc.collect()
        
        all_results[display_name] = accuracies

    return all_results, x_labels

def plot_full_curve(results, x_labels):
    plt.figure(figsize=(12, 6))
    
    markers = ['o', 's', '^', 'D']
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    x_indices = range(len(x_labels))
    
    for i, (name, accs) in enumerate(results.items()):
        plt.plot(x_indices, accs, 
                 marker=markers[i % len(markers)], 
                 color=colors[i % len(colors)], 
                 linewidth=2.5, 
                 label=name)
        
        for x, y in zip(x_indices, accs):
            plt.text(x, y + 0.01, f"{y:.1%}", fontsize=8, ha='center', va='bottom')

    plt.xlabel('Experiment Phase', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.title('Domain Shift Impact & Scaling Law Recovery (Mixed Pool)', fontsize=14)
    
    plt.xticks(x_indices, x_labels)
    
    plt.axvline(x=1.5, color='gray', linestyle='--', alpha=0.7)
    plt.text(0.5, plt.ylim()[0] + (plt.ylim()[1]-plt.ylim()[0])*0.5, "Domain Shift\n(Drop)", ha='center', color='red', fontsize=10)
    plt.text(4, plt.ylim()[0] + (plt.ylim()[1]-plt.ylim()[0])*0.5, "Knowledge Scaling\n(Recovery)", ha='center', color='green', fontsize=10)

    plt.grid(True, linestyle='--', alpha=0.4)
    plt.legend(fontsize=10)
    plt.tight_layout()
    
    out_path = 'rag_full_scaling_curve_mixed.png'
    plt.savefig(out_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… Plot saved to: {out_path}")

if __name__ == "__main__":
    results, x_labels = run_full_experiment()
    plot_full_curve(results, x_labels)
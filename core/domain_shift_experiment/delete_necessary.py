import sys
import os
from pathlib import Path
import random
import re
import string
import matplotlib.pyplot as plt
import torch
import gc
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from langchain.schema import Document

# ================= è·¯å¾„è‡ªåŠ¨é…ç½® =================
current_file_path = Path(__file__).resolve()
project_root = current_file_path.parent
while not (project_root / "core").exists():
    if project_root == project_root.parent: 
        project_root = current_file_path.parent 
        break
    project_root = project_root.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

# å¯¼å…¥ core
import core.generator
from core.data_processor import _build_hybrid_vector_index
from core.retriever import QARetriever
from core.generator import generate_llm_response
from RAG_project.config import settings

# ğŸŸ¢ 1. ä¿®æ”¹ Promptï¼šå¼ºåˆ¶ç®€çŸ­å›ç­”ï¼Œæ–¹ä¾¿åšåŒ…å«åŒ¹é…
core.generator.CONTEXT_PROMPT_TEMPLATE_EN = '''
Based on the provided context, answer the question using ONLY a few words (e.g., a name, date, or entity).
Do NOT write full sentences.

[Context]
{context}

[Question]
{query}

[Answer]
'''

# ğŸŸ¢ 2. é…ç½®è°ƒæ•´
settings.KNOWLEDGE_DATASET_CONFIG = {
    'chunk_size': 512,
    'chunk_overlap': 50
}
settings.TEMPERATURE = 0.01
settings.EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2" # å°æ¨¡å‹æ›´å®¹æ˜“ä½“ç°å™ªéŸ³å¹²æ‰°

# å®éªŒå‚æ•°
QUERIES_PER_STEP = 40    # æ¯ä¸ªæ—¶é—´æ­¥æµ‹è¯• 40 ä¸ªé—®é¢˜ (20 old / 20 new åŠ¨æ€å˜åŒ–)
DOCS_PER_DOMAIN = 2000   # æ¯ä¸ªé¢†åŸŸæœ€å¤š 2000 æ¡æ–‡æ¡£ (èŠ‚çœæ—¶é—´)

# ================= æŒ‡æ ‡å·¥å…·: Containment =================
def normalize_answer(s):
    def white_space_fix(text):
        return ' '.join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    def lower(text):
        return text.lower()
    return white_space_fix(remove_punc(lower(s)))

def calculate_containment(prediction, ground_truths):
    """å¦‚æœç”Ÿæˆçš„ç­”æ¡ˆåŒ…å«äº†ä»»æ„ä¸€ä¸ªæ ‡å‡†ç­”æ¡ˆï¼Œè®°ä¸º 1.0"""
    norm_pred = normalize_answer(prediction)
    for gt in ground_truths:
        norm_gt = normalize_answer(gt)
        if len(norm_gt) < 2: continue # è·³è¿‡è¿‡çŸ­çš„ç­”æ¡ˆ
        if norm_gt in norm_pred:
            return 1.0
    return 0.0

# ================= æ•°æ®å‡†å¤‡ =================
def prepare_data():
    print("ğŸ“¦ å‡†å¤‡åŒé¢†åŸŸæ•°æ® (Wiki vs SQuAD)...")
    
    # 1. Domain A: Wiki (Old)
    wiki_ds = load_dataset("rag-datasets/rag-mini-wikipedia", "text-corpus", split="passages")
    wiki_qa = load_dataset("rag-datasets/rag-mini-wikipedia", "question-answer", split="test")
    
    w_docs = []
    for i, item in enumerate(wiki_ds):
        if i >= DOCS_PER_DOMAIN: break
        w_docs.append(Document(page_content=item['passage'], metadata={"doc_id": f"wiki_{item['id']}", "source": "wiki"}))
        
    w_qs = []
    for item in wiki_qa.select(range(QUERIES_PER_STEP)): 
        w_qs.append({"question": item['question'], "answers": [item['answer']] if 'answer' in item else []})

    # 2. Domain B: SQuAD (New)
    squad_ds = load_dataset("squad_v2", split="validation")
    s_docs = []
    s_qs = []
    seen = set()
    count = 0
    
    for item in squad_ds:
        ctx = item['context']
        if ctx not in seen and len(s_docs) < DOCS_PER_DOMAIN:
            s_docs.append(Document(page_content=ctx, metadata={"doc_id": f"squad_{item['id']}", "source": "squad"}))
            seen.add(ctx)
        
        if len(item['answers']['text']) > 0 and count < QUERIES_PER_STEP:
            s_qs.append({"question": item['question'], "answers": item['answers']['text']})
            count += 1
            
    print(f"âœ… Ready: {len(w_docs)} Wiki Docs, {len(s_docs)} SQuAD Docs")
    return w_docs, w_qs, s_docs, s_qs

# ================= Pipeline =================
def run_pipeline(docs, test_set, desc):
    if not docs: return 0.0
    
    # ç¦ç”¨ Hybridï¼Œåªç”¨ Denseï¼Œå› ä¸º Dense å¯¹å™ªéŸ³æœ€æ•æ„Ÿ
    vector_db = _build_hybrid_vector_index(docs)
    retriever = QARetriever(vector_db=vector_db, docs=docs, hybrid_search=False)
    
    total_acc = 0
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    pbar = tqdm(test_set, desc=f"   Running {desc}", leave=False)
    
    for item in pbar:
        try:
            results = retriever.retrieve(item['question'], rerank_top_k=3)
            ctx = [{"text": r['text'], "score": r['scores']['rerank']} for r in results]
            _, resp = generate_llm_response(query=item['question'], context=ctx, language="en")
            
            score = calculate_containment(resp, item['answers'])
            total_acc += score
            
            pbar.set_postfix({"Acc": f"{total_acc / (pbar.n + 1):.2%}"})
        except: 
            pass
            
    avg_acc = total_acc / len(test_set)
    
    del vector_db, retriever
    gc.collect()
    torch.cuda.empty_cache()
    
    return avg_acc

# ================= ä¸»å®éªŒé€»è¾‘ =================
def run_smooth_shift():
    w_docs, w_qs, s_docs, s_qs = prepare_data()
    
    # æ¨¡æ‹Ÿå¹³æ»‘è¿ç§»çš„æ—¶é—´æ­¥ï¼šæ–°ä¸šåŠ¡å æ¯” (Alpha)
    alphas = [0.0, 0.25, 0.5, 0.75, 1.0]
    
    res_cum = []  # Cumulative (Add Only)
    res_agg = []  # Aggressive (Delete All Old)
    res_ada = []  # Adaptive (Proportional)
    x_labels = []
    
    print("\nğŸš€ å¼€å§‹ Smooth Shift å®éªŒ...")
    
    for alpha in alphas:
        label = f"{int(alpha*100)}% New"
        x_labels.append(label)
        print(f"\nâ³ Time Step: {label} (Query Mix)")
        
        # 1. æ„é€ å½“å‰çš„ Query åˆ†å¸ƒ (æ··åˆ Old & New é—®é¢˜)
        n_new = int(len(s_qs) * alpha)
        n_old = int(len(w_qs) * (1 - alpha))
        # æˆªå–å¯¹åº”æ•°é‡çš„é—®é¢˜
        current_test_set = w_qs[:n_old] + s_qs[:n_new]
        
        # --- A. Cumulative (åªåŠ ä¸åˆ ) ---
        # å§‹ç»ˆä¿ç•™å…¨éƒ¨æ—§æ•°æ® (2000æ¡)ï¼ŒåªæŒ‰æ¯”ä¾‹åŠ æ–°æ•°æ®
        kb_cum = w_docs[:] + s_docs[:int(len(s_docs)*alpha)]
        if not kb_cum: kb_cum = w_docs[:] 
        acc = run_pipeline(kb_cum, current_test_set, "ğŸ”´ Cumulative")
        res_cum.append(acc)
        
        # --- B. Aggressive (å…¨åˆ æ—§çš„) ---
        # åªè¦å¼€å§‹è½¬å‹ï¼Œå°±åªä¿ç•™æ–°æ•°æ®
        kb_agg = s_docs[:int(len(s_docs)*alpha)]
        if not kb_agg: 
            acc = 0.0 # ç©ºåº“
        else:
            acc = run_pipeline(kb_agg, current_test_set, "ğŸ”µ Aggressive")
        res_agg.append(acc)
        
        # --- C. Adaptive (åŠ¨æ€å¹³è¡¡) ---
        # åˆ ä¸€éƒ¨åˆ†æ—§çš„ï¼ŒåŠ ä¸€éƒ¨åˆ†æ–°çš„ï¼Œä¿æŒ KB åˆ†å¸ƒ = Query åˆ†å¸ƒ
        n_w_keep = int(len(w_docs) * (1 - alpha))
        n_s_keep = int(len(s_docs) * alpha)
        kb_ada = w_docs[:n_w_keep] + s_docs[:n_s_keep]
        if not kb_ada: kb_ada = w_docs[:1] # é˜²æ­¢ç©º
        
        acc = run_pipeline(kb_ada, current_test_set, "ğŸŸ¢ Adaptive")
        res_ada.append(acc)
        
    return res_cum, res_agg, res_ada, x_labels

# ================= ç»˜å›¾ =================
def plot_results(rc, rg, ra, labels):
    plt.figure(figsize=(10, 6))
    
    # ç»˜åˆ¶æ›²çº¿
    plt.plot(labels, ra, marker='*', markersize=12, color='green', linewidth=3, label='Adaptive (Add & Partial Delete)')
    plt.plot(labels, rc, marker='o', color='red', linestyle='--', label='Cumulative (Add Only)')
    plt.plot(labels, rg, marker='x', color='blue', linestyle='-.', label='Aggressive (Delete All Old)')
    
    # æ ‡æ³¨ä¸­é—´çš„å™ªéŸ³å¹²æ‰°
    mid = 2 # 50% New
    gap_noise = ra[mid] - rc[mid]
    plt.annotate(f"Noise Penalty\n(-{gap_noise:.1%})", 
                 xy=(mid, rc[mid]), xytext=(mid, rc[mid]-0.1),
                 arrowprops=dict(facecolor='red', shrink=0.05), ha='center', color='red')
    
    # æ ‡æ³¨å‰æœŸçš„é—å¿˜
    early = 1 # 25% New
    gap_forget = ra[early] - rg[early]
    plt.annotate(f"Forgetting\n(-{gap_forget:.1%})", 
                 xy=(early, rg[early]), xytext=(early, rg[early]-0.15),
                 arrowprops=dict(facecolor='blue', shrink=0.05), ha='center', color='blue')

    plt.ylabel('Accuracy (Answer Containment)')
    plt.xlabel('Shift Progress (Old -> New Domain)')
    plt.title('RAG Data Strategy during Smooth Domain Shift')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    out = 'rag_smooth_shift_containment.png'
    plt.savefig(out, dpi=300, bbox_inches='tight')
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {out}")

if __name__ == "__main__":
    c, g, a, l = run_smooth_shift()
    plot_results(c, g, a, l)
import json
import os
from pathlib import Path
from typing import List, Dict
import numpy as np
from tqdm import tqdm

# å‘é‡åº“ä¸æ¨¡å‹
import faiss
from sentence_transformers import SentenceTransformer

os.environ["HF-ENDPOINT"] = "https://hf-mirror.com"

# ================= é…ç½® =================
DATA_ROOT = Path("./data")
FIQA_DIR = DATA_ROOT / "raw_data" / "fiqa"
MODEL_NAME = "all-MiniLM-L6-v2"  # ä¸€ä¸ªå…¸å‹çš„é€šç”¨å°æ¨¡å‹ï¼Œå®¹æ˜“åœ¨ä¸“ä¸šé¢†åŸŸç¿»è½¦
SAMPLE_SIZE = 100000  # ä¸ºäº†é€Ÿåº¦ï¼Œåªå–å‰2000ä¸ªæ–‡æ¡£åšå®éªŒ
TEST_QUERIES_COUNT = 500 # æµ‹è¯•50ä¸ªé—®é¢˜

def load_fiqa_data():
    """è¯»å– FiQA çš„åŸå§‹ JSONL æ•°æ®"""
    print("ğŸ“– æ­£åœ¨åŠ è½½è¯­æ–™åº“ (Corpus)...")
    corpus = {}
    with open(FIQA_DIR / "corpus.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            doc = json.loads(line)
            corpus[doc['_id']] = doc['text']
            if len(corpus) >= SAMPLE_SIZE: break
    
    print("ğŸ“– æ­£åœ¨åŠ è½½æŸ¥è¯¢é›† (Queries)...")
    queries = {}
    with open(FIQA_DIR / "queries.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            q = json.loads(line)
            queries[q['_id']] = q['text']
            
    print("ğŸ“– æ­£åœ¨åŠ è½½æ ‡å‡†ç­”æ¡ˆ (Qrels)...")
    qrels = {} # query_id -> [doc_id, ...]
    with open(FIQA_DIR / "qrels" / "test.tsv", 'r', encoding='utf-8') as f:
        next(f) # skip header
        for line in f:
            qid, docid, score = line.strip().split('\t')
            if qid not in qrels: qrels[qid] = []
            qrels[qid].append(docid)
            
    return corpus, queries, qrels

def run_experiment():
    print(f"\nğŸ”¬ å¯åŠ¨ Domain Shift è¯Šæ–­å®éªŒ (Model: {MODEL_NAME})")
    print("-" * 60)
    
    # 1. åŠ è½½æ¨¡å‹
    model = SentenceTransformer(MODEL_NAME)
    
    # 2. å‡†å¤‡æ•°æ®
    corpus, queries, qrels = load_fiqa_data()
    
    # è¿‡æ»¤ï¼šåªä¿ç•™æˆ‘ä»¬åœ¨ Sample Corpus é‡Œæœ‰ç­”æ¡ˆçš„é—®é¢˜
    valid_qids = []
    for qid, docids in qrels.items():
        if qid in queries and any(did in corpus for did in docids):
            valid_qids.append(qid)
    
    test_qids = valid_qids[:TEST_QUERIES_COUNT]
    print(f"ğŸ“Š å®éªŒè§„æ¨¡: æ–‡æ¡£åº“ {len(corpus)} æ¡, æµ‹è¯•é—®é¢˜ {len(test_qids)} ä¸ª")

    if len(test_qids) == 0:
        print("âŒ é”™è¯¯: é‡‡æ ·å¤ªå°ï¼Œæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„é—®é¢˜-æ–‡æ¡£å¯¹ï¼Œè¯·å¢åŠ  SAMPLE_SIZEã€‚")
        return

    # 3. [Dynamic Indexing] æ¨¡æ‹Ÿæ›´æ–°å‘é‡åº“
    print("\nğŸ—ï¸ [Step 1] æ­£åœ¨æ„å»ºåŠ¨æ€ç´¢å¼• (Embedding + Indexing)...")
    doc_ids = list(corpus.keys())
    doc_texts = list(corpus.values())
    
    # ç¼–ç 
    doc_embeddings = model.encode(doc_texts, show_progress_bar=True, batch_size=32)
    
    # å»ºåº“
    dimension = doc_embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension) # Inner Product (Sim to Cosine if normalized)
    faiss.normalize_L2(doc_embeddings)
    index.add(doc_embeddings)
    
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆã€‚")

    # 4. [Retrieval Evaluation] è¯„ä¼°æ£€ç´¢æ•ˆæœ
    print("\nğŸ” [Step 2] æ‰§è¡Œæ£€ç´¢å¹¶è¯„ä¼° Hit Rate...")
    
    hits = 0
    mrr_sum = 0
    top_k = 10
    
    failure_cases = []
    success_cases = []

    for qid in tqdm(test_qids):
        query_text = queries[qid]
        target_doc_ids = set(qrels[qid])
        
        # ç¼–ç æŸ¥è¯¢
        q_emb = model.encode([query_text])
        faiss.normalize_L2(q_emb)
        
        # æœç´¢
        distances, indices = index.search(q_emb, k=top_k)
        retrieved_indices = indices[0]
        
        # æ£€æŸ¥å‘½ä¸­
        is_hit = False
        for rank, idx in enumerate(retrieved_indices):
            if idx == -1: continue
            retrieved_doc_id = doc_ids[idx]
            
            if retrieved_doc_id in target_doc_ids:
                hits += 1
                mrr_sum += 1.0 / (rank + 1)
                is_hit = True
                
                # è®°å½•ä¸€ä¸ªæˆåŠŸæ¡ˆä¾‹
                if len(success_cases) < 2:
                    success_cases.append({
                        "q": query_text, 
                        "d": corpus[retrieved_doc_id][:100] + "..."
                    })
                break
        
        if not is_hit:
            # è®°å½•å¤±è´¥æ¡ˆä¾‹ç”¨äºåˆ†æ
            if len(failure_cases) < 3:
                # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„å†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                correct_doc_content = "N/A"
                for did in target_doc_ids:
                    if did in corpus:
                        correct_doc_content = corpus[did][:100] + "..."
                        break
                
                failure_cases.append({
                    "q": query_text,
                    "target_doc": correct_doc_content,
                    "retrieved_doc": corpus[doc_ids[retrieved_indices[0]]][:100] + "..."
                })

    # 5. è®¡ç®—æŒ‡æ ‡
    hit_rate = hits / len(test_qids)
    mrr = mrr_sum / len(test_qids)

    print("\n" + "="*60)
    print(f"ğŸ“ˆ å®éªŒç»“æœ (Domain: Finance/FiQA)")
    print(f"   Hit Rate @ {top_k}: {hit_rate:.2%}  (ç›®æ ‡æ–‡æ¡£åœ¨å‰10ä¸ªç»“æœé‡Œå‡ºç°çš„æ¦‚ç‡)")
    print(f"   MRR @ {top_k}     : {mrr:.4f}     (å¹³å‡å€’æ•°æ’å)")
    print("="*60)

    # 6. è‡ªåŠ¨è¯Šæ–­ä¸å»ºè®®
    print("\nğŸ©º [è¯Šæ–­æŠ¥å‘Š]")
    
    if hit_rate > 0.7:
        print("âœ… ç»“è®º: å•çº¯æ›´æ–° Vectorbase è¶³å¤Ÿæœ‰æ•ˆã€‚")
        print("   åŸå› : é€šç”¨æ¨¡å‹åœ¨è¿™ä¸ªå­é¢†åŸŸè¡¨ç°å°šå¯ï¼Œæœ¯è¯­é‡å åº¦è¾ƒé«˜ã€‚")
    elif hit_rate > 0.4:
        print("âš ï¸ ç»“è®º: æ•ˆæœä¸€èˆ¬ï¼Œå»ºè®®å°è¯•å¾®è°ƒæ£€ç´¢å™¨ (Retriever Fine-tuning)ã€‚")
        print("   åŸå› : æ¨¡å‹èƒ½æ‡‚éƒ¨åˆ†å†…å®¹ï¼Œä½†åœ¨å¤„ç†ä¸“ä¸šåŒ¹é…æ—¶æœ‰å›°éš¾ã€‚")
    else:
        print("âŒ ç»“è®º: å•çº¯æ›´æ–° Vectorbase å¤±è´¥ï¼å¿…é¡»å¾®è°ƒæ£€ç´¢å™¨æˆ–ä½¿ç”¨é¢†åŸŸä¸“ç”¨æ¨¡å‹ã€‚")
        print("   åŸå› : å‘ç”Ÿäº†ä¸¥é‡çš„ Semantic Shiftï¼Œæ¨¡å‹å®Œå…¨æŠ“çã€‚")

    print("\nğŸ“ [å¤±è´¥æ¡ˆä¾‹åˆ†æ - ä¸ºä»€ä¹ˆéœ€è¦å¾®è°ƒ?]")
    for i, case in enumerate(failure_cases):
        print(f"\nCase {i+1}:")
        print(f"â“ Query: {case['q']}")
        print(f"âœ… ç›®æ ‡æ–‡æ¡£ (æ¨¡å‹æ²¡é€‰): {case['target_doc']}")
        print(f"âŒ æ£€ç´¢ç»“æœ (æ¨¡å‹é€‰äº†): {case['retrieved_doc']}")
        print("   ğŸ‘‰ åˆ†æ: å¦‚æœç›®æ ‡æ–‡æ¡£é‡Œæ²¡æœ‰Queryçš„å…³é”®è¯ï¼ˆçº¯è¯­ä¹‰åŒ¹é…ï¼‰ï¼Œé€šç”¨æ¨¡å‹é€šå¸¸ä¼šæŒ‚ã€‚")

if __name__ == "__main__":
    if not (FIQA_DIR / "corpus.jsonl").exists():
        print("âŒ æ•°æ®æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œ 01_download_real_data.py")
    else:
        run_experiment()
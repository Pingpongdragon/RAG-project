"""
KB åŸºç¡€æ•°æ®ç»“æ„ - ç®€åŒ–ç‰ˆ
"""
import numpy as np
from typing import Dict, List
from pathlib import Path
import json
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from RAG_project.models.embeddings import embedding_service



class KBDocument:
    """çŸ¥è¯†åº“æ–‡æ¡£"""
    def __init__(self, doc_id: str, domain: str, content: str, title: str = "", embedding: np.ndarray = None):
        self.doc_id = doc_id
        self.domain = domain
        self.content = content
        self.title = title
        self.embedding = embedding
        self.access_count = 0
        self.last_access_step = -1


class TopicCluster:
    """åŸŸå†…çš„å­è¯é¢˜ç°‡"""
    def __init__(self, cluster_id: str, centroid: np.ndarray, domain: str):
        self.cluster_id = cluster_id
        self.centroid = centroid
        self.domain = domain
        self.docs: List[KBDocument] = []
        self.heat = 1.0
        self.last_access_step = -1
        self.creation_step = -1
        self.size = 0
    
    def add_doc(self, doc: KBDocument):
        """æ·»åŠ æ–‡æ¡£å¹¶æ›´æ–°è´¨å¿ƒ"""
        self.docs.append(doc)
        self.size += 1
        
        # é‡æ–°è®¡ç®—è´¨å¿ƒï¼ˆæ‰€æœ‰æ–‡æ¡£embeddingçš„å¹³å‡å€¼ï¼‰
        embeddings = np.array([d.embedding for d in self.docs])
        self.centroid = np.mean(embeddings, axis=0)
        
        # å½’ä¸€åŒ–è´¨å¿ƒå‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
        self.centroid = self.centroid / (np.linalg.norm(self.centroid) + 1e-8)
    
    def update_heat(self, step: int, decay: float = 0.95):
        """æ›´æ–°çƒ­åº¦"""
        if self.last_access_step >= 0:
            steps_passed = step - self.last_access_step
            self.heat = self.heat * (decay ** steps_passed) + 1.0
        else:
            self.heat += 1.0
        self.last_access_step = step
    
    def compute_similarity(self, query_vec: np.ndarray) -> float:
        """è®¡ç®—ç›¸ä¼¼åº¦"""
        dot = np.dot(self.centroid, query_vec)
        norm1 = np.linalg.norm(self.centroid)
        norm2 = np.linalg.norm(query_vec)
        return dot / (norm1 * norm2 + 1e-8)


class DomainBucket:
    """å•ä¸ª Domain çš„å­˜å‚¨æ¡¶"""
    def __init__(self, domain_name: str, capacity: int = 2000, 
                 similarity_threshold: float = 0.45, max_clusters: int = 12):
        self.domain_name = domain_name
        self.capacity = capacity
        self.similarity_threshold = similarity_threshold
        self.max_clusters = max_clusters
        
        self.clusters: List[TopicCluster] = []
        self.doc_count = 0
    
    def add_document(self, doc: KBDocument, step: int) -> bool:
        """æ·»åŠ æ–‡æ¡£"""
        if doc.domain != self.domain_name:
            return False
        
        best_cluster = None
        max_sim = -1.0
        
        for cluster in self.clusters:
            sim = cluster.compute_similarity(doc.embedding)
            if sim > max_sim:
                max_sim = sim
                best_cluster = cluster
        
        if best_cluster and max_sim >= self.similarity_threshold:
            best_cluster.add_doc(doc)
            best_cluster.update_heat(step)
        else:
            if len(self.clusters) >= self.max_clusters:
                if best_cluster:
                    best_cluster.add_doc(doc)
                    best_cluster.update_heat(step)
                else:
                    return False
            else:
                new_cluster = TopicCluster(
                    cluster_id=f"{self.domain_name}_c{len(self.clusters)}",
                    centroid=doc.embedding.copy(),
                    domain=self.domain_name
                )
                new_cluster.add_doc(doc)
                new_cluster.update_heat(step)
                new_cluster.creation_step = step
                self.clusters.append(new_cluster)
        
        self.doc_count += 1
        
        while self.doc_count > self.capacity and len(self.clusters) > 1:
            self._evict_coldest_cluster(current_step=step)
        
        return True
    
    def _evict_coldest_cluster(self, current_step: int = 0):
        """ç®€åŒ–ç‰ˆæ·˜æ±°ç­–ç•¥ï¼šåªåŸºäºçƒ­åº¦"""
        if not self.clusters:
            return
        
        # è®¡ç®—æ¯ä¸ªç°‡çš„è™šæ‹Ÿçƒ­åº¦ï¼ˆè€ƒè™‘è¡°å‡ï¼‰
        def get_virtual_heat(cluster):
            if cluster.last_access_step >= 0:
                steps_passed = current_step - cluster.last_access_step
                return cluster.heat * (0.95 ** steps_passed)
            return cluster.heat
        
        # æ‰¾åˆ°çƒ­åº¦æœ€ä½çš„ç°‡
        target_cluster = min(self.clusters, key=get_virtual_heat)
        cluster_size = len(target_cluster.docs)
        
        # ç®€å•ç­–ç•¥ï¼šç›´æ¥ç§»é™¤æ•´ä¸ªå†·ç°‡
        if cluster_size <= 50:  # å°ç°‡ç›´æ¥åˆ é™¤
            removed_count = cluster_size
            self.clusters.remove(target_cluster)
        else:  # å¤§ç°‡ç§»é™¤éƒ¨åˆ†æ–‡æ¡£
            num_to_remove = max(1, int(cluster_size * 0.3))  # ç§»é™¤30%
            
            # æŒ‰æ–‡æ¡£çš„è®¿é—®æ—¶é—´æ’åºï¼Œç§»é™¤æœ€æ—§çš„
            target_cluster.docs.sort(key=lambda d: (d.last_access_step, d.access_count))
            target_cluster.docs = target_cluster.docs[num_to_remove:]
            
            # é‡æ–°è®¡ç®—è´¨å¿ƒ
            if target_cluster.docs:
                embeddings = np.array([d.embedding for d in target_cluster.docs])
                target_cluster.centroid = np.mean(embeddings, axis=0)
                target_cluster.centroid = target_cluster.centroid / (np.linalg.norm(target_cluster.centroid) + 1e-8)
            
            removed_count = num_to_remove
        
        self.doc_count -= removed_count
    
    def search(self, query_vec: np.ndarray, step: int, top_k: int = 5) -> List[KBDocument]:
        """æ£€ç´¢æ–‡æ¡£ - ä½¿ç”¨ retriever è¿›è¡Œç²¾ç¡®æ£€ç´¢"""
        if not self.clusters:
            return []
        
        # 1. æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç°‡
        best_cluster = max(self.clusters, key=lambda c: c.compute_similarity(query_vec))
        best_cluster.update_heat(step)
        
        # 2. ç°‡å†…æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆä¿ç•™åŸæ¥çš„ç®€å•å®ç°ï¼‰
        doc_scores = []
        for doc in best_cluster.docs:
            sim = np.dot(query_vec, doc.embedding) / (
                np.linalg.norm(query_vec) * np.linalg.norm(doc.embedding) + 1e-8
            )
            doc_scores.append((doc, sim))
        
        # 3. æŒ‰ç›¸ä¼¼åº¦é™åºæ’åºå¹¶å– top_k
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        # 4. æ›´æ–°è®¿é—®ç»Ÿè®¡
        results = []
        for doc, score in doc_scores[:top_k]:
            doc.access_count += 1
            doc.last_access_step = step
            results.append(doc)
        
        return results
    
    def get_all_docs(self) -> List[KBDocument]:
        """è·å–æ‰€æœ‰æ–‡æ¡£"""
        all_docs = []
        for cluster in self.clusters:
            all_docs.extend(cluster.docs)
        return all_docs
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "domain": self.domain_name,
            "doc_count": self.doc_count,
            "cluster_count": len(self.clusters),
            "avg_cluster_size": self.doc_count / len(self.clusters) if self.clusters else 0,
            "cluster_heats": [c.heat for c in self.clusters],
            
        }


class ClusteredKnowledgeBase:
    """åŸºäºèšç±»çš„çŸ¥è¯†åº“ - ç®€åŒ–ç‰ˆ"""
    def __init__(self, capacity: int = 8000, encoder=None):
        self.capacity = capacity
        self.domains = ["0_entertainment", "1_stem", "2_humanities", "3_lifestyle"]
        self.encoder = encoder  # å¤–éƒ¨ä¼ å…¥çš„encoder
        
        per_domain_capacity = capacity // len(self.domains)
        
        self.buckets: Dict[str, DomainBucket] = {
            domain: DomainBucket(domain, per_domain_capacity)
            for domain in self.domains
        }
    
    def add_document(self, doc: KBDocument, step: int) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°KB"""
        if doc.domain not in self.buckets:
            return False
        return self.buckets[doc.domain].add_document(doc, step)
    
    def search(self, query_vec: np.ndarray, query_domain: str, step: int, top_k: int = 5) -> List[KBDocument]:
        """å‘é‡æ£€ç´¢"""
        if query_domain not in self.buckets:
            return []
        return self.buckets[query_domain].search(query_vec, step, top_k)
    
    def get_distribution(self) -> Dict[str, float]:
        """è·å–domainåˆ†å¸ƒ"""
        total_docs = sum(bucket.doc_count for bucket in self.buckets.values())
        if total_docs == 0:
            return {d: 0.0 for d in self.domains}
        return {
            domain: bucket.doc_count / total_docs
            for domain, bucket in self.buckets.items()
        }
    
    def get_statistics(self) -> Dict:
        """âœ… ä¿®å¤ï¼šè¿”å›å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯å­—å…¸"""
        total_docs = sum(b.doc_count for b in self.buckets.values())
        
        return {
            "total_docs": total_docs,
            "capacity": self.capacity,
            "distribution": self.get_distribution(),  # âœ… è°ƒç”¨ä¸Šé¢çš„æ–¹æ³•
            "buckets": {
                domain: {
                    "doc_count": bucket.doc_count,
                    "capacity": bucket.capacity,
                    "cluster_count": len(bucket.clusters),
                    "utilization": bucket.doc_count / bucket.capacity if bucket.capacity > 0 else 0.0
                }
                for domain, bucket in self.buckets.items()
            }
        }
    
    def clear_domain(self, domain: str):
        """æ¸…ç©ºæŸä¸ªdomain"""
        if domain in self.buckets:
            self.buckets[domain] = DomainBucket(domain, self.buckets[domain].capacity)


def load_kb_documents() -> Dict[str, List[KBDocument]]:
    """ä» hotpot_kb åŠ è½½æ–‡æ¡£æ± """
    HERE = Path(__file__).parent
    KB_DIR = HERE / "dataset_split_domain" / "hotpot_kb"
    
    if not KB_DIR.exists():
        raise FileNotFoundError(f"âŒ KB ç›®å½•ä¸å­˜åœ¨: {KB_DIR}")
    
    pool = {}
    domains = ["0_entertainment", "1_stem", "2_humanities", "3_lifestyle"]
    
    for domain in domains:
        kb_file = KB_DIR / f"{domain}.jsonl"
        if not kb_file.exists():
            continue
        
        docs = []
        with open(kb_file, 'r', encoding='utf-8') as f:
            for line in f:
                obj = json.loads(line)
                doc = KBDocument(
                    doc_id=obj["doc_id"],
                    domain=domain,
                    content=obj["text"],
                    title=obj.get("title", ""),
                    embedding=None  # å…ˆä¸è®¾ç½®embedding
                )
                docs.append(doc)
        
        # æ‰¹é‡ç”Ÿæˆ embeddingï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        if docs:
            print(f"  ğŸ”§ æ­£åœ¨ä¸º {domain} ç”Ÿæˆ embedding ({len(docs)} æ–‡æ¡£)...")
            texts = [doc.content for doc in docs]
            
            # ä½¿ç”¨ SentenceTransformer çš„ encode æ–¹æ³•æ‰¹é‡ç¼–ç 
            embeddings = embedding_service.encode(
                texts,
                batch_size=32,  # æ‰¹é‡å¤„ç†åŠ é€Ÿ
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True  # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            )
            
            # å°† embedding èµ‹å€¼ç»™æ–‡æ¡£
            for doc, emb in zip(docs, embeddings):
                doc.embedding = emb
        
        pool[domain] = docs
    
    return pool
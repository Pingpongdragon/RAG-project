"""
ä¸»å®éªŒè„šæœ¬:è¿è¡Œä¸‰ä¸ªç­–ç•¥å¹¶å¯¹æ¯”
"""
from kb_base import ClusteredKnowledgeBase, load_kb_documents
from evaluator import load_test_data, compute_retrieval_score
from RAG_project.updator.static_updater import StaticUpdater
from RAG_project.updator.reactive_updater import ReactiveUpdater
from RAG_project.updator.cluster_updater import ClusteredAdaptiveUpdater
from RAG_project.core.detector.detector import AutoAdaptiveDetector


def run_experiment(updater, detector, queries, name: str):
    """è¿è¡Œå®éªŒï¼ˆæ‰¹é‡ç”Ÿæˆembeddingç‰ˆï¼‰"""
    from RAG_project.models.embeddings import embedding_service
    
    # ğŸ”¥ æ‰¹é‡ç”Ÿæˆæ‰€æœ‰æŸ¥è¯¢çš„ embedding
    print(f"   ğŸ”§ æ‰¹é‡ç”Ÿæˆ {len(queries)} ä¸ªæŸ¥è¯¢çš„ embedding...")
    query_texts = [q["query"] for q in queries]
    query_embeddings = embedding_service.encode(
        query_texts,
        batch_size=32,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    print(f"   âœ… Embedding ç”Ÿæˆå®Œæˆ")
    
    metrics = {
        "total_updates": 0,
        "total_cost": 0,
        "retrieval_scores": [],
        "alert_count": 0
    }
    
    for i, query in enumerate(queries):
        query_vec = query_embeddings[i]
        
        # ç›´æ¥è°ƒç”¨ KB çš„ search
        retrieved_docs = updater.kb.search(query_vec, query["domain"], step=i, top_k=10)
        
        # è®¡ç®—å¬å›ç‡
        gold_doc_ids = query.get("gold_doc_ids", [])
        if gold_doc_ids and retrieved_docs:
            retrieved_ids = set(doc.doc_id for doc in retrieved_docs)
            gold_ids = set(gold_doc_ids)
            matched_count = len(retrieved_ids & gold_ids)
            score = matched_count / len(gold_ids)
        else:
            score = 0.0
        
        metrics["retrieval_scores"].append(score)
        
        # æ£€æµ‹
        detection_result = detector.detect(query["domain"], score, i)
        
        if detection_result.is_global_shift or detection_result.is_intra_degradation:
            metrics["alert_count"] += 1
        
        # æ›´æ–°
        update_result = updater.update(detection_result, i)
        
        if update_result.get("action") not in ["no_update", None]:
            metrics["total_updates"] += 1
            metrics["total_cost"] += update_result.get("removed", 0) + update_result.get("added", 0)
            
            # âœ… å…³é”®ï¼šæ›´æ–°åé€šçŸ¥ detector åŒæ­¥ KB åˆ†å¸ƒ
            kb_stats = updater.kb.get_statistics()
            new_kb_dist = kb_stats["distribution"]
            detector.update_kb_distribution(new_kb_dist)
    
    avg_score = sum(metrics["retrieval_scores"]) / len(metrics["retrieval_scores"])
    
    print(f"\n{'='*50}")
    print(f"ğŸ“Š {name} ç»“æœ:")
    print(f"{'='*50}")
    print(f"  å¹³å‡æ£€ç´¢å¾—åˆ†: {avg_score:.3f}")
    print(f"  æ€»æ›´æ–°æ¬¡æ•°: {metrics['total_updates']}")
    print(f"  æ€»æˆæœ¬: {metrics['total_cost']}")
    print(f"  å‘Šè­¦æ¬¡æ•°: {metrics['alert_count']}")
    
    return metrics


def init_kb(doc_pool, capacity=10000):
    """åˆå§‹åŒ– KB"""
    kb = ClusteredKnowledgeBase(capacity=capacity)
    
    for domain, docs in doc_pool.items():
        for doc in docs[:capacity//4]:
            kb.add_document(doc, step=0)
    
    return kb


if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸš€ KB Updater å®éªŒå¼€å§‹ (çœŸå®è¯„åˆ†ç‰ˆ)")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“š åŠ è½½æ•°æ®...")
    try:
        doc_pool = load_kb_documents()
        print(f"   âœ… æ–‡æ¡£æ± : {sum(len(docs) for docs in doc_pool.values())} æ¡")
    except FileNotFoundError as e:
        print(f"   âŒ é”™è¯¯: {e}")
        exit(1)
    
    # åŠ è½½ä¸åŒç±»å‹çš„domain shiftæ•°æ®é›†è¿›è¡Œæµ‹è¯•
    shift_types = ["sudden", "gradual", "recurring"]  # å¯ä»¥æ·»åŠ  "recurring" å¦‚æœæ–‡ä»¶å­˜åœ¨
    
    for shift_type in shift_types:
        print(f"\n{'='*70}")
        print(f"ğŸ“Š æµ‹è¯• {shift_type.upper()} Domain Shift")
        print(f"{'='*70}")
        
        try:
            queries = load_test_data(shift_type=shift_type)
            print(f"   âœ… æŸ¥è¯¢æ•°æ®: {len(queries)} æ¡")
        except FileNotFoundError as e:
            print(f"   âŒ é”™è¯¯: {e}")
            continue
        
        # åˆå§‹åŒ–ä¸‰ä¸ªç­–ç•¥
        kb1 = init_kb(doc_pool)
        kb2 = init_kb(doc_pool)
        kb3 = init_kb(doc_pool)
        
        detector1 = AutoAdaptiveDetector()
        detector2 = AutoAdaptiveDetector()
        detector3 = AutoAdaptiveDetector()
        
        static_updater = StaticUpdater(kb1, doc_pool)
        reactive_updater = ReactiveUpdater(kb2, doc_pool)
        adaptive_updater = ClusteredAdaptiveUpdater(kb3, doc_pool)
        
        # è¿è¡Œå®éªŒ
        print(f"\nğŸ”¬ è¿è¡Œå®éªŒ ({shift_type})...")
        
        results = {}
        results["Static"] = run_experiment(static_updater, detector1, queries, f"Static ({shift_type})")
        results["Reactive"] = run_experiment(reactive_updater, detector2, queries, f"Reactive ({shift_type})")
        results["Adaptive"] = run_experiment(adaptive_updater, detector3, queries, f"Adaptive ({shift_type})")
        
        # å¯¹æ¯”ç»“æœ
        print(f"\n{'='*70}")
        print(f"ğŸ“ˆ {shift_type.upper()} å¯¹æ¯”æ€»ç»“")
        print(f"{'='*70}")
        for name, metrics in results.items():
            avg_score = sum(metrics["retrieval_scores"]) / len(metrics["retrieval_scores"])
            print(f"{name:12s} | å¾—åˆ†: {avg_score:.3f} | æ›´æ–°: {metrics['total_updates']:3d} | æˆæœ¬: {metrics['total_cost']:6d}")
    
    print("\n" + "="*70)
    print("âœ… å®éªŒå®Œæˆ!")
    print("="*70)
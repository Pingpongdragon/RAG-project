import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
import json
import numpy as np
import os
import random
from tqdm import tqdm
from sklearn.metrics import classification_report

# ==========================================
# 0. å…¨å±€é…ç½®ä¸æ ‡ç­¾æ˜ å°„
# ==========================================
LABEL_MAP = {
    "entertainment": 0,
    "stem": 1,
    "humanities": 2,
    "lifestyle": 3
}
ID2LABEL = {v: k for k, v in LABEL_MAP.items()}


class Config:
    data_path = "/home/jyliu/RAG_project/train_distill_mixed_qwen_v4.jsonl"

    model_name = "distilbert-base-uncased"
    num_labels = 4
    max_len = 128
    batch_size = 32
    lr = 3e-5
    epochs = 8              # 1ä¸‡æ¡æ•°æ®ï¼Œ8è½®è¶³å¤Ÿæ”¶æ•›
    temperature = 2.0        # é™ä½æ¸©åº¦ï¼Œåˆ†å¸ƒæ›´é”åˆ©
    alpha = 0.3              # 1ä¸‡æ¡æ ‡æ³¨æ•°æ®è¶³å¤Ÿï¼Œæ›´å¤šä¾èµ– hard label
    val_split = 0.1
    device = "cpu"
    save_dir = "/home/jyliu/RAG_project/detector/mini_router_best"
    seed = 42
    warmup_ratio = 0.1       # 10% warmup
    max_grad_norm = 1.0      # æ¢¯åº¦è£å‰ª
    early_stopping_patience = 3  # æ—©åœè€å¿ƒå€¼


# ==========================================
# 1. å›ºå®šéšæœºç§å­
# ==========================================
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# ==========================================
# 2. æ•°æ®é›†åŠ è½½ç±»
# ==========================================
class DistillationDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_len):
        self.data = []
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {data_path}")

        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    if 'text' in item and 'teacher_probs' in item and 'hard_label' in item:
                        self.data.append(item)

        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…± {len(self.data)} æ¡ã€‚")

        # ç»Ÿè®¡å„ç±»åˆ«åˆ†å¸ƒ
        label_counts = {}
        for item in self.data:
            lbl = item['hard_label']
            label_counts[lbl] = label_counts.get(lbl, 0) + 1
        print("ğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
        for lbl_id in sorted(label_counts.keys()):
            lbl_name = ID2LABEL.get(lbl_id, f"unknown_{lbl_id}")
            count = label_counts[lbl_id]
            ratio = count / len(self.data) * 100
            print(f"   {lbl_name} ({lbl_id}): {count} æ¡ ({ratio:.1f}%)")

        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        teacher_probs = torch.tensor(item['teacher_probs'], dtype=torch.float)
        hard_label = torch.tensor(item['hard_label'], dtype=torch.long)

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'teacher_probs': teacher_probs,
            'hard_label': hard_label
        }


# ==========================================
# 3. æŸå¤±å‡½æ•°
# ==========================================
def distillation_loss(student_logits, teacher_probs, hard_labels, temp, alpha):
    # Soft Loss: KLæ•£åº¦
    student_log_probs = F.log_softmax(student_logits / temp, dim=1)
    soft_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temp ** 2)

    # Hard Loss: äº¤å‰ç†µ
    hard_loss = F.cross_entropy(student_logits, hard_labels)

    return alpha * soft_loss + (1.0 - alpha) * hard_loss


# ==========================================
# 4. è¯„ä¼°å‡½æ•°
# ==========================================
def evaluate(model, dataloader, device):
    """åŸºç¡€è¯„ä¼°ï¼šè¿”å›å‡†ç¡®ç‡å’ŒéªŒè¯æŸå¤±"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            hard_labels = batch['hard_label'].to(device)

            outputs = model(input_ids, attention_mask=mask)
            logits = outputs.logits

            loss = F.cross_entropy(logits, hard_labels)
            total_loss += loss.item()

            preds = torch.argmax(logits, dim=1)
            correct += (preds == hard_labels).sum().item()
            total += hard_labels.size(0)

    avg_loss = total_loss / len(dataloader)
    accuracy = correct / total
    return accuracy, avg_loss


def evaluate_detailed(model, dataloader, device):
    """è¯¦ç»†è¯„ä¼°ï¼šè¾“å‡ºæ¯ä¸ªç±»åˆ«çš„ precision / recall / f1"""
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            hard_labels = batch['hard_label'].to(device)

            outputs = model(input_ids, attention_mask=mask)
            preds = torch.argmax(outputs.logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(hard_labels.cpu().numpy())

    target_names = [ID2LABEL[i] for i in range(Config.num_labels)]
    report = classification_report(all_labels, all_preds, target_names=target_names, digits=4)
    print(report)
    return report


# ==========================================
# 5. è®­ç»ƒä¸»å¾ªç¯
# ==========================================
def train():
    set_seed(Config.seed)

    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)
    full_dataset = DistillationDataset(Config.data_path, tokenizer, Config.max_len)

    # åˆ‡åˆ†æ•°æ®é›†
    val_size = int(len(full_dataset) * Config.val_split)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(Config.seed)
    )

    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, num_workers=2, pin_memory=True)

    print(f"ğŸ“¦ è®­ç»ƒé›†: {len(train_dataset)} æ¡ | éªŒè¯é›†: {len(val_dataset)} æ¡")

    model = AutoModelForSequenceClassification.from_pretrained(Config.model_name, num_labels=Config.num_labels)
    model.to(Config.device)

    optimizer = AdamW(model.parameters(), lr=Config.lr, weight_decay=0.01)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ (å¸¦ warmup)
    total_steps = len(train_loader) * Config.epochs
    warmup_steps = int(total_steps * Config.warmup_ratio)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

    print(f"\nğŸš€ å¼€å§‹åœ¨ {Config.device} ä¸Šè®­ç»ƒ...")
    print(f"   æ€»æ­¥æ•°: {total_steps} | Warmup æ­¥æ•°: {warmup_steps}")
    print(f"   æ¸©åº¦: {Config.temperature} | Alpha(è½¯æŸå¤±æƒé‡): {Config.alpha}")

    best_acc = 0
    patience_counter = 0

    for epoch in range(Config.epochs):
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{Config.epochs}")

        for batch in pbar:
            optimizer.zero_grad()

            input_ids = batch['input_ids'].to(Config.device)
            mask = batch['attention_mask'].to(Config.device)
            teacher_probs = batch['teacher_probs'].to(Config.device)
            hard_labels = batch['hard_label'].to(Config.device)

            outputs = model(input_ids, attention_mask=mask)
            loss = distillation_loss(outputs.logits, teacher_probs, hard_labels, Config.temperature, Config.alpha)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Config.max_grad_norm)
            optimizer.step()
            scheduler.step()

            train_loss += loss.item()

            # è®­ç»ƒå‡†ç¡®ç‡
            preds = torch.argmax(outputs.logits, dim=1)
            train_correct += (preds == hard_labels).sum().item()
            train_total += hard_labels.size(0)

            current_lr = scheduler.get_last_lr()[0]
            pbar.set_postfix({"loss": f"{loss.item():.4f}", "lr": f"{current_lr:.2e}"})

        avg_train_loss = train_loss / len(train_loader)
        train_acc = train_correct / train_total

        # éªŒè¯
        val_acc, val_loss = evaluate(model, val_loader, Config.device)
        print(f"ğŸ“Š Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

        # ä¿å­˜è¡¨ç°æœ€å¥½çš„æ¨¡å‹ + æ—©åœ
        if val_acc > best_acc:
            best_acc = val_acc
            patience_counter = 0
            print(f"âœ¨ å‘ç°æ›´å¥½çš„æ¨¡å‹ (Val Acc: {val_acc:.4f})ï¼Œå·²ä¿å­˜è‡³ {Config.save_dir}")
            model.save_pretrained(Config.save_dir)
            tokenizer.save_pretrained(Config.save_dir)
        else:
            patience_counter += 1
            print(f"â³ æ¨¡å‹æœªæå‡ï¼Œè€å¿ƒå€¼: {patience_counter}/{Config.early_stopping_patience}")
            if patience_counter >= Config.early_stopping_patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼æœ€ä½³ Val Acc: {best_acc:.4f}")
                break

    print(f"\nğŸ è®­ç»ƒç»“æŸï¼Œæœ€ä½³ Val Acc: {best_acc:.4f}")

    # åŠ è½½æœ€ä½³æ¨¡å‹åšè¯¦ç»†è¯„ä¼°
    if os.path.exists(Config.save_dir):
        print("\n" + "=" * 50)
        print("      ğŸ“‹ æœ€ä½³æ¨¡å‹åˆ†ç±»è¯¦ç»†æŠ¥å‘Š")
        print("=" * 50)
        best_model = AutoModelForSequenceClassification.from_pretrained(Config.save_dir).to(Config.device)
        evaluate_detailed(best_model, val_loader, Config.device)


# ==========================================
# 6. åœ¨çº¿æ£€æµ‹å™¨
# ==========================================
class OnlineDetector:
    def __init__(self, model_path):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model.eval()

    def predict(self, query):
        inputs = self.tokenizer(query, return_tensors="pt", truncation=True, max_length=128).to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = F.softmax(outputs.logits, dim=1).cpu().numpy()[0]

        pred_id = int(np.argmax(probs))
        entropy = -np.sum(probs * np.log(probs + 1e-9))

        return {
            "query": query,
            "top_label": ID2LABEL[pred_id],
            "confidence": float(probs[pred_id]),
            "all_probs": {ID2LABEL[i]: float(p) for i, p in enumerate(probs)},
            "entropy": float(entropy)
        }

    def predict_batch(self, queries, batch_size=32):
        """æ‰¹é‡é¢„æµ‹ï¼Œæé«˜æ¨ç†æ•ˆç‡"""
        results = []
        for i in range(0, len(queries), batch_size):
            batch_queries = queries[i:i + batch_size]
            inputs = self.tokenizer(
                batch_queries,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=128
            ).to(self.device)

            if 'token_type_ids' in inputs:
                inputs.pop('token_type_ids')

            with torch.no_grad():
                outputs = self.model(**inputs)
                batch_probs = F.softmax(outputs.logits, dim=1).cpu().numpy()

            for j, (query, probs) in enumerate(zip(batch_queries, batch_probs)):
                pred_id = int(np.argmax(probs))
                entropy = -np.sum(probs * np.log(probs + 1e-9))
                results.append({
                    "query": query,
                    "top_label": ID2LABEL[pred_id],
                    "confidence": float(probs[pred_id]),
                    "all_probs": {ID2LABEL[k]: float(p) for k, p in enumerate(probs)},
                    "entropy": float(entropy)
                })
        return results


# ==========================================
# 7. å…¥å£
# ==========================================
if __name__ == "__main__":
    # 1. æ‰§è¡Œè®­ç»ƒ
    train()

    # 2. æµ‹è¯•æ¨ç† (ä½¿ç”¨ä¿å­˜çš„æœ€ä½³æ¨¡å‹)
    if os.path.exists(Config.save_dir):
        detector = OnlineDetector(Config.save_dir)
        print("\n" + "=" * 50)
        print("      ğŸ§ª Online Detection Test")
        print("=" * 50)

        samples = [
            # entertainment
            "What company sponsored the Toyota Owners 400 from 2007 to 2011?",
            "Who won the Best Actor Oscar in 2020?",
            # stem
            "How to write a transformer model in PyTorch?",
            "Explain the difference between TCP and UDP protocols.",
            # humanities
            "The impact of the French Revolution on modern democracy",
            "What are the main themes of Shakespeare's Hamlet?",
            # lifestyle
            "What are the best exercises for losing belly fat?",
            "How to make a perfect sourdough bread at home?",
        ]

        for q in samples:
            res = detector.predict(q)
            print(f"\nQ: {res['query']}")
            print(f"   ğŸ·ï¸  Domain: [{res['top_label'].upper()}] (Conf: {res['confidence']:.3f}, Entropy: {res['entropy']:.4f})")
            probs_str = " | ".join([f"{k}: {v:.3f}" for k, v in res['all_probs'].items()])
            print(f"   ğŸ“Š {probs_str}")
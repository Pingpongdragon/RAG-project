"""
Ours: Soft-Weighted FID + BBSE Ensemble

==========================
æ ¸å¿ƒåˆ›æ–°: Soft-Weighted FID
==========================

é—®é¢˜:
    DriftLens (Greco et al., TKDE 2025) æŒ‰ argmax ç¡¬åˆ†é…æ ·æœ¬åˆ° per-label ç»„ã€‚
    å½“æ¨¡åž‹ä¸ç¡®å®šæ—¶ (å¦‚ P = [0.35, 0.30, 0.20, 0.15])ï¼Œç¡¬åˆ†é…ä¼š:
      1. å°†è¾¹ç•Œæ ·æœ¬åˆ†åˆ°é”™è¯¯çš„ç»„ â†’ æ±¡æŸ“é«˜æ–¯ä¼°è®¡
      2. ä¸¢å¤±"æ¨¡åž‹ä¸ç¡®å®š"è¿™ä¸ªé‡è¦ä¿¡å·

è§£å†³:
    æ¯æ¡æ ·æœ¬å¯¹ **æ‰€æœ‰** label çš„é«˜æ–¯ä¼°è®¡éƒ½æœ‰è´¡çŒ®ï¼Œ
    æƒé‡ = P(label_k | x) (å³ softmax æ¦‚çŽ‡)

    åŠ æƒé«˜æ–¯ä¼°è®¡:
      Î¼_k = Î£_i w_{i,k} Â· x_i / Î£_i w_{i,k}
      Î£_k = Î£_i w_{i,k} Â· (x_i - Î¼_k)(x_i - Î¼_k)^T / Î£_i w_{i,k}

    å…¶ä¸­ w_{i,k} = P(class=k | query_i)

    è¿™ä¿è¯:
      - é«˜ç½®ä¿¡æ ·æœ¬ä¸»è¦è´¡çŒ®ç»™å¯¹åº” class çš„é«˜æ–¯
      - ä½Žç½®ä¿¡æ ·æœ¬å‡åŒ€åˆ†æ•£åˆ°æ‰€æœ‰ class â†’ ä¸ä¼šæ±¡æŸ“ä»»ä½•å•ä¸€ class
      - æ¨¡åž‹è¶Šæ ¡å‡†ï¼Œæ•ˆæžœè¶Šå¥½ (æ‰€ä»¥å…ˆåš Temperature Scaling)

========================
åŒè§†è§’ Ensemble
========================

è§†è§’ 1 â€” Soft FID: æ£€æµ‹ **æ¡ä»¶åˆ†å¸ƒ** P(prob_vec | class=k) çš„å˜åŒ–
    â†’ æ•æ‰: "åŒä¸€ç±»åˆ«å†…çš„æ¦‚çŽ‡å‘é‡åˆ†å¸ƒå½¢çŠ¶å˜äº†"
    â†’ ä¾‹å¦‚: STEM çš„é—®é¢˜ä»Ž "ç‰©ç†é¢˜" å˜æˆ "CS é¢˜"ï¼Œè™½ç„¶éƒ½é¢„æµ‹ä¸º STEMï¼Œ
            ä½†æ¦‚çŽ‡åˆ†å¸ƒçš„ç½®ä¿¡åº¦/å½¢çŠ¶ä¸åŒ

è§†è§’ 2 â€” BBSE: æ£€æµ‹ **æ ‡ç­¾å…ˆéªŒ** P(class=k) çš„å˜åŒ–
    â†’ æ•æ‰: "ç”¨æˆ·æ„Ÿå…´è¶£çš„é¢†åŸŸæ¯”ä¾‹å˜äº†"
    â†’ ä¾‹å¦‚: ä»¥å‰ 40% STEMï¼ŒçŽ°åœ¨ 70% STEM

    BBSE è®ºæ–‡:
        Lipton et al., "Detecting and Correcting for Label Shift
        with Black Box Predictors", ICML 2018
        https://arxiv.org/abs/1802.03916

    æ ¡å‡†æ”¹è¿›:
        Alexandari et al., "Maximum Likelihood with Bias-Corrected
        Calibration is Hard-To-Beat at Label Shift Adaptation",
        ICML 2020

åˆ¤å®š: Soft-FID AND BBSE éƒ½æŠ¥è­¦æ‰ç¡®è®¤ shift â†’ é™ä½Žå‡é˜³æ€§
"""

import numpy as np
from typing import Dict, Optional, Tuple
import logging

from ..base import DOMAINS, NUM_CLASSES
from .utils import estimate_gaussian, frechet_distance, bootstrap_threshold

logger = logging.getLogger(__name__)


class SoftFIDBBSEDetector:
    """
    Soft-Weighted FID + BBSE åŒè§†è§’æ¼‚ç§»æ£€æµ‹
    """

    def __init__(
        self,
        kb_reference_probs: np.ndarray,
        confusion_matrix: Optional[np.ndarray] = None,
        n_bootstrap: int = 1000,
        threshold_percentile: float = 95.0,
        window_size: int = 50,
        bbse_l1_threshold: float = 0.3,
    ):
        """
        Args:
            kb_reference_probs: shape (N, C) â€” KB æ¦‚çŽ‡å‘é‡
            confusion_matrix: shape (C, C) â€” C[i,j] = çœŸå®ž i è¢«é¢„æµ‹ä¸º j çš„æ¬¡æ•°
            n_bootstrap: é˜ˆå€¼ä¼°è®¡é‡‡æ ·æ¬¡æ•°
            threshold_percentile: é˜ˆå€¼åˆ†ä½æ•°
            window_size: æ£€æµ‹çª—å£å¤§å°
            bbse_l1_threshold: BBSE L1 è·ç¦»é˜ˆå€¼
        """
        self.ref = np.array(kb_reference_probs, dtype=np.float64)
        self.window_size = window_size
        self.bbse_l1_threshold = bbse_l1_threshold

        # ===== 1. Soft-Weighted Per-Label åŸºçº¿é«˜æ–¯ =====
        # æ¯æ¡ KB æ–‡æ¡£çš„æ¦‚çŽ‡å‘é‡æœ¬èº«å°±æ˜¯è½¯æƒé‡
        self.soft_label_mu = {}
        self.soft_label_sigma = {}

        for k in range(NUM_CLASSES):
            weights = self.ref[:, k]  # P(class=k | doc_i)
            self.soft_label_mu[k], self.soft_label_sigma[k] = estimate_gaussian(
                self.ref, weights=weights
            )

        # Per-batch åŸºçº¿
        self.batch_mu, self.batch_sigma = estimate_gaussian(self.ref)

        # ===== 2. Bootstrap é˜ˆå€¼ =====
        # Soft-FID global
        self.batch_threshold = bootstrap_threshold(
            self.ref, n_bootstrap, window_size, threshold_percentile,
            compute_fn=self._compute_soft_global_fid
        )
        logger.info(f"ðŸ“ [Ours] Soft batch FID threshold = {self.batch_threshold:.4f}")

        # Soft-FID per-label
        self.label_thresholds = {}
        for k in range(NUM_CLASSES):
            th = bootstrap_threshold(
                self.ref, n_bootstrap, window_size, threshold_percentile,
                compute_fn=lambda w, _k=k: self._compute_soft_label_fid(w, _k)
            )
            self.label_thresholds[k] = th
            logger.info(f"   {DOMAINS[k]:15s} soft FID threshold = {th:.4f}")

        # ===== 3. BBSE (Lipton et al., ICML 2018) =====
        self.kb_dist_vec = np.mean(self.ref, axis=0)
        self.C_norm = None
        if confusion_matrix is not None:
            cm = np.array(confusion_matrix, dtype=np.float64)
            row_sums = cm.sum(axis=1, keepdims=True)
            self.C_norm = cm / np.maximum(row_sums, 1e-10)
            cond = np.linalg.cond(self.C_norm.T)
            logger.info(f"ðŸ“Š [Ours] BBSE æ··æ·†çŸ©é˜µæ¡ä»¶æ•° = {cond:.1f}")
            if cond > 100:
                logger.warning(f"âš ï¸ BBSE æ¡ä»¶æ•° > 100, ä¼°è®¡ä¸ç¨³å®š")

    def _compute_soft_global_fid(self, window: np.ndarray) -> float:
        mu, sigma = estimate_gaussian(window)
        return frechet_distance(self.batch_mu, self.batch_sigma, mu, sigma)

    def _compute_soft_label_fid(self, window: np.ndarray, label_idx: int) -> float:
        weights = window[:, label_idx]
        mu, sigma = estimate_gaussian(window, weights=weights)
        return frechet_distance(
            self.soft_label_mu[label_idx], self.soft_label_sigma[label_idx],
            mu, sigma
        )

    def _bbse_estimate(self, query_probs: np.ndarray) -> Tuple[np.ndarray, float]:
        """BBSE: C^T Â· w = Î¼Ì‚ â†’ w"""
        if self.C_norm is None:
            return self.kb_dist_vec.copy(), 0.0

        mu_hat = np.mean(query_probs, axis=0)
        try:
            w = np.linalg.solve(self.C_norm.T, mu_hat)
            w = np.maximum(w, 0)
            s = w.sum()
            w = w / s if s > 0 else np.ones(NUM_CLASSES) / NUM_CLASSES
        except np.linalg.LinAlgError:
            w = mu_hat

        l1 = float(np.sum(np.abs(w - self.kb_dist_vec)))
        return w, l1

    def detect(
        self, query_probs: np.ndarray
    ) -> Tuple[float, Dict[str, float], float, np.ndarray, bool]:
        """
        åŒè§†è§’æ£€æµ‹

        Returns:
            (soft_fid_global, soft_fid_per_label, bbse_l1,
             estimated_dist, is_shift)
        """
        # ===== è§†è§’ 1: Soft-Weighted FID =====
        q_mu, q_sigma = estimate_gaussian(query_probs)
        soft_global = frechet_distance(self.batch_mu, self.batch_sigma, q_mu, q_sigma)

        soft_per_label = {}
        any_label_shift = False
        for k in range(NUM_CLASSES):
            weights = query_probs[:, k]
            q_label_mu, q_label_sigma = estimate_gaussian(query_probs, weights=weights)
            fid_k = frechet_distance(
                self.soft_label_mu[k], self.soft_label_sigma[k],
                q_label_mu, q_label_sigma
            )
            soft_per_label[DOMAINS[k]] = fid_k
            if fid_k > self.label_thresholds[k]:
                any_label_shift = True
                logger.info(
                    f"   [Soft-FID] {DOMAINS[k]}: {fid_k:.4f} > {self.label_thresholds[k]:.4f}"
                )

        fid_shift = (soft_global > self.batch_threshold) or any_label_shift

        # ===== è§†è§’ 2: BBSE =====
        est_dist, bbse_l1 = self._bbse_estimate(query_probs)
        bbse_shift = bbse_l1 > self.bbse_l1_threshold

        # ===== AND æŠ•ç¥¨ =====
        is_shift = fid_shift and bbse_shift

        if is_shift:
            logger.warning(
                f"ðŸš¨ [Ours] FID={soft_global:.4f} (th={self.batch_threshold:.4f}) | "
                f"BBSE L1={bbse_l1:.4f} (th={self.bbse_l1_threshold})"
            )

        return soft_global, soft_per_label, bbse_l1, est_dist, is_shift
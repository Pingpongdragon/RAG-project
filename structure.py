import json
from datasets import load_dataset
from pathlib import Path
from typing import Dict, Any
import os

# ä½¿ç”¨é•œåƒåŠ é€Ÿ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

class DatasetStructureViewer:
    """æŸ¥çœ‹æ•°æ®é›†ç»“æ„"""
    
    def __init__(self):
        self.indent = "  "
    
    def print_structure(self, obj: Any, level: int = 0, max_str_len: int = 100):
        """é€’å½’æ‰“å°æ•°æ®ç»“æ„"""
        prefix = self.indent * level
        
        if isinstance(obj, dict):
            print(f"{prefix}{{")
            for key, value in list(obj.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”®
                print(f"{prefix}  '{key}': ", end="")
                if isinstance(value, (dict, list)):
                    print()
                    self.print_structure(value, level + 2, max_str_len)
                else:
                    val_str = str(value)
                    if len(val_str) > max_str_len:
                        val_str = val_str[:max_str_len] + "..."
                    print(f"{repr(val_str)},")
            if len(obj) > 10:
                print(f"{prefix}  ... ({len(obj) - 10} more keys)")
            print(f"{prefix}}}")
        
        elif isinstance(obj, list):
            print(f"{prefix}[")
            for i, item in enumerate(obj[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªå…ƒç´ 
                self.print_structure(item, level + 1, max_str_len)
                if i < min(2, len(obj) - 1):
                    print(f"{prefix},")
            if len(obj) > 3:
                print(f"{prefix}  ... ({len(obj) - 3} more items)")
            print(f"{prefix}]")
        
        else:
            val_str = str(obj)
            if len(val_str) > max_str_len:
                val_str = val_str[:max_str_len] + "..."
            print(f"{prefix}{repr(val_str)}")
    
    def view_wizard_of_wikipedia(self):
        """æŸ¥çœ‹ Wizard of Wikipedia æ•°æ®é›†ç»“æ„"""
        print("=" * 80)
        print("ğŸ“š Wizard of Wikipedia æ•°æ®é›†ç»“æ„")
        print("=" * 80)
        
        try:
            # åŠ è½½æ•°æ®é›†ï¼ˆåªåŠ è½½ä¸€å°éƒ¨åˆ†ï¼‰
            print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")
            dataset = load_dataset("chujiezheng/wizard_of_wikipedia", split="train[:10]")
            
            print(f"\nâœ… æ•°æ®é›†ä¿¡æ¯:")
            print(f"  - Split: train (showing first 10 samples)")
            print(f"  - Total features: {len(dataset.features)}")
            print(f"  - Features: {list(dataset.features.keys())}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„å®Œæ•´ç»“æ„
            print(f"\nğŸ“‹ ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å®Œæ•´ç»“æ„:")
            sample = dataset[0]
            self.print_structure(sample)
            
            # æ˜¾ç¤ºå­—æ®µè¯´æ˜
            print(f"\nğŸ“ å­—æ®µè¯´æ˜:")
            print(f"  â€¢ chosen_topic: é€‰æ‹©çš„ä¸»é¢˜")
            print(f"  â€¢ persona: è§’è‰²è®¾å®š")
            print(f"  â€¢ wizard_eval: å·«å¸ˆè¯„åˆ†")
            print(f"  â€¢ dialog: å¯¹è¯å†…å®¹åˆ—è¡¨")
            print(f"  â€¢ knowledge: æ¯è½®å¯¹è¯çš„å€™é€‰çŸ¥è¯†æ®µè½")
            print(f"  â€¢ topics: å¯é€‰ä¸»é¢˜åˆ—è¡¨")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            topics_count = len(sample.get('topics', []))
            dialog_turns = len(sample.get('dialog', []))
            knowledge_turns = len(sample.get('knowledge', []))
            print(f"  â€¢ å¯é€‰ä¸»é¢˜æ•°: {topics_count}")
            print(f"  â€¢ å¯¹è¯è½®æ•°: {dialog_turns}")
            print(f"  â€¢ çŸ¥è¯†æ®µè½ç»„æ•°: {knowledge_turns}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    
    def view_hotpotqa_distractor(self):
        """æŸ¥çœ‹ HotpotQA Distractor æ•°æ®é›†ç»“æ„"""
        print("\n" + "=" * 80)
        print("ğŸ”¥ HotpotQA Distractor æ•°æ®é›†ç»“æ„")
        print("=" * 80)
        
        try:
            print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")
            dataset = load_dataset("hotpot_qa", "distractor", split="train[:10]")
            
            print(f"\nâœ… æ•°æ®é›†ä¿¡æ¯:")
            print(f"  - Split: train (showing first 10 samples)")
            print(f"  - Total features: {len(dataset.features)}")
            print(f"  - Features: {list(dataset.features.keys())}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
            print(f"\nğŸ“‹ ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å®Œæ•´ç»“æ„:")
            sample = dataset[0]
            self.print_structure(sample)
            
            # æ˜¾ç¤ºå­—æ®µè¯´æ˜
            print(f"\nğŸ“ å­—æ®µè¯´æ˜:")
            print(f"  â€¢ id: æ ·æœ¬å”¯ä¸€æ ‡è¯†")
            print(f"  â€¢ question: é—®é¢˜")
            print(f"  â€¢ answer: ç­”æ¡ˆ")
            print(f"  â€¢ type: é—®é¢˜ç±»å‹ (comparison/bridge)")
            print(f"  â€¢ level: éš¾åº¦çº§åˆ« (easy/medium/hard)")
            print(f"  â€¢ supporting_facts: æ”¯æŒäº‹å® [[title, sent_id], ...]")
            print(f"  â€¢ context: 10ç¯‡æ–‡æ¡£ (2ç¯‡é‡‘æ ‡å‡† + 8ç¯‡å¹²æ‰°)")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  â€¢ é—®é¢˜ç±»å‹: {sample.get('type')}")
            print(f"  â€¢ éš¾åº¦: {sample.get('level')}")
            print(f"  â€¢ æ–‡æ¡£æ•°é‡: {len(sample.get('context', []))}")
            print(f"  â€¢ æ”¯æŒäº‹å®æ•°é‡: {len(sample.get('supporting_facts', []))}")
            
            # æ˜¾ç¤º context ç»“æ„
            if sample.get('context'):
                print(f"\nğŸ“„ Context ç»“æ„ç¤ºä¾‹ (ç¬¬ä¸€ç¯‡æ–‡æ¡£):")
                first_doc = sample['context'][0]
                print(f"  Title: {first_doc[0]}")
                print(f"  Sentences: {len(first_doc[1])} å¥")
                print(f"  First sentence: {first_doc[1][0][:100]}...")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    
    def view_hotpotqa_fullwiki(self):
        """æŸ¥çœ‹ HotpotQA FullWiki æ•°æ®é›†ç»“æ„"""
        print("\n" + "=" * 80)
        print("ğŸŒ HotpotQA FullWiki æ•°æ®é›†ç»“æ„")
        print("=" * 80)
        
        try:
            print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")
            dataset = load_dataset("hotpot_qa", "fullwiki", split="train[:10]")
            
            print(f"\nâœ… æ•°æ®é›†ä¿¡æ¯:")
            print(f"  - Split: train (showing first 10 samples)")
            print(f"  - Total features: {len(dataset.features)}")
            print(f"  - Features: {list(dataset.features.keys())}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
            print(f"\nğŸ“‹ ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å®Œæ•´ç»“æ„:")
            sample = dataset[0]
            self.print_structure(sample)
            
            # æ˜¾ç¤ºå­—æ®µè¯´æ˜
            print(f"\nğŸ“ å­—æ®µè¯´æ˜:")
            print(f"  â€¢ id: æ ·æœ¬å”¯ä¸€æ ‡è¯†")
            print(f"  â€¢ question: é—®é¢˜")
            print(f"  â€¢ answer: ç­”æ¡ˆ")
            print(f"  â€¢ type: é—®é¢˜ç±»å‹")
            print(f"  â€¢ level: éš¾åº¦çº§åˆ«")
            print(f"  â€¢ supporting_facts: æ”¯æŒäº‹å®")
            print(f"  âš ï¸  æ³¨æ„: FullWiki ç‰ˆæœ¬æ²¡æœ‰ context å­—æ®µ")
            print(f"  âš ï¸  éœ€è¦ä»å®Œæ•´ Wikipedia ä¸­æ£€ç´¢æ–‡æ¡£")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  â€¢ é—®é¢˜ç±»å‹: {sample.get('type')}")
            print(f"  â€¢ éš¾åº¦: {sample.get('level')}")
            print(f"  â€¢ æ”¯æŒäº‹å®æ•°é‡: {len(sample.get('supporting_facts', []))}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    
    def compare_datasets(self):
        """å¯¹æ¯”ä¸¤ä¸ªæ•°æ®é›†çš„å·®å¼‚"""
        print("\n" + "=" * 80)
        print("ğŸ” æ•°æ®é›†å¯¹æ¯”")
        print("=" * 80)
        
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç‰¹æ€§                â”‚ Wizard of Wikipedia  â”‚ HotpotQA            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ä»»åŠ¡ç±»å‹            â”‚ çŸ¥è¯†å¯¹è¯ç”Ÿæˆ         â”‚ å¤šè·³é—®ç­”             â”‚
â”‚ æ•°æ®æ ¼å¼            â”‚ å¯¹è¯è½®æ¬¡             â”‚ é—®é¢˜-ç­”æ¡ˆå¯¹          â”‚
â”‚ çŸ¥è¯†æ¥æº            â”‚ Wikipedia            â”‚ Wikipedia           â”‚
â”‚ æ¨ç†è·³æ•°            â”‚ 1è·³                  â”‚ 2-4è·³                â”‚
â”‚ å¹³å‡å¯¹è¯è½®æ•°        â”‚ 9è½®                  â”‚ N/A                 â”‚
â”‚ å€™é€‰æ–‡æ¡£æ•°          â”‚ å¤šä¸ªæ®µè½/è½®          â”‚ 10ç¯‡ (distractor)   â”‚
â”‚ æ ‡æ³¨ç±»å‹            â”‚ å¯¹è¯+çŸ¥è¯†é€‰æ‹©        â”‚ é—®ç­”+æ”¯æŒäº‹å®        â”‚
â”‚ è®­ç»ƒé›†å¤§å°          â”‚ ~18k å¯¹è¯            â”‚ ~90k é—®é¢˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ ä½¿ç”¨å»ºè®®:
  â€¢ Wizard of Wikipedia: é€‚åˆå¯¹è¯ç³»ç»Ÿã€çŸ¥è¯†å¢å¼ºç”Ÿæˆ
  â€¢ HotpotQA: é€‚åˆæ¨ç†èƒ½åŠ›ã€å¤šè·³é—®ç­”ç ”ç©¶
        """)
    
    def save_samples_to_file(self, output_dir: str = "./dataset_samples"):
        """ä¿å­˜æ ·æœ¬åˆ°æ–‡ä»¶ä»¥ä¾¿è¯¦ç»†æŸ¥çœ‹"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print("\n" + "=" * 80)
        print("ğŸ’¾ ä¿å­˜æ ·æœ¬åˆ°æ–‡ä»¶")
        print("=" * 80)
        
        datasets_to_save = [
            ("chujiezheng/wizard_of_wikipedia", None, "wizard_sample.json"),
            ("hotpot_qa", "distractor", "hotpotqa_distractor_sample.json"),
            ("hotpot_qa", "fullwiki", "hotpotqa_fullwiki_sample.json"),
        ]
        
        for dataset_name, config, filename in datasets_to_save:
            try:
                print(f"\næ­£åœ¨ä¿å­˜ {dataset_name} ({config or 'default'})...")
                
                if config:
                    dataset = load_dataset(dataset_name, config, split="train[:5]")
                else:
                    dataset = load_dataset(dataset_name, split="train[:5]")
                
                # è½¬æ¢ä¸º list of dicts
                samples = [dict(item) for item in dataset]
                
                output_file = output_path / filename
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(samples, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… å·²ä¿å­˜ {len(samples)} ä¸ªæ ·æœ¬åˆ°: {output_file}")
                
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {e}")


def main():
    viewer = DatasetStructureViewer()
    
    # æŸ¥çœ‹å„ä¸ªæ•°æ®é›†ç»“æ„
    viewer.view_wizard_of_wikipedia()
    viewer.view_hotpotqa_distractor()
    viewer.view_hotpotqa_fullwiki()
    
    # å¯¹æ¯”æ•°æ®é›†
    viewer.compare_datasets()
    
    # ä¿å­˜æ ·æœ¬åˆ°æ–‡ä»¶
    viewer.save_samples_to_file()
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æ•°æ®é›†ç»“æ„æŸ¥çœ‹å®Œæˆï¼")
    print("ğŸ’¡ æç¤º: æŸ¥çœ‹ ./dataset_samples/ ç›®å½•ä¸‹çš„ JSON æ–‡ä»¶è·å–æ›´å¤šç»†èŠ‚")
    print("=" * 80)


if __name__ == "__main__":
    main()